{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Final Exam Exercise\n",
    "\n",
    "---\n",
    "__Name:__   Megan Jasek  <br \\> \n",
    "__Class:__ MIDS w261 (Summer 2016, Section ?) <br \\> \n",
    "__Email:__ meganjasek@ischool.berkeley.edu <br \\> \n",
    "__Week:__  14 <br \\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# purpose of cell: download and view test data set from DropBox for use in subsequent cells\n",
    "\n",
    "#!wget https://www.dropbox.com/sh/2c0k5adwz36lkcw/AADxzBgNxNF5Q6-eanjnK64qa/PageRank-test.txt\n",
    "#!cat PageRank-test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Spark* implementation of basic PageRank\n",
    "\n",
    "-----\n",
    "\n",
    "- Per Jimi, if we translate the MapReduce concepts to Spark, that will start our juices and we should be set up nicely for the Final Exam.\n",
    "- We can run locally since the Final Exam is expected to be similar in format to the Midterm Exam (e.g., not require AWS or SoftLayer).\n",
    "- He encouraged us to feel free to share notebook(s) on Google Groups since that might help each other.\n",
    "\n",
    "-----\n",
    "\n",
    "**The remaining text below is verbatim from HW9.1, except for the first sentence which replaces 'MRJob' with 'Spark'.**\n",
    "\n",
    "As we had written for HW9.1 (basic MRJob implementation), now write a basic Spark implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input.\n",
    "\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d, one of the hyperlinks in the current page. This probability is represented by a so-called ‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page.\n",
    "\n",
    "As you build your code, use the test data\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image in the Wikipedia article: https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A,0.033 <br />\n",
    "B,0.384 <br />\n",
    "C,0.343 <br />\n",
    "D,0.039 <br />\n",
    "E,0.081 <br />\n",
    "F,0.039 <br />\n",
    "G,0.016 <br />\n",
    "H,0.016 <br />\n",
    "I,0.016 <br />\n",
    "J,0.016 <br />\n",
    "K,0.016 <br />\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Context to use throughout this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f0dc55bc8d0>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f0dc55bc910>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"HW11\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data into an RDD and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t{'C': 1}\r\n",
      "C\t{'B': 1}\r\n",
      "D\t{'A': 1, 'B': 1}\r\n",
      "E\t{'D': 1, 'B': 1, 'F': 1}\r\n",
      "F\t{'B': 1, 'E': 1}\r\n",
      "G\t{'B': 1, 'E': 1}\r\n",
      "H\t{'B': 1, 'E': 1}\r\n",
      "I\t{'B': 1, 'E': 1}\r\n",
      "J\t{'E': 1}\r\n",
      "K\t{'E': 1}"
     ]
    }
   ],
   "source": [
    "!cat PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[('A', {}), ('C', {'B': 1}), ('E', {'B': 1, 'D': 1, 'F': 1}), ('G', {'B': 1, 'E': 1}), ('I', {'B': 1, 'E': 1}), ('K', {'E': 1}), ('H', {'B': 1, 'E': 1}), ('J', {'E': 1}), ('B', {'C': 1}), ('D', {'A': 1, 'B': 1}), ('F', {'B': 1, 'E': 1})]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "Node = namedtuple('Node', 'name adj_list page_rank')\n",
    "\n",
    "def parsePoint(line):\n",
    "    all_nodes = []\n",
    "    node_num, adj_dict = line.strip().split('\\t')\n",
    "    adj_dict = ast.literal_eval(adj_dict)\n",
    "    all_nodes.append((str(node_num), adj_dict))\n",
    "    for node in adj_dict:\n",
    "        all_nodes.append((node, {}))\n",
    "    return all_nodes\n",
    "\n",
    "def aggNodes(x, y):\n",
    "    if x != {}:\n",
    "        return x\n",
    "    elif y != {}:\n",
    "        return y\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "def initNode(node, n):\n",
    "    return Node(node[0], node[1], 1.0/n)\n",
    "\n",
    "fileName = 'PageRank-test.txt'\n",
    "\n",
    "rawNodes = sc.textFile(fileName).flatMap(parsePoint).reduceByKey(aggNodes)\n",
    "num_nodes = rawNodes.count()\n",
    "print n\n",
    "print rawNodes.take(n)\n",
    "\n",
    "# Initialize Page Ranks\n",
    "nodes = rawNodes.map(lambda node: initNode(node, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
