{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Final Exam Exercise\n",
    "\n",
    "---\n",
    "__Name:__   Megan Jasek  <br \\> \n",
    "__Class:__ MIDS w261 (Summer 2016, Section ?) <br \\> \n",
    "__Email:__ meganjasek@ischool.berkeley.edu <br \\> \n",
    "__Week:__  14 <br \\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# purpose of cell: download and view test data set from DropBox for use in subsequent cells\n",
    "\n",
    "#!wget https://www.dropbox.com/sh/2c0k5adwz36lkcw/AADxzBgNxNF5Q6-eanjnK64qa/PageRank-test.txt\n",
    "#!cat PageRank-test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Spark* implementation of basic PageRank\n",
    "\n",
    "-----\n",
    "\n",
    "- Per Jimi, if we translate the MapReduce concepts to Spark, that will start our juices and we should be set up nicely for the Final Exam.\n",
    "- We can run locally since the Final Exam is expected to be similar in format to the Midterm Exam (e.g., not require AWS or SoftLayer).\n",
    "- He encouraged us to feel free to share notebook(s) on Google Groups since that might help each other.\n",
    "\n",
    "-----\n",
    "\n",
    "**The remaining text below is verbatim from HW9.1, except for the first sentence which replaces 'MRJob' with 'Spark'.**\n",
    "\n",
    "As we had written for HW9.1 (basic MRJob implementation), now write a basic Spark implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input.\n",
    "\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d, one of the hyperlinks in the current page. This probability is represented by a so-called ‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page.\n",
    "\n",
    "As you build your code, use the test data\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image in the Wikipedia article: https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A,0.033 <br />\n",
    "B,0.384 <br />\n",
    "C,0.343 <br />\n",
    "D,0.039 <br />\n",
    "E,0.081 <br />\n",
    "F,0.039 <br />\n",
    "G,0.016 <br />\n",
    "H,0.016 <br />\n",
    "I,0.016 <br />\n",
    "J,0.016 <br />\n",
    "K,0.016 <br />\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Context to use throughout this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7fc6766e98d0>\n",
      "<pyspark.sql.context.SQLContext object at 0x7fc6766e9910>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"HW11\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print sc\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data into an RDD and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t{'C': 1}\r\n",
      "C\t{'B': 1}\r\n",
      "D\t{'A': 1, 'B': 1}\r\n",
      "E\t{'D': 1, 'B': 1, 'F': 1}\r\n",
      "F\t{'B': 1, 'E': 1}\r\n",
      "G\t{'B': 1, 'E': 1}\r\n",
      "H\t{'B': 1, 'E': 1}\r\n",
      "I\t{'B': 1, 'E': 1}\r\n",
      "J\t{'E': 1}\r\n",
      "K\t{'E': 1}"
     ]
    }
   ],
   "source": [
    "!cat PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# return a node and adjacency list for all nodes in the file plus all nodes\n",
    "# in the adjacency lists of nodes in the file.  For the latter, output (node, {})\n",
    "def parsePoint(line):\n",
    "    all_nodes = []\n",
    "    node_num, adj_dict = line.strip().split('\\t')\n",
    "    adj_dict = ast.literal_eval(adj_dict)\n",
    "    all_nodes.append((str(node_num), adj_dict))\n",
    "    for node in adj_dict:\n",
    "        all_nodes.append((node, {}))\n",
    "    return all_nodes\n",
    "\n",
    "# aggregate all adjacency lists by key\n",
    "def reduceNodes(x, y):\n",
    "    if x != {}:\n",
    "        return x\n",
    "    elif y != {}:\n",
    "        return y\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "# Initialize all nodes with a page rank of 1/n\n",
    "def initNode(node, n):\n",
    "    return (node[0], (node[1], 1.0/n))\n",
    "\n",
    "# Distribute the page rank across all nodes and keep track of dangling mass page rank with '*'\n",
    "def mapPageRankStep1(node):\n",
    "    name = node[0]\n",
    "    adj_list = node[1][0]\n",
    "    page_rank = node[1][1]\n",
    "    all_nodes = []\n",
    "    degree = float(len(adj_list))\n",
    "    if adj_list == {}:\n",
    "        all_nodes.append(('*',({}, page_rank)))\n",
    "    all_nodes.append((name, (adj_list, 0.0)))\n",
    "    for item in adj_list:\n",
    "        all_nodes.append((item, ({}, page_rank/degree)))\n",
    "    return all_nodes\n",
    "\n",
    "# aggregate the page rank\n",
    "def reducePageRankStep1(x, y):\n",
    "    page_rank = x[1]+y[1]\n",
    "    if x[0] != {}:\n",
    "        return (x[0], page_rank)\n",
    "    elif y[0] != {}:\n",
    "        return (y[0], page_rank)\n",
    "    else:\n",
    "        return ({}, page_rank)\n",
    "\n",
    "# Return the final page rank calculation\n",
    "def mapPageRankStep2(node, damping_factor, total_nodes, dangling_mass):\n",
    "    page_rank = (1-damping_factor)*(1.0/total_nodes)+damping_factor*((dangling_mass/total_nodes)+node[1][1])\n",
    "    return (node[0], (node[1][0], page_rank))\n",
    "\n",
    "# Calculate the error between two node RDDs\n",
    "def getError(n1, n2):\n",
    "    return sum(abs(np.asarray(n1.map(lambda x: x[1][1]).collect()) - np.asarray(n2.map(lambda x: x[1][1]).collect())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes 11\n",
      "Initial page ranks\n",
      "A 0.0909090909091\n",
      "B 0.0909090909091\n",
      "C 0.0909090909091\n",
      "D 0.0909090909091\n",
      "E 0.0909090909091\n",
      "F 0.0909090909091\n",
      "G 0.0909090909091\n",
      "H 0.0909090909091\n",
      "I 0.0909090909091\n",
      "J 0.0909090909091\n",
      "K 0.0909090909091\n",
      "\n",
      "Iteration 1\n",
      "Dangling mass 0.0909090909091\n",
      "Total page rank 1.0\n",
      "Error 0.943663911846\n",
      "\n",
      "Iteration 2\n",
      "Dangling mass 0.0592975206612\n",
      "Total page rank 1.0\n",
      "Error 0.640171550213\n",
      "\n",
      "Iteration 3\n",
      "Dangling mass 0.0379464062109\n",
      "Total page rank 1.0\n",
      "Error 0.382958337128\n",
      "\n",
      "Iteration 4\n",
      "Dangling mass 0.0640190695934\n",
      "Total page rank 1.0\n",
      "Error 0.302769848196\n",
      "\n",
      "Iteration 5\n",
      "Dangling mass 0.0375959647951\n",
      "Total page rank 1.0\n",
      "Error 0.227349178989\n",
      "\n",
      "Iteration 6\n",
      "Dangling mass 0.0386749363905\n",
      "Total page rank 1.0\n",
      "Error 0.18873867623\n",
      "\n",
      "Iteration 7\n",
      "Dangling mass 0.0341177257382\n",
      "Total page rank 1.0\n",
      "Error 0.152290457013\n",
      "\n",
      "Iteration 8\n",
      "Dangling mass 0.0346526855821\n",
      "Total page rank 1.0\n",
      "Error 0.128755161228\n",
      "\n",
      "Iteration 9\n",
      "Dangling mass 0.0332641479909\n",
      "Total page rank 1.0\n",
      "Error 0.107417609557\n",
      "\n",
      "Iteration 10\n",
      "Dangling mass 0.0332687068063\n",
      "Total page rank 1.0\n",
      "Error 0.091230582511\n",
      "\n",
      "Iteration 11\n",
      "Dangling mass 0.0329301017862\n",
      "Total page rank 1.0\n",
      "Error 0.0770567797055\n",
      "\n",
      "Iteration 12\n",
      "Dangling mass 0.0329194443643\n",
      "Total page rank 1.0\n",
      "Error 0.0654966156936\n",
      "\n",
      "Iteration 13\n",
      "Dangling mass 0.0328282893463\n",
      "Total page rank 1.0\n",
      "Error 0.0555541429909\n",
      "\n",
      "Iteration 14\n",
      "Dangling mass 0.0328197384167\n",
      "Total page rank 1.0\n",
      "Error 0.047219700035\n",
      "\n",
      "Iteration 15\n",
      "Dangling mass 0.0327957341511\n",
      "Total page rank 1.0\n",
      "Error 0.0400994512127\n",
      "\n",
      "Iteration 16\n",
      "Dangling mass 0.0327922737609\n",
      "Total page rank 1.0\n",
      "Error 0.0340839987432\n",
      "\n",
      "Iteration 17\n",
      "Dangling mass 0.0327858041648\n",
      "Total page rank 1.0\n",
      "Error 0.0289598559597\n",
      "\n",
      "Iteration 18\n",
      "Dangling mass 0.0327845372023\n",
      "Total page rank 1.0\n",
      "Error 0.0246156817625\n",
      "\n",
      "Iteration 19\n",
      "Dangling mass 0.0327827832963\n",
      "Total page rank 1.0\n",
      "Error 0.0209198122727\n",
      "\n",
      "Iteration 20\n",
      "Dangling mass 0.0327823578046\n",
      "Total page rank 1.0\n",
      "Error 0.017781774674\n",
      "\n",
      "Iteration 21\n",
      "Dangling mass 0.032781877034\n",
      "Total page rank 1.0\n",
      "Error 0.0151134482863\n",
      "\n",
      "Iteration 22\n",
      "Dangling mass 0.0327817396015\n",
      "Total page rank 1.0\n",
      "Error 0.0128464098038\n",
      "\n",
      "Iteration 23\n",
      "Dangling mass 0.0327816067481\n",
      "Total page rank 1.0\n",
      "Error 0.0109191314434\n",
      "\n",
      "Iteration 24\n",
      "Dangling mass 0.032781563641\n",
      "Total page rank 1.0\n",
      "Error 0.00928125506489\n",
      "\n",
      "Iteration 25\n",
      "Dangling mass 0.0327815266405\n",
      "Total page rank 1.0\n",
      "Error 0.00788897267555\n",
      "\n",
      "Iteration 26\n",
      "Dangling mass 0.0327815133704\n",
      "Total page rank 1.0\n",
      "Error 0.00670562472339\n",
      "\n",
      "Iteration 27\n",
      "Dangling mass 0.0327815029967\n",
      "Total page rank 1.0\n",
      "Error 0.0056997531903\n",
      "\n",
      "Iteration 28\n",
      "Dangling mass 0.0327814989681\n",
      "Total page rank 1.0\n",
      "Error 0.00484478958915\n",
      "\n",
      "Iteration 29\n",
      "Dangling mass 0.0327814960427\n",
      "Total page rank 1.0\n",
      "Error 0.00411806295669\n",
      "\n",
      "Iteration 30\n",
      "Dangling mass 0.0327814948319\n",
      "Total page rank 1.0\n",
      "Error 0.00350035332606\n",
      "\n",
      "Iteration 31\n",
      "Dangling mass 0.0327814940028\n",
      "Total page rank 1.0\n",
      "Error 0.00297529792119\n",
      "\n",
      "Iteration 32\n",
      "Dangling mass 0.0327814936416\n",
      "Total page rank 1.0\n",
      "Error 0.0025290031772\n",
      "\n",
      "Iteration 33\n",
      "Dangling mass 0.0327814934057\n",
      "Total page rank 1.0\n",
      "Error 0.00214965199581\n",
      "\n",
      "Iteration 34\n",
      "Dangling mass 0.0327814932986\n",
      "Total page rank 1.0\n",
      "Error 0.00182720417989\n",
      "\n",
      "Iteration 35\n",
      "Dangling mass 0.0327814932312\n",
      "Total page rank 1.0\n",
      "Error 0.00155312334682\n",
      "\n",
      "Iteration 36\n",
      "Dangling mass 0.0327814931996\n",
      "Total page rank 1.0\n",
      "Error 0.00132015483991\n",
      "\n",
      "Iteration 37\n",
      "Dangling mass 0.0327814931803\n",
      "Total page rank 1.0\n",
      "Error 0.00112213155375\n",
      "\n",
      "Iteration 38\n",
      "Dangling mass 0.032781493171\n",
      "Total page rank 1.0\n",
      "Error 0.000953811819252\n",
      "\n",
      "Final page ranks\n",
      "A 0.0327814931654\n",
      "B 0.384181829876\n",
      "C 0.34312940441\n",
      "D 0.0390870921068\n",
      "E 0.0808856932454\n",
      "F 0.0390870921068\n",
      "G 0.0161694790178\n",
      "H 0.0161694790178\n",
      "I 0.0161694790178\n",
      "J 0.0161694790178\n",
      "K 0.0161694790178\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "damping_factor = 0.85\n",
    "epsilon = 0.001\n",
    "\n",
    "# Read in data and count nodes.  Be sure and entries for the dangling nodes.\n",
    "fileName = 'PageRank-test.txt'\n",
    "rawNodes = sc.textFile(fileName).flatMap(parsePoint).reduceByKey(reduceNodes)\n",
    "total_nodes = rawNodes.count()\n",
    "print 'Total nodes', total_nodes\n",
    "\n",
    "# Initialize Page Ranks\n",
    "nodesInit = rawNodes.map(lambda node: initNode(node, total_nodes)).cache()\n",
    "print 'Initial page ranks'\n",
    "#print nodesInit.take(total_nodes)\n",
    "for x in nodesInit.sortByKey().collect():\n",
    "    print x[0], x[1][1]\n",
    "print\n",
    "\n",
    "nodesPrev = nodesInit\n",
    "i = 1\n",
    "stop = False\n",
    "while (stop == False):\n",
    "    print 'Iteration', str(i) \n",
    "    # Loop through all of the nodes and accumulate the page rank.  Keep a special node called '*'\n",
    "    # to store the dangling mass\n",
    "    nodes1 = nodesPrev.flatMap(mapPageRankStep1).reduceByKey(reducePageRankStep1).cache()\n",
    "    # Extract the dangling mass from the RDD\n",
    "    dangling_mass = nodes1.filter(lambda node: node[0]=='*').collect()[0][1][1]\n",
    "    print 'Dangling mass', dangling_mass\n",
    "\n",
    "    # Use the dangling mass to calculate the final page ranks for this iteration\n",
    "    nodes2 = (nodes1.filter(lambda node: node[0]!='*')\n",
    "              .map(lambda node: mapPageRankStep2(node, damping_factor, total_nodes, dangling_mass))\n",
    "              .cache())\n",
    "    print 'Total page rank', nodes2.map(lambda x: x[1][1]).reduce(lambda x,y: x+y)\n",
    "    # Calculate the error between page ranks in this iteration and the previous iteration\n",
    "    error = getError(nodesPrev, nodes2)\n",
    "    print 'Error', error\n",
    "    # If the error is less than epsilon, then set stop to True\n",
    "    stop = error < epsilon\n",
    "    nodesPrev = nodes2\n",
    "    i += 1\n",
    "    print\n",
    "# Print the final values\n",
    "print 'Final page ranks'\n",
    "for x in nodes2.sortByKey().collect():\n",
    "    print x[0], x[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
