{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261 - Machine Learning At Scale\n",
    "## Assignment - Week 09\n",
    "---\n",
    "\n",
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Date:**  7/17/16\n",
    "\n",
    "---\n",
    "### Instructions\n",
    "\n",
    "Due by 07/17/2016\n",
    "\n",
    "[Submission Link - Google Form](https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form) \n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "### Useful References\n",
    "\n",
    "* Data-intensive text processing with MapReduce. San Rafael, CA: Morgan & Claypool Publishers. Chapter 5. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\">HW 9 Dataset</h2>\n",
    "\n",
    "Note that all referenced files life in the enclosing directory. [Checkout the Data subdirectory on Dropbox](https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0) or the AWS S3 buckets (details contained each question). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.0: Short answer questions </h2>\n",
    "\n",
    "__ What is PageRank and what is it used for in the context of web search?__\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the Steady State Distibution? __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ OPTIONAL: In topic-specific pagerank, how can we ensure that the irreducible property is satifsied? (HINT: see HW9.4) __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.1: MRJob implementation of basic PageRank </h2>\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d,one of the hyperlinks in the current page. This probability is represented by a so-called *damping factor* d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "> Dropbox: https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the [Wikipedia article](https://en.wikipedia.org/wiki/PageRank)\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "<pre>\n",
    "\n",
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.1 Implementation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test file:  PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PageRank-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank-test.txt\n",
    "B\t{'C': 1}\n",
    "C\t{'B': 1}\n",
    "D\t{'A': 1, 'B': 1}\n",
    "E\t{'D': 1, 'B': 1, 'F': 1}\n",
    "F\t{'B': 1, 'E': 1}\n",
    "G\t{'B': 1, 'E': 1}\n",
    "H\t{'B': 1, 'E': 1}\n",
    "I\t{'B': 1, 'E': 1}\n",
    "J\t{'E': 1}\n",
    "K\t{'E': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test file:  Lecture9_10_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Lecture9_10_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile Lecture9_10_test.txt\n",
    "N1\t{'N2': 1, 'N4': 1}\n",
    "N2\t{'N3': 1, 'N5': 1}\n",
    "N3\t{'N4': 1}\n",
    "N4\t{'N5': 1}\n",
    "N5\t{'N1': 1, 'N2': 1, 'N3': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output from PageRank-test.txt\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A, 0.033  \n",
    "B, 0.384  \n",
    "C, 0.343  \n",
    "D, 0.039  \n",
    "E, 0.081  \n",
    "F, 0.039  \n",
    "G, 0.016  \n",
    "H, 0.016  \n",
    "I, 0.016  \n",
    "J, 0.016  \n",
    "K, 0.016  \n",
    "\n",
    "0.999 = 0.033+0.384+0.343+0.039+0.081+0.039+0.016+0.016+0.016+0.016+0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankInit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankInit.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import json\n",
    "\n",
    "# This class takes an adjacency list as input and outputs and initialized work file with \n",
    "# the following format:\n",
    "# node_number \\t [adjacency_list, infinity, '', 'U']\n",
    "# One starting node is defined as an argument passed in to the class.  The default is node '1'.\n",
    "# That starting node has the format:\n",
    "# node_number \\t [adjacency_list, 0.0, '', 'Q']\n",
    "# U means Unvisited\n",
    "# Q means Queued in the queue\n",
    "class MRPageRankInit(MRJob):\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option called 'start_index' to indicate the starting\n",
    "        # index of the SSSP algorithm.\n",
    "        # Configure a new command line option called 'unweighted' to make the graph unweighted.\n",
    "        # If this is set to '1', then all distances from node to node are set to a weight of 1.\n",
    "        super(MRPageRankInit, self).configure_options()\n",
    "        self.add_passthrough_option('--damping_factor', type='int', default=0.15)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRPageRankInit, self).__init__(*args, **kwargs)\n",
    "        self.damping_factor = self.options.damping_factor\n",
    "        self.total_nodes = 0\n",
    "\n",
    "    # For the node equal to the start index, yield\n",
    "    #    node_number \\t tuple(adj_list, 0.0, \"\", 'Q')\n",
    "    # For all other nodes, yield\n",
    "    #    node_number \\t tuple(adj_list, infinity, \"\", 'U')    \n",
    "    def mapper(self, _, line):\n",
    "        node_num, adj_dict = line.strip().split('\\t')\n",
    "        adj_dict = ast.literal_eval(adj_dict)\n",
    "        self.total_nodes += 1\n",
    "        yield node_num, tuple((adj_dict, 1.0))\n",
    "\n",
    "    def mapper_final(self):\n",
    "        yield '*total_nodes', self.total_nodes\n",
    "    \n",
    "    def reducer(self, node_num, data):\n",
    "        if node_num == '*total_nodes':\n",
    "            for d in data:\n",
    "                self.total_nodes += d\n",
    "        else:\n",
    "            for adj_dict, value in data:\n",
    "                yield node_num, tuple((adj_dict, value/self.total_nodes))\n",
    "        \n",
    "    # Create the steps for the MRJob.  There is only a mapper in this job.\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper, mapper_final=self.mapper_final,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRPageRankInit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/PageRankInit.hadoop.20160713.043551.333962\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/PageRankInit.hadoop.20160713.043551.333962/output...\n",
      "\"N1\"\t[{\"N2\": 1, \"N4\": 1}, 0.2]\n",
      "\"N2\"\t[{\"N3\": 1, \"N5\": 1}, 0.2]\n",
      "\"N3\"\t[{\"N4\": 1}, 0.2]\n",
      "\"N4\"\t[{\"N5\": 1}, 0.2]\n",
      "\"N5\"\t[{\"N1\": 1, \"N2\": 1, \"N3\": 1}, 0.2]\n",
      "Removing temp directory /tmp/PageRankInit.hadoop.20160713.043551.333962...\n"
     ]
    }
   ],
   "source": [
    "!python PageRankInit.py Lecture9_10_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "import ast\n",
    "import json\n",
    "\n",
    "# This class implements the Single Source Shortest Path (SSSP) breadth-first search\n",
    "# algorithm for an unweighted graph in MapReduce.  The functions of the mapper and reducer\n",
    "# are explained below.  Once a node is put in to the V state its shortest path has been found\n",
    "# (this is not true for a weighted graph)\n",
    "class MRPageRank(MRJob):\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option to capture the stop_index for the shortest path\n",
    "        super(MRPageRank, self).configure_options()\n",
    "        self.add_passthrough_option('--damping_factor', type='int', default=0.15)\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRPageRank, self).__init__(*args, **kwargs)\n",
    "        self.damping_factor = self.options.damping_factor\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        # If a node is in the Q state then yield 2 things.  1.  The adjacency list, the\n",
    "        # minimum distance and the shortest path plus its own node name as this is the shortest\n",
    "        # path that will be found.  2.  All of the nodes in its adj_list each with the following:\n",
    "        # An empty adjacency list, the current min_dist plus the distance to this node, the\n",
    "        # current shortest path plus the path to this node, state of Q.\n",
    "        node_num, data = line.strip().split('\\t')\n",
    "        data = json.loads(data)\n",
    "        node_num = node_num.strip('\"')\n",
    "        adj_dict = data[0]\n",
    "        degree = float(len(adj_dict))\n",
    "        page_rank = data[1]\n",
    "        # Yield the node to preserve the graph structure\n",
    "        yield node_num, tuple((adj_dict, 0.0))\n",
    "        for node, value in adj_dict.iteritems():\n",
    "            yield node, tuple(({}, page_rank/degree))\n",
    "    \n",
    "    # For each element in the data list, do the following:\n",
    "    # - Use the adjacency list for a node that is not {}, there will always be one.\n",
    "    # - Find the minimum distance of all of the elements of the list, use the shortest\n",
    "    # path and state from this element as well.\n",
    "    # If an element is in the Q state and there is no_stop_index, then increment the\n",
    "    # 'Number_In_Q' counter\n",
    "    # If an element is in the V state (meaning its shortest path has already been found) and\n",
    "    # it is the stopping index, then increment the 'Stop_Index_Found' counter.\n",
    "    def reducer(self, node, data):\n",
    "        f_adj_dict = Counter()\n",
    "        f_page_rank = 0.0\n",
    "        for adj_dict, page_rank in data:\n",
    "            #print adj_dict, page_rank\n",
    "            if adj_dict != {} and f_adj_dict == {}:\n",
    "                f_adj_dict = adj_dict\n",
    "            f_page_rank += float(page_rank)\n",
    "        yield node, tuple((f_adj_dict, f_page_rank))\n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRPageRank.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_criterion_reached(old_page_ranks, page_ranks, epsilon):\n",
    "    Stop = False\n",
    "    total_error = 0.0\n",
    "    for pr1, pr2 in zip(old_page_ranks, page_ranks):\n",
    "        total_error += abs(pr1-pr2)\n",
    "    print total_error\n",
    "    if total_error < epsilon:\n",
    "        Stop = True\n",
    "    return Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.06666666666666667]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.16666666666666669]\n",
      "N3 [{u'N4': 1}, 0.16666666666666669]\n",
      "N4 [{u'N5': 1}, 0.30000000000000004]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.30000000000000004]\n",
      "0.4\n",
      "Iteration 1\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10000000000000002]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.13333333333333336]\n",
      "N3 [{u'N4': 1}, 0.18333333333333335]\n",
      "N4 [{u'N5': 1}, 0.2]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.3833333333333334]\n",
      "0.266666666667\n",
      "Iteration 2\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.1277777777777778]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1777777777777778]\n",
      "N3 [{u'N4': 1}, 0.19444444444444448]\n",
      "N4 [{u'N5': 1}, 0.23333333333333336]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.2666666666666667]\n",
      "0.233333333333\n",
      "Iteration 3\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.0888888888888889]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1527777777777778]\n",
      "N3 [{u'N4': 1}, 0.1777777777777778]\n",
      "N4 [{u'N5': 1}, 0.25833333333333336]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.3222222222222223]\n",
      "0.161111111111\n",
      "Iteration 4\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10740740740740744]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1518518518518519]\n",
      "N3 [{u'N4': 1}, 0.18379629629629635]\n",
      "N4 [{u'N5': 1}, 0.22222222222222227]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.33472222222222225]\n",
      "0.0740740740741\n",
      "Iteration 5\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.11157407407407409]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1652777777777778]\n",
      "N3 [{u'N4': 1}, 0.18750000000000006]\n",
      "N4 [{u'N5': 1}, 0.23750000000000007]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.2981481481481482]\n",
      "0.0731481481481\n",
      "Iteration 6\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.09938271604938274]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15516975308641978]\n",
      "N3 [{u'N4': 1}, 0.18202160493827163]\n",
      "N4 [{u'N5': 1}, 0.2432870370370371]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.320138888888889]\n",
      "0.0555555555556\n",
      "Iteration 7\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.106712962962963]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15640432098765436]\n",
      "N3 [{u'N4': 1}, 0.1842978395061729]\n",
      "N4 [{u'N5': 1}, 0.231712962962963]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.320871913580247]\n",
      "0.0231481481481\n",
      "Iteration 8\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10695730452674901]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1603137860082305]\n",
      "N3 [{u'N4': 1}, 0.18515946502057617]\n",
      "N4 [{u'N5': 1}, 0.2376543209876544]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.3099151234567902]\n",
      "0.0219135802469\n",
      "Iteration 9\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.1033050411522634]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15678369341563791]\n",
      "N3 [{u'N4': 1}, 0.18346193415637865]\n",
      "N4 [{u'N5': 1}, 0.23863811728395068]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31781121399176965]\n",
      "0.0177597736626\n",
      "Iteration 10\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10593707133058988]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1575895919067216]\n",
      "N3 [{u'N4': 1}, 0.18432891803840884]\n",
      "N4 [{u'N5': 1}, 0.23511445473251036]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31702996399176964]\n",
      "0.00860982510288\n",
      "Iteration 11\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10567665466392322]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15864519032921814]\n",
      "N3 [{u'N4': 1}, 0.184471450617284]\n",
      "N4 [{u'N5': 1}, 0.23729745370370378]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31390925068587117]\n",
      "0.00676225994513\n",
      "Iteration 12\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.1046364168952904]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.157474744227252]\n",
      "N3 [{u'N4': 1}, 0.18395901205989945]\n",
      "N4 [{u'N5': 1}, 0.23730977794924563]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31662004886831285]\n",
      "0.00544624485597\n",
      "Iteration 13\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10554001628943761]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.1578582247370828]\n",
      "N3 [{u'N4': 1}, 0.18427738840306362]\n",
      "N4 [{u'N5': 1}, 0.23627722050754466]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31604715006287165]\n",
      "0.00321091249428\n",
      "Iteration 14\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10534905002095722]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15811905816567604]\n",
      "N3 [{u'N4': 1}, 0.18427816238949862]\n",
      "N4 [{u'N5': 1}, 0.23704739654778242]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31520633287608607]\n",
      "0.00206356691053\n",
      "Iteration 15\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10506877762536203]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15774330263584063]\n",
      "N3 [{u'N4': 1}, 0.18412830670820005]\n",
      "N4 [{u'N5': 1}, 0.23695268739997724]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31610692563062043]\n",
      "0.00180118550907\n",
      "Iteration 16\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10536897521020681]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15790336402288782]\n",
      "N3 [{u'N4': 1}, 0.18424062652812712]\n",
      "N4 [{u'N5': 1}, 0.23666269552088107]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31582433871789756]\n",
      "0.00114515758364\n",
      "Iteration 17\n",
      "N1 [{u'N2': 1, u'N4': 1}, 0.10527477957263252]\n",
      "N2 [{u'N3': 1, u'N5': 1}, 0.15795926717773592]\n",
      "N3 [{u'N4': 1}, 0.18422646158407643]\n",
      "N4 [{u'N5': 1}, 0.23692511413323053]\n",
      "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31561437753232496]\n",
      "0.000636643534395\n"
     ]
    }
   ],
   "source": [
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "#from PageRank import MRPageRank\n",
    "#from PageRankInit import MRPageRankInit\n",
    "import PageRank\n",
    "import PageRankInit\n",
    "reload(PageRank)\n",
    "reload(PageRankInit)\n",
    "import json\n",
    "\n",
    "# Set the name of the file that gets passed from iteration to iteration\n",
    "work_filename = 'work_table.txt'\n",
    "\n",
    "# Initialize the work table file\n",
    "page_ranks = []\n",
    "mr_job = PageRankInit.MRPageRankInit(args=['Lecture9_10_test.txt'])\n",
    "with mr_job.make_runner() as runner, open(work_filename, 'w') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key, value =  mr_job.parse_output_line(line)\n",
    "        page_ranks.append(value[1])\n",
    "        f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "\n",
    "# Run the SSSP MRJob\n",
    "#stop_index = 'no_stop_index'\n",
    "damping_factor = 0.15\n",
    "epsilon = 0.001\n",
    "#mr_job = MRPageRank(args=[work_filename, '--damping_factor', str(damping_factor)])\n",
    "mr_job = PageRank.MRPageRank(args=[work_filename])\n",
    "    \n",
    "# Update work table file iteratively\n",
    "i = 0\n",
    "# Set stop condition to False\n",
    "Stop = False\n",
    "while(Stop == False):\n",
    "#while(i < 10):\n",
    "    work_table = {}\n",
    "    old_page_ranks = page_ranks\n",
    "    page_ranks = []\n",
    "    # Print the iteration number\n",
    "    print('Iteration %d' % (i))\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            work_table[key] = value\n",
    "            page_ranks.append(value[1])\n",
    "        \n",
    "        # Update work_table for the next iteration\n",
    "        with open(work_filename, 'w') as f:\n",
    "            for key, value in work_table.iteritems():\n",
    "                f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "        \n",
    "        Stop = stop_criterion_reached(old_page_ranks, page_ranks, epsilon)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(0.10530158257403283+0.1579039788294203+0.18422267372257844+0.23678682235503043+0.3157849425189383)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.1 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.2: Exploring PageRank teleportation and network plots </h2>\n",
    "\n",
    "* In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "* Using the graph in HW1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha, so that each nodes radius is proportional to its PageRank score. \n",
    "* In particular you should do this for the following damping factors: [0,0.25,0.5,0.75, 0.85, 1]. \n",
    "* Note your plots should look like the following: https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.2 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.2 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.3: Applying PageRank to the Wikipedia hyperlinks network </h2>\n",
    "\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 5 iterations, and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15).\n",
    "* Have the top 100 ranked pages changed? Comment on your findings. \n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.3 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"color:darkgreen\">  HW 9.3 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.4: Topic-specific PageRank implementation using MRJob </h2>\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation, as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.   \n",
    "This caveat lies in footnote 3 on page 3:\n",
    "```\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "```\n",
    "and must be adhered to for convergence to be guaranteed.   \n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors (one for the vanilla PageRank implementation in 9.1, and one for each topic with the topic specific implementation). Print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the requirements for irreducibility with topic-specific PageRank. In particular, the literature ensures irreducibility by requiring that nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, please comment on why the literature's method is difficult to implement, and what what extra computation it will require.   \n",
    "\n",
    "Then for your code, please use the alternative, non-uniform damping vector:\n",
    "\n",
    "```\n",
    "vji = beta*(1/|Tj|); if node i lies in topic Tj\n",
    "\n",
    "vji = (1-beta)*(1/(N - |Tj|)); if node i lies outside of topic Tj\n",
    "```\n",
    "for beta in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes. If beta > 0.5, PageRank is topic-sensitive, and if beta < 0.5, the PageRank is anti-topic-sensitive. For any value of beta irreducibility should hold, so please try beta=0.99, and perhaps some other values locally, on the smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.4 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.4 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h3 style='color:darkblue'>---------  OPTIONAL QUESTIONS SECTION --------</h3></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.5: (OPTIONAL) Applying topic-specific PageRank to Wikipedia</h2>\n",
    "\n",
    "Here you will apply your topic-specific PageRank implementation to Wikipedia, defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10, so that there are 10 topics. \n",
    "\n",
    "* Once again, print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses. Run for 10 iterations.\n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run in HW 9.3. \n",
    "* Then plot the pagerank values for the same 100 pages that result from the topic specific pagerank after 10 iterations run. \n",
    "* Comment on your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.5 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.5 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.6:  (OPTIONAL) TextRank</h2>\n",
    "\n",
    "* What is TextRank? Describe the main steps in the algorithm. Why does TextRank work?\n",
    "* Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):\n",
    "```\n",
    "\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" \n",
    "```\n",
    "* The extracted keywords should in the following set:\n",
    "```\n",
    "linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.6 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.6 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HWK 9 --------</h2></div></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
