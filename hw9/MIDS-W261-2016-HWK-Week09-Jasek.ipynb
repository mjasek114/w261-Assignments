{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261 - Machine Learning At Scale\n",
    "## Assignment - Week 09\n",
    "---\n",
    "\n",
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Date:**  7/17/16\n",
    "\n",
    "---\n",
    "### Instructions\n",
    "\n",
    "Due by 07/17/2016\n",
    "\n",
    "[Submission Link - Google Form](https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form) \n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "### Useful References\n",
    "\n",
    "* Data-intensive text processing with MapReduce. San Rafael, CA: Morgan & Claypool Publishers. Chapter 5. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\">HW 9 Dataset</h2>\n",
    "\n",
    "Note that all referenced files life in the enclosing directory. [Checkout the Data subdirectory on Dropbox](https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0) or the AWS S3 buckets (details contained each question). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.0: Short answer questions </h2>\n",
    "\n",
    "__ What is PageRank and what is it used for in the context of web search?__\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the Steady State Distibution? __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ OPTIONAL: In topic-specific pagerank, how can we ensure that the irreducible property is satifsied? (HINT: see HW9.4) __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.1: MRJob implementation of basic PageRank </h2>\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d,one of the hyperlinks in the current page. This probability is represented by a so-called *damping factor* d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "> Dropbox: https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the [Wikipedia article](https://en.wikipedia.org/wiki/PageRank)\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "<pre>\n",
    "\n",
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.1 Implementation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test file:  PageRank-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PageRank-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank-test.txt\n",
    "B\t{'C': 1}\n",
    "C\t{'B': 1}\n",
    "D\t{'A': 1, 'B': 1}\n",
    "E\t{'D': 1, 'B': 1, 'F': 1}\n",
    "F\t{'B': 1, 'E': 1}\n",
    "G\t{'B': 1, 'E': 1}\n",
    "H\t{'B': 1, 'E': 1}\n",
    "I\t{'B': 1, 'E': 1}\n",
    "J\t{'E': 1}\n",
    "K\t{'E': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output from PageRank-test.txt\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "\n",
    "A, 0.033  \n",
    "B, 0.384  \n",
    "C, 0.343  \n",
    "D, 0.039  \n",
    "E, 0.081  \n",
    "F, 0.039  \n",
    "G, 0.016  \n",
    "H, 0.016  \n",
    "I, 0.016  \n",
    "J, 0.016  \n",
    "K, 0.016  \n",
    "\n",
    "0.999 = 0.033+0.384+0.343+0.039+0.081+0.039+0.016+0.016+0.016+0.016+0.016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test file:  Lecture9_10_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Lecture9_10_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile Lecture9_10_test.txt\n",
    "N1\t{'N2': 1, 'N4': 1}\n",
    "N2\t{'N3': 1, 'N5': 1}\n",
    "N3\t{'N4': 1}\n",
    "N4\t{'N5': 1}\n",
    "N5\t{'N1': 1, 'N2': 1, 'N3': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output from Lecture9_10_test.txt\n",
    "\n",
    "Iteration 10  \n",
    "N1 [{u'N2': 1, u'N4': 1}, 0.10593707133058988]  \n",
    "N2 [{u'N3': 1, u'N5': 1}, 0.1575895919067216]  \n",
    "N3 [{u'N4': 1}, 0.18432891803840884]  \n",
    "N4 [{u'N5': 1}, 0.23511445473251036]  \n",
    "N5 [{u'N1': 1, u'N2': 1, u'N3': 1}, 0.31702996399176964]  \n",
    "Total Error:  0.00860982510288  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test file:  Lecture9_10d_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Lecture9_10d_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile Lecture9_10d_test.txt\n",
    "N1\t{'N2': 1, 'N4': 1}\n",
    "N2\t{'N3': 1, 'N5': 1}\n",
    "N3\t{'N4': 1}\n",
    "N4\t{'N5': 1, 'N6': 1}\n",
    "N5\t{'N1': 1, 'N2': 1, 'N3': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankInit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankInit.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "import json\n",
    "\n",
    "# This class takes an adjacency list as input and outputs and initialized work file with \n",
    "# the following format:\n",
    "# node_number \\t [adjacency_list, infinity, '', 'U']\n",
    "# One starting node is defined as an argument passed in to the class.  The default is node '1'.\n",
    "# That starting node has the format:\n",
    "# node_number \\t [adjacency_list, 0.0, '', 'Q']\n",
    "# U means Unvisited\n",
    "# Q means Queued in the queue\n",
    "class MRPageRankInit(MRJob):\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option to capture the stop_index for the shortest path\n",
    "        super(MRPageRankInit, self).configure_options()\n",
    "        self.add_passthrough_option('--platform', type='str', default='hadoop')\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRPageRankInit, self).__init__(*args, **kwargs)\n",
    "        self.total_nodes_calculated = False\n",
    "        self.total_nodes = 0\n",
    "\n",
    "    # For the node equal to the start index, yield\n",
    "    #    node_number \\t tuple(adj_list, 0.0, \"\", 'Q')\n",
    "    # For all other nodes, yield\n",
    "    #    node_number \\t tuple(adj_list, infinity, \"\", 'U')    \n",
    "    def mapper(self, _, line):\n",
    "        node_num, adj_dict = line.strip().split('\\t')\n",
    "        adj_dict = ast.literal_eval(adj_dict)\n",
    "        self.total_nodes += 1\n",
    "        yield node_num, tuple((adj_dict, 1.0))\n",
    "        yield '*' + node_num, 1\n",
    "        for node in adj_dict:\n",
    "            yield node, tuple(({}, 1.0))\n",
    "            yield '*' + node, 1\n",
    "\n",
    "    def reducer(self, node_num, data):\n",
    "        if node_num[0] == '*':\n",
    "            self.total_nodes += 1\n",
    "        else:\n",
    "            if self.total_nodes_calculated == False:\n",
    "                yield '*total_nodes', self.total_nodes\n",
    "                self.total_nodes_calculated = True\n",
    "            f_adj_dict = {}\n",
    "            for adj_dict, value in data:\n",
    "                if adj_dict != {} and f_adj_dict == {}:\n",
    "                    f_adj_dict = adj_dict\n",
    "            print node_num, tuple((f_adj_dict, 1.0/self.total_nodes))\n",
    "            yield node_num, tuple((f_adj_dict, 1.0/self.total_nodes))\n",
    "        \n",
    "    # Create the steps for the MRJob.  There is only a mapper in this job.\n",
    "    # Run with 1 reducer when not running in Hadoop\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP_LOCAL = {\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        if self.options.platform == 'local':\n",
    "            JOBCONF_STEP = JOBCONF_STEP_LOCAL\n",
    "        else:\n",
    "            JOBCONF_STEP = {}\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRPageRankInit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/PageRankInit.hadoop.20160714.065856.164552\n",
      "Running step 1 of 1...\n",
      "N1 ({u'N2': 1, u'N4': 1}, 0.16666666666666666)\n",
      "N2 ({u'N3': 1, u'N5': 1}, 0.16666666666666666)\n",
      "N3 ({u'N4': 1}, 0.16666666666666666)\n",
      "N4 ({u'N5': 1, u'N6': 1}, 0.16666666666666666)\n",
      "N5 ({u'N1': 1, u'N2': 1, u'N3': 1}, 0.16666666666666666)\n",
      "N6 ({}, 0.16666666666666666)\n",
      "Streaming final output from /tmp/PageRankInit.hadoop.20160714.065856.164552/output...\n",
      "\"*total_nodes\"\t6\n",
      "\"N1\"\t[{\"N2\": 1, \"N4\": 1}, 0.16666666666666666]\n",
      "\"N2\"\t[{\"N3\": 1, \"N5\": 1}, 0.16666666666666666]\n",
      "\"N3\"\t[{\"N4\": 1}, 0.16666666666666666]\n",
      "\"N4\"\t[{\"N5\": 1, \"N6\": 1}, 0.16666666666666666]\n",
      "\"N5\"\t[{\"N1\": 1, \"N2\": 1, \"N3\": 1}, 0.16666666666666666]\n",
      "\"N6\"\t[{}, 0.16666666666666666]\n",
      "Removing temp directory /tmp/PageRankInit.hadoop.20160714.065856.164552...\n"
     ]
    }
   ],
   "source": [
    "#!python PageRankInit.py Lecture9_10_test.txt '--platform=local'\n",
    "#!python PageRankInit.py PageRank-test.txt '--platform=local'\n",
    "!python PageRankInit.py Lecture9_10d_test.txt '--platform=local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankStep1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankStep1.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "\n",
    "class MRPageRankStep1(MRJob):\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option to capture the stop_index for the shortest path\n",
    "        super(MRPageRankStep1, self).configure_options()\n",
    "        self.add_passthrough_option('--num_converter', type='int', default=1)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRPageRankStep1, self).__init__(*args, **kwargs)\n",
    "        self.total_page_rank_dangling = 0.0\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        node_num, data = line.strip().split('\\t')\n",
    "        node_num = node_num.strip('\"')\n",
    "        #print node_num, data\n",
    "        if node_num != '*total_nodes':\n",
    "            data = json.loads(data)\n",
    "            adj_dict = data[0]\n",
    "            degree = float(len(adj_dict))\n",
    "            page_rank = data[1]\n",
    "            # If the node is a dangling node, then add its page rank to the dangling mass count\n",
    "            if adj_dict == {}:\n",
    "                self.total_page_rank_dangling += page_rank\n",
    "            # Yield the node to preserve the graph structure\n",
    "            yield node_num, tuple((adj_dict, 0.0))\n",
    "            for node, value in adj_dict.iteritems():\n",
    "                yield node, tuple(({}, page_rank/degree))\n",
    "    \n",
    "    def mapper_final(self):\n",
    "        #print self.options.num_converter\n",
    "        yield '*total_dangling', tuple(({}, self.total_page_rank_dangling))\n",
    "    \n",
    "    def reducer(self, node, data):\n",
    "        #print node\n",
    "        #yield node, None\n",
    "        if node == '*total_dangling':\n",
    "            for adj_dict, value in data:\n",
    "                self.total_page_rank_dangling += float(value)\n",
    "            #print 'dm: ', self.total_page_rank_dangling\n",
    "            self.increment_counter('PageRank_Counters', 'Dangling_Mass', \n",
    "                                   int(self.options.num_converter*self.total_page_rank_dangling))\n",
    "        else:\n",
    "            f_adj_dict = {}\n",
    "            f_page_rank = 0.0\n",
    "            for adj_dict, page_rank in data:\n",
    "                if adj_dict != {} and f_adj_dict == {}:\n",
    "                    f_adj_dict = adj_dict\n",
    "                f_page_rank += float(page_rank)\n",
    "            yield node, tuple((f_adj_dict, f_page_rank))\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, mapper_final=self.mapper_final, reducer=self.reducer)\n",
    "            #MRStep(mapper=self.mapper, mapper_final=self.mapper_final)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRPageRankStep1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PageRankStep1 locally and in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/PageRankStep1.hadoop.20160715.190342.820443\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/PageRankStep1.hadoop.20160715.190342.820443/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Running step 1 of 1...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar747606793857688514/] [] /tmp/streamjob7982153103817775143.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1468607945031_0006\n",
      "  Submitted application application_1468607945031_0006\n",
      "  The url to track the job: http://master:8088/proxy/application_1468607945031_0006/\n",
      "  Running job: job_1468607945031_0006\n",
      "  Job job_1468607945031_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1468607945031_0006 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/PageRankStep1.hadoop.20160715.190342.820443/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=696\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=931\n",
      "\t\tFILE: Number of bytes written=362611\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=910\n",
      "\t\tHDFS: Number of bytes written=368\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11873280\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3726336\n",
      "\t\tTotal time spent by all map tasks (ms)=11595\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11595\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3639\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3639\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11595\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3639\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2020\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=366\n",
      "\t\tInput split bytes=214\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=865\n",
      "\t\tMap output materialized bytes=937\n",
      "\t\tMap output records=30\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=645873664\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=937\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=60\n",
      "\t\tTotal committed heap usage (bytes)=514850816\n",
      "\t\tVirtual memory (bytes) snapshot=6289022976\n",
      "\tPageRank_Counters\n",
      "\t\tDangling_Mass=909090909\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/PageRankStep1.hadoop.20160715.190342.820443/output...\n",
      "\"A\"\t[{}, 0.045454545454545456]\n",
      "\"B\"\t[{\"C\": 1}, 0.3484848484848485]\n",
      "\"C\"\t[{\"B\": 1}, 0.09090909090909091]\n",
      "\"D\"\t[{\"A\": 1, \"B\": 1}, 0.030303030303030304]\n",
      "\"E\"\t[{\"B\": 1, \"D\": 1, \"F\": 1}, 0.36363636363636365]\n",
      "\"F\"\t[{\"B\": 1, \"E\": 1}, 0.030303030303030304]\n",
      "\"G\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"H\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"I\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"J\"\t[{\"E\": 1}, 0.0]\n",
      "\"K\"\t[{\"E\": 1}, 0.0]\n",
      "STDERR: 16/07/15 14:04:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/PageRankStep1.hadoop.20160715.190342.820443...\n",
      "Removing temp directory /tmp/PageRankStep1.hadoop.20160715.190342.820443...\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "total_nodes = 11.0\n",
    "#total_nodes = 15192277.0\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "num_converter = 10000000000\n",
    "#!hdfs dfs -rm -r /user/hadoop/HW9Output/PageRank1\n",
    "#!hdfs dfs -rm -r /user/hadoop/HW9Input/PageRank1\n",
    "#!hdfs dfs -cp /user/hadoop/HW9Results/PageRankInit_test /user/hadoop/HW9Input/PageRank1\n",
    "#!hdfs dfs -copyToLocal /user/hadoop/HW9Input/PageRank1/part-00000 pagerank1_test.txt\n",
    "#!python PageRankInit.py Lecture9_10d_test.txt '--platform=local'\n",
    "#!python PageRankStep1.py pagerank1_test.txt '--num_converter=10000000000'\n",
    "!python PageRankStep1.py hdfs:///user/hadoop/HW9Input/PageRank1 -r hadoop '--num_converter=10000000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRankStep2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRankStep2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "\n",
    "class MRPageRankStep2(MRJob):\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option to capture the stop_index for the shortest path\n",
    "        super(MRPageRankStep2, self).configure_options()\n",
    "        self.add_passthrough_option('--damping_factor', type='float', default=0.15)\n",
    "        self.add_passthrough_option('--total_nodes', type='float', default=1.0)\n",
    "        self.add_passthrough_option('--dangling_mass', type='float', default=0.0)\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        node_num, data = line.strip().split('\\t')\n",
    "        node_num = node_num.strip('\"')\n",
    "        #print node_num, data\n",
    "        if node_num != '*total_nodes':\n",
    "            data = json.loads(data)\n",
    "            adj_dict = data[0]\n",
    "            page_rank = data[1]\n",
    "            f_page_rank = (1-self.options.damping_factor)*(1.0/self.options.total_nodes)+ \\\n",
    "                self.options.damping_factor*((self.options.dangling_mass/self.options.total_nodes)+page_rank)\n",
    "            #print node_num, tuple((adj_dict, f_page_rank))                    \n",
    "            yield node_num, tuple((adj_dict, f_page_rank))\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRPageRankStep2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ({}, 0.09090909090909091)\n",
      "B ({u'C': 1}, 0.09090909090909091)\n",
      "C ({u'B': 1}, 0.09090909090909091)\n",
      "D ({u'A': 1, u'B': 1}, 0.09090909090909091)\n",
      "E ({u'B': 1, u'D': 1, u'F': 1}, 0.09090909090909091)\n",
      "F ({u'B': 1, u'E': 1}, 0.09090909090909091)\n",
      "G ({u'B': 1, u'E': 1}, 0.09090909090909091)\n",
      "H ({u'B': 1, u'E': 1}, 0.09090909090909091)\n",
      "I ({u'B': 1, u'E': 1}, 0.09090909090909091)\n",
      "J ({u'E': 1}, 0.09090909090909091)\n",
      "K ({u'E': 1}, 0.09090909090909091)\n",
      "dm 0.0909090909\n",
      "dm 0.0592975206\n",
      "dm 0.0379464062\n",
      "dm 0.0640190695\n",
      "dm 0.0375959647\n",
      "dm 0.0386749363\n",
      "dm 0.0341177257\n",
      "dm 0.0346526855\n",
      "dm 0.0332641479\n",
      "dm 0.0332687067\n",
      "dm 0.0329301017\n",
      "dm 0.0329194443\n",
      "dm 0.0328282893\n",
      "dm 0.0328197384\n",
      "dm 0.0327957341\n",
      "dm 0.0327922737\n",
      "dm 0.0327858041\n",
      "dm 0.0327845371\n",
      "dm 0.0327827832\n",
      "dm 0.0327823577\n",
      "dm 0.032781877\n",
      "dm 0.0327817395\n",
      "dm 0.0327816067\n",
      "dm 0.0327815636\n",
      "dm 0.0327815266\n",
      "dm 0.0327815133\n",
      "dm 0.0327815029\n",
      "dm 0.0327814989\n",
      "dm 0.032781496\n",
      "dm 0.0327814948\n"
     ]
    }
   ],
   "source": [
    "import PageRankStep1\n",
    "import PageRankStep2\n",
    "import PageRankInit\n",
    "reload(PageRankStep1)\n",
    "reload(PageRankStep2)\n",
    "reload(PageRankInit)\n",
    "import json\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "#damping_factor = 1.0\n",
    "epsilon = 0.01\n",
    "iterations = 30\n",
    "num_converter = 10000000000\n",
    "\n",
    "# Set the name of the file that gets passed from iteration to iteration\n",
    "work_filename = 'work_table.txt'\n",
    "\n",
    "# Initialize a list ot store the page_ranks from iteration to iteration\n",
    "page_ranks = []\n",
    "# Call the PageRankInit MRJob\n",
    "#mr_job = PageRankInit.MRPageRankInit(args=['Lecture9_10_test.txt', '--platform', 'local'])\n",
    "#mr_job = PageRankInit.MRPageRankInit(args=['Lecture9_10d_test.txt', '--platform', 'local'])\n",
    "mr_job = PageRankInit.MRPageRankInit(args=['PageRank-test.txt', '--platform', 'local'])\n",
    "with mr_job.make_runner() as runner, open(work_filename, 'w') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key, value =  mr_job.parse_output_line(line)\n",
    "        # Store the total_nodes\n",
    "        if key == '*total_nodes':\n",
    "            total_nodes = float(value)\n",
    "        else:\n",
    "            page_ranks.append(value[1])\n",
    "        f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "\n",
    "work_filename_1 = 'work_table_1.txt'\n",
    "work_filename_2 = 'work_table_2.txt'\n",
    "i = 0\n",
    "while(i < iterations):\n",
    "    # Run the PageRankStep1 MRJob\n",
    "    mr_job = PageRankStep1.MRPageRankStep1(args=[work_filename, '--num_converter', str(num_converter)])\n",
    "    with mr_job.make_runner() as runner, open(work_filename_1, 'w') as f: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job.parse_output_line(line)\n",
    "            f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "    dangling_mass = runner.counters()[0]['PageRank_Counters']['Dangling_Mass']/float(num_converter)\n",
    "    print 'dm', dangling_mass\n",
    "\n",
    "    # Run the PageRankStep2 MRJob\n",
    "    mr_job = PageRankStep2.MRPageRankStep2(args=[work_filename_1, '--damping_factor', damping_factor, '--total_nodes',\n",
    "                                                total_nodes, '--dangling_mass', dangling_mass])\n",
    "    with mr_job.make_runner() as runner, open(work_filename_2, 'w') as f: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job.parse_output_line(line)\n",
    "            f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "\n",
    "    !mv $work_filename_2 $work_filename\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/14 14:06:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#### Create HDFS directories\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW9Output\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW9Input\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW9Results\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW9Data\n",
    "#!hdfs dfs -copyFromLocal PageRank-test.txt /user/hadoop/HW9Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PageRankInit with HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/14 01:18:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/HW9Results/PageRankInit_test': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/14 01:19:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes:  11\n"
     ]
    }
   ],
   "source": [
    "import PageRank\n",
    "import PageRankInit\n",
    "reload(PageRank)\n",
    "reload(PageRankInit)\n",
    "import json\n",
    "\n",
    "def get_total_nodes(ofile):\n",
    "    ls_output = !hdfs dfs -ls $ofile\n",
    "    num_output_files = len(ls_output)-3\n",
    "    for i in range(num_output_files):\n",
    "        snum = str(i)\n",
    "        zeros = '0'*(5-len(snum))\n",
    "        rfile = ofile+'/part-'+zeros+snum\n",
    "        grep_output = !hdfs dfs -cat $rfile | grep '*total_nodes'\n",
    "        if len(grep_output) == 2:\n",
    "            _, total_nodes = grep_output[1].strip().split('\\t')\n",
    "            break\n",
    "    return total_nodes\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "#damping_factor = 1.0\n",
    "epsilon = 0.01\n",
    "\n",
    "# Set the name of the file that gets passed from iteration to iteration\n",
    "work_filename = 'work_table.txt'\n",
    "\n",
    "# Initialize a list ot store the page_ranks from iteration to iteration\n",
    "page_ranks = []\n",
    "# Call the PageRankInit MRJob\n",
    "ifile = 'hdfs:///user/hadoop/HW9Data/PageRank-test.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/HW7data/all-pages-indexed-out.txt'\n",
    "ofile ='HW9Results/PageRankInit_test'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "mr_job = PageRankInit.MRPageRankInit(args=[ifile, '-r', 'hadoop', '--output-dir', ofile])\n",
    "with mr_job.make_runner() as runner, open(work_filename, 'w') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key, value =  mr_job.parse_output_line(line)\n",
    "        # Store the total_nodes\n",
    "        if key == '*total_nodes':\n",
    "            total_nodes = float(value)\n",
    "        else:\n",
    "            page_ranks.append(value[1])\n",
    "        f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "\n",
    "total_nodes = get_total_nodes('/user/hadoop/'+ofile)\n",
    "print 'Total nodes: ', total_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/13 23:39:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-07-13 23:38 /user/hadoop/HW9Output/PageRankInit/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        464 2016-07-13 23:38 /user/hadoop/HW9Output/PageRankInit/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hadoop/$ofile\n",
    "#!hdfs dfs -cat /user/hadoop/$ofile/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a nohup job to start the Hadoop job in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting start_job_PageRankInit.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile start_job_PageRankInit.sh\n",
    "#!/bin/bash\n",
    "hdfs dfs -rm -r /user/hadoop/HW9Output/PageRankInit\n",
    "nohup python PageRankInit.py -r hadoop hdfs:///user/hadoop/HW7data/all-pages-indexed-out.txt --output-dir=HW9Output/PageRankInit --no-output > nohup/PageRankInit.out &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x start_job_PageRankInit.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/14 01:08:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/HW9Output/PageRankInit': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!./start_job_PageRankInit.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/14 09:29:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"*total_nodes\"\t15192277\n",
      "\"1\"\t[{}, 6.582291778908455e-08]\n",
      "\"10\"\t[{\"12\": 3, \"14478968\": 1, \"14965909\": 1, \"5172381\": 2, \"3403886\": 1, \"8031052\": 1, \"6857278\": 2, \"9416257\": 1, \"14481011\": 1, \"12336458\": 1, \"994890\": 1}, 6.582291778908455e-08]\n",
      "\"100\"\t[{}, 6.582291778908455e-08]\n",
      "\"1000\"\t[{}, 6.582291778908455e-08]\n",
      "\"10000\"\t[{}, 6.582291778908455e-08]\n",
      "\"100000\"\t[{\"100001\": 1}, 6.582291778908455e-08]\n",
      "\"1000000\"\t[{\"1000001\": 1}, 6.582291778908455e-08]\n",
      "\"10000000\"\t[{}, 6.582291778908455e-08]\n",
      "\"10000001\"\t[{}, 6.582291778908455e-08]\n",
      "cat: Unable to write to output stream.\n",
      "16/07/14 09:29:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"*total_nodes\"\t11\n",
      "\"A\"\t[{}, 0.09090909090909091]\n",
      "\"B\"\t[{\"C\": 1}, 0.09090909090909091]\n",
      "\"C\"\t[{\"B\": 1}, 0.09090909090909091]\n",
      "\"D\"\t[{\"A\": 1, \"B\": 1}, 0.09090909090909091]\n",
      "\"E\"\t[{\"B\": 1, \"D\": 1, \"F\": 1}, 0.09090909090909091]\n",
      "\"F\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"G\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"H\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"I\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "['16/07/14 09:29:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable', '\"*total_nodes\"\\t15192277']\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -ls /user/hadoop/HW9Output/PageRankInit\n",
    "#!hdfs dfs -cp /user/hadoop/HW9Output/PageRankInit /user/hadoop/HW9Results/PageRankInit_wikipedia\n",
    "!hdfs dfs -cat /user/hadoop/HW9Results/PageRankInit_wikipedia/part-00000 | head\n",
    "!hdfs dfs -cat /user/hadoop/HW9Results/PageRankInit_test/part-00000 | head\n",
    "g = !hdfs dfs -cat /user/hadoop/HW9Results/PageRankInit_wikipedia/part-00000 | grep '*total_nodes'\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PageRank in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_nodes(ofile):\n",
    "    ls_output = !hdfs dfs -ls $ofile\n",
    "    num_output_files = len(ls_output)-3\n",
    "    for i in range(num_output_files):\n",
    "        snum = str(i)\n",
    "        zeros = '0'*(5-len(snum))\n",
    "        rfile = ofile+'/part-'+zeros+snum\n",
    "        grep_output = !hdfs dfs -cat $rfile | grep '*total_nodes'\n",
    "        if len(grep_output) == 2:\n",
    "            _, total_nodes = grep_output[1].strip().split('\\t')\n",
    "            break\n",
    "    return total_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Total Nodes from the wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_nodes = get_total_nodes('/user/hadoop/HW9Results/PageRankInit_wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15192277\n"
     ]
    }
   ],
   "source": [
    "print total_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PageRank Steps 1 and 2 in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/15 14:40:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:40:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\n"
     ]
    }
   ],
   "source": [
    "#hdfs dfs -ls /user/hadoop/HW9Input/PageRank1\n",
    "#!hdfs dfs -cat /user/hadoop/HW9Input/PageRank1/part-00000\n",
    "#!hdfs dfs -cat /user/hadoop/HW9Input/PageRank1/part-00001\n",
    "#!hdfs dfs -rm -r /user/hadoop/HW9Input/PageRank1\n",
    "\n",
    "#!hdfs dfs -mv /user/hadoop/HW9Output/PageRank2 /user/hadoop/HW9Input/PageRank1\n",
    "#!hdfs dfs -ls /user/hadoop/HW9Output/PageRank2\n",
    "#!hdfs dfs -cat /user/hadoop/HW9Output/PageRank2/part-00000\n",
    "#!hdfs dfs -cat /user/hadoop/HW9Output/PageRank2/part-00001\n",
    "\n",
    "#!hdfs dfs -rm -r /user/hadoop/HW9Output/PageRank1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/15 14:45:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:45:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\n",
      "16/07/15 14:45:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/HW9Output/PageRank2': No such file or directory\n",
      "16/07/15 14:45:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:46:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\n",
      "16/07/15 14:46:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:46:02 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/07/15 14:46:02 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:46:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{}, 0.045454545454545456]\n",
      "B [{u'C': 1}, 0.3484848484848485]\n",
      "C [{u'B': 1}, 0.09090909090909091]\n",
      "D [{u'A': 1, u'B': 1}, 0.030303030303030304]\n",
      "E [{u'B': 1, u'D': 1, u'F': 1}, 0.36363636363636365]\n",
      "F [{u'B': 1, u'E': 1}, 0.030303030303030304]\n",
      "G [{u'B': 1, u'E': 1}, 0.0]\n",
      "H [{u'B': 1, u'E': 1}, 0.0]\n",
      "I [{u'B': 1, u'E': 1}, 0.0]\n",
      "J [{u'E': 1}, 0.0]\n",
      "K [{u'E': 1}, 0.0]\n",
      "Dangling Mass: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0909090909\n",
      "A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:47:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{}, 0.059297520660454545]\n",
      "B [{u'C': 1}, 0.31687327823621214]\n",
      "C [{u'B': 1}, 0.09793388429681818]\n",
      "D [{u'A': 1, u'B': 1}, 0.046418732781666666]\n",
      "E [{u'B': 1, u'D': 1, u'F': 1}, 0.329752066115]\n",
      "F"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:47:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{u'B': 1, u'E': 1}, 0.046418732781666666]\n",
      "G [{u'B': 1, u'E': 1}, 0.020661157024090913]\n",
      "H [{u'B': 1, u'E': 1}, 0.020661157024090913]\n",
      "I [{u'B': 1, u'E': 1}, 0.020661157024090913]\n",
      "J [{u'E': 1}, 0.020661157024090913]\n",
      "K [{u'E': 1}, 0.020661157024090913]\n",
      "16/07/15 14:47:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:47:20 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\n",
      "16/07/15 14:47:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:47:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/15 14:47:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:48:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{}, 0.023209366390833333]\n",
      "B [{u'C': 1}, 0.28526170798628786]\n",
      "C [{u'B': 1}, 0.31687327823621214]\n",
      "D [{u'A': 1, u'B': 1}, 0.10991735537166668]\n",
      "E [{u'B': 1, u'D': 1, u'F': 1}, 0.09552341597515153]\n",
      "F [{u'B': 1, u'E': 1}, 0.10991735537166668]\n",
      "G [{u'B': 1, u'E': 1}, 0.0]\n",
      "H [{u'B': 1, u'E': 1}, 0.0]\n",
      "I [{u'B': 1, u'E': 1}, 0.0]\n",
      "J [{u'E': 1}, 0.0]\n",
      "K [{u'E': 1}, 0.0]\n",
      "Dangling Mass: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0592975206\n",
      "F"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:48:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{u'B': 1, u'E': 1}, 0.11164819683955304]\n",
      "G [{u'B': 1, u'E': 1}, 0.018218444773636367]\n",
      "H [{u'B': 1, u'E': 1}, 0.018218444773636367]\n",
      "I [{u'B': 1, u'E': 1}, 0.018218444773636367]\n",
      "J [{u'E': 1}, 0.018218444773636367]\n",
      "K [{u'E': 1}, 0.018218444773636367]\n",
      "A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/07/15 14:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{}, 0.0379464062058447]\n",
      "B [{u'C': 1}, 0.260690896561981]\n",
      "C [{u'B': 1}, 0.28756073127441667]\n",
      "D [{u'A': 1, u'B': 1}, 0.11164819683955304]\n",
      "E [{u'B': 1, u'D': 1, u'F': 1}, 0.09941334835251517]\n"
     ]
    }
   ],
   "source": [
    "import PageRankStep1\n",
    "import PageRankStep2\n",
    "import PageRankInit\n",
    "reload(PageRankStep1)\n",
    "reload(PageRankStep2)\n",
    "reload(PageRankInit)\n",
    "\n",
    "iterations = 2\n",
    "total_nodes = 11.0\n",
    "#total_nodes = 15192277.0\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "num_converter = 10000000000\n",
    "\n",
    "!hdfs dfs -rm -r /user/hadoop/HW9Output/PageRank1\n",
    "!hdfs dfs -rm -r /user/hadoop/HW9Output/PageRank2\n",
    "!hdfs dfs -rm -r /user/hadoop/HW9Input/PageRank1\n",
    "!hdfs dfs -cp /user/hadoop/HW9Results/PageRankInit_test /user/hadoop/HW9Input/PageRank1\n",
    "\n",
    "i = 0\n",
    "while(i < iterations):\n",
    "    print 'Iteration ', str(i)\n",
    "    # Run the PageRankStep1 MRJob\n",
    "    mr_job1 = PageRankStep1.MRPageRankStep1(args=['hdfs:///user/hadoop/HW9Input/PageRank1', '-r', 'hadoop', \n",
    "                                                  '--num_converter', str(num_converter), '--output-dir', 'HW9Output/PageRank1'])\n",
    "    with mr_job1.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job1.parse_output_line(line)\n",
    "            print key, value\n",
    "\n",
    "    dangling_mass = runner.counters()[0]['PageRank_Counters']['Dangling_Mass']/float(num_converter)\n",
    "    print 'Dangling Mass: ', dangling_mass\n",
    "\n",
    "    # Run the PageRankStep2 MRJob\n",
    "    mr_job2 = PageRankStep2.MRPageRankStep2(args=['hdfs:///user/hadoop/HW9Output/PageRank1', '-r', 'hadoop', \n",
    "                                                  '--damping_factor', str(damping_factor), '--total_nodes', \n",
    "                                                  str(total_nodes), '--dangling_mass', str(dangling_mass), \n",
    "                                                  '--output-dir', 'HW9Output/PageRank2'])\n",
    "    with mr_job2.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job2.parse_output_line(line)\n",
    "            print key, value\n",
    "    print 'Done with step 2'\n",
    "    \n",
    "    i += 1\n",
    "    if (i < iterations):\n",
    "        !hdfs dfs -rm -r /user/hadoop/HW9Input/PageRank1\n",
    "        !hdfs dfs -mv /user/hadoop/HW9Output/PageRank2 /user/hadoop/HW9Input/PageRank1\n",
    "        !hdfs dfs -rm -r /user/hadoop/HW9Output/PageRank1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/15 14:23:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"A\"\t[{}, 0.045454545454545456]\n",
      "\"B\"\t[{\"C\": 1}, 0.3484848484848485]\n",
      "\"C\"\t[{\"B\": 1}, 0.09090909090909091]\n",
      "\"D\"\t[{\"A\": 1, \"B\": 1}, 0.030303030303030304]\n",
      "\"E\"\t[{\"B\": 1, \"D\": 1, \"F\": 1}, 0.36363636363636365]\n",
      "\"F\"\t[{\"B\": 1, \"E\": 1}, 0.030303030303030304]\n",
      "\"G\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"H\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"I\"\t[{\"B\": 1, \"E\": 1}, 0.0]\n",
      "\"J\"\t[{\"E\": 1}, 0.0]\n",
      "\"K\"\t[{\"E\": 1}, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -ls /user/hadoop/HW9Output/PageRank1\n",
    "!hdfs dfs -cat /user/hadoop/HW9Output/PageRank1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_driver.py\n",
    "from PageRankStep1 import MRPageRankStep1\n",
    "from PageRankStep2 import MRPageRankStep2\n",
    "import subprocess\n",
    "\n",
    "iterations = 25\n",
    "total_nodes = 11.0\n",
    "#total_nodes = 15192277.0\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "num_converter = 10000000000\n",
    "\n",
    "# Run the PageRank MRJob\n",
    "try:\n",
    "    subprocess.check_call(['hdfs', 'dfs', '-rm', '-r', '/user/hadoop/HW9Output/PageRank1'])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    subprocess.check_call(['hdfs', 'dfs', '-rm', '-r', '/user/hadoop/HW9Output/PageRank2'])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    subprocess.check_call(['hdfs', 'dfs', '-rm', '-r', '/user/hadoop/HW9Input/PageRank1'])\n",
    "except:\n",
    "    pass\n",
    "subprocess.check_call(['hdfs', 'dfs', '-cp', '/user/hadoop/HW9Results/PageRankInit_test', '/user/hadoop/HW9Input/PageRank1'])\n",
    "#subprocess.check_call(['hdfs', 'dfs', '-cp', '/user/hadoop/HW9Results/PageRankInit_wikipedia', '/user/hadoop/HW9Input/PageRank1'])\n",
    "\n",
    "i = 0\n",
    "while(i < iterations):\n",
    "    print 'Iteration ', str(i)\n",
    "    # Run the PageRankStep1 MRJob\n",
    "    mr_job1 = MRPageRankStep1(args=['hdfs:///user/hadoop/HW9Input/PageRank1', '-r', 'hadoop', '--num_converter', \n",
    "                                    str(num_converter), '--output-dir', 'HW9Output/PageRank1', '--no-output'])\n",
    "    with mr_job1.make_runner() as runner: \n",
    "        runner.run()\n",
    "    dangling_mass = runner.counters()[0]['PageRank_Counters']['Dangling_Mass']/float(num_converter)\n",
    "    print 'Dangling Mass: ', dangling_mass\n",
    "\n",
    "    # Run the PageRankStep2 MRJob\n",
    "    mr_job2 = MRPageRankStep2(args=['hdfs:///user/hadoop/HW9Output/PageRank1', '-r', 'hadoop', '--damping_factor', \n",
    "                                    str(damping_factor), '--total_nodes', str(total_nodes), '--dangling_mass', \n",
    "                                    str(dangling_mass), '--output-dir', 'HW9Output/PageRank2', '--no-output'])\n",
    "    with mr_job2.make_runner() as runner: \n",
    "        runner.run()\n",
    "    print 'Done with step 2'\n",
    "\n",
    "    i += 1\n",
    "    if (i < iterations):\n",
    "        subprocess.check_call(['hdfs', 'dfs', '-rm', '-r', '/user/hadoop/HW9Input/PageRank1'])\n",
    "        subprocess.check_call(['hdfs', 'dfs', '-mv', '/user/hadoop/HW9Output/PageRank2', '/user/hadoop/HW9Input/PageRank1'])\n",
    "        subprocess.check_call(['hdfs', 'dfs', '-rm', '-r', '/user/hadoop/HW9Output/PageRank1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing start_job_PageRank.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile start_job_PageRank.sh\n",
    "#!/bin/bash\n",
    "nohup python PageRank_driver.py > nohup/PageRank_driver.out &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x start_job_PageRank.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: redirecting stderr to stdout\r\n"
     ]
    }
   ],
   "source": [
    "!./start_job_PageRank.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/15 15:03:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:03:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\r\n",
      "16/07/15 15:03:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:03:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank2\r\n",
      "16/07/15 15:03:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:03:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\r\n",
      "16/07/15 15:03:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:03:08 WARN hdfs.DFSClient: DFSInputStream has been closed already\r\n",
      "16/07/15 15:03:08 WARN hdfs.DFSClient: DFSInputStream has been closed already\r\n",
      "No handlers could be found for logger \"mrjob.hadoop\"\r\n",
      "16/07/15 15:04:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:04:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\r\n",
      "16/07/15 15:04:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:04:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:04:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\r\n",
      "16/07/15 15:05:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:05:23 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\r\n",
      "16/07/15 15:05:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:05:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:05:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\r\n",
      "16/07/15 15:06:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:06:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\r\n",
      "16/07/15 15:06:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:06:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:06:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\r\n",
      "16/07/15 15:07:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:07:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Input/PageRank1\r\n",
      "16/07/15 15:08:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:08:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/07/15 15:08:03 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /user/hadoop/HW9Output/PageRank1\r\n"
     ]
    }
   ],
   "source": [
    "!cat nohup/PageRank_driver.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/07/14 16:43:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"*total_nodes\"\t11\n",
      "\"A\"\t[{}, 0.09090909090909091]\n",
      "\"B\"\t[{\"C\": 1}, 0.09090909090909091]\n",
      "\"C\"\t[{\"B\": 1}, 0.09090909090909091]\n",
      "\"D\"\t[{\"A\": 1, \"B\": 1}, 0.09090909090909091]\n",
      "\"E\"\t[{\"B\": 1, \"D\": 1, \"F\": 1}, 0.09090909090909091]\n",
      "\"F\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"G\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"H\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"I\"\t[{\"B\": 1, \"E\": 1}, 0.09090909090909091]\n",
      "\"J\"\t[{\"E\": 1}, 0.09090909090909091]\n",
      "\"K\"\t[{\"E\": 1}, 0.09090909090909091]\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/hadoop/HW9Input/PageRank1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.1 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.2: Exploring PageRank teleportation and network plots </h2>\n",
    "\n",
    "* In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "* Using the graph in HW1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha, so that each nodes radius is proportional to its PageRank score. \n",
    "* In particular you should do this for the following damping factors: [0,0.25,0.5,0.75, 0.85, 1]. \n",
    "* Note your plots should look like the following: https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.2 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.2 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.3: Applying PageRank to the Wikipedia hyperlinks network </h2>\n",
    "\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 5 iterations, and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15).\n",
    "* Have the top 100 ranked pages changed? Comment on your findings. \n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.3 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"color:darkgreen\">  HW 9.3 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.4: Topic-specific PageRank implementation using MRJob </h2>\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation, as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.   \n",
    "This caveat lies in footnote 3 on page 3:\n",
    "```\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "```\n",
    "and must be adhered to for convergence to be guaranteed.   \n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors (one for the vanilla PageRank implementation in 9.1, and one for each topic with the topic specific implementation). Print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the requirements for irreducibility with topic-specific PageRank. In particular, the literature ensures irreducibility by requiring that nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, please comment on why the literature's method is difficult to implement, and what what extra computation it will require.   \n",
    "\n",
    "Then for your code, please use the alternative, non-uniform damping vector:\n",
    "\n",
    "```\n",
    "vji = beta*(1/|Tj|); if node i lies in topic Tj\n",
    "\n",
    "vji = (1-beta)*(1/(N - |Tj|)); if node i lies outside of topic Tj\n",
    "```\n",
    "for beta in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes. If beta > 0.5, PageRank is topic-sensitive, and if beta < 0.5, the PageRank is anti-topic-sensitive. For any value of beta irreducibility should hold, so please try beta=0.99, and perhaps some other values locally, on the smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.4 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.4 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h3 style='color:darkblue'>---------  OPTIONAL QUESTIONS SECTION --------</h3></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.5: (OPTIONAL) Applying topic-specific PageRank to Wikipedia</h2>\n",
    "\n",
    "Here you will apply your topic-specific PageRank implementation to Wikipedia, defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10, so that there are 10 topics. \n",
    "\n",
    "* Once again, print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses. Run for 10 iterations.\n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run in HW 9.3. \n",
    "* Then plot the pagerank values for the same 100 pages that result from the topic specific pagerank after 10 iterations run. \n",
    "* Comment on your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.5 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.5 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkblue\"> HW 9.6:  (OPTIONAL) TextRank</h2>\n",
    "\n",
    "* What is TextRank? Describe the main steps in the algorithm. Why does TextRank work?\n",
    "* Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):\n",
    "```\n",
    "\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" \n",
    "```\n",
    "* The extracted keywords should in the following set:\n",
    "```\n",
    "linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.6 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.6 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HWK 9 --------</h2></div></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "print \"Megan is awesome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megan is awesome\r\n"
     ]
    }
   ],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_driver.py\n",
    "nohup python test.py > nohup/test &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x test_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: redirecting stderr to stdout\r\n"
     ]
    }
   ],
   "source": [
    "!./test_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megan is awesome\r\n"
     ]
    }
   ],
   "source": [
    "!cat nohup/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "16/07/14 01:46:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/HW9Output/PageRank': No such file or directory\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "coercing to Unicode: need string or buffer, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a64d5bbc25d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_output_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_job_files_for_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upload_local_files_to_hdfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job_in_hadoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_input_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run_job_in_hadoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_job_in_hadoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m             \u001b[0mstep_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args_for_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36m_num_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_num_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;34m\"\"\"Get the number of steps (calls :py:meth:`get_steps`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_interpreter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36m_get_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    728\u001b[0m                 env = combine_local_envs(os.environ,\n\u001b[0;32m    729\u001b[0m                                          {'PYTHONPATH': os.path.abspath('.')})\n\u001b[1;32m--> 730\u001b[1;33m                 \u001b[0msteps_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[0;32m    708\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m                                 errread, errwrite)\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[0;32m   1333\u001b[0m                         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: coercing to Unicode: need string or buffer, float found"
     ]
    }
   ],
   "source": [
    "import PageRank\n",
    "import PageRankInit\n",
    "reload(PageRank)\n",
    "reload(PageRankInit)\n",
    "import json\n",
    "\n",
    "def get_total_nodes(ofile):\n",
    "    ls_output = !hdfs dfs -ls $ofile\n",
    "    num_output_files = len(ls_output)-3\n",
    "    for i in range(num_output_files):\n",
    "        snum = str(i)\n",
    "        zeros = '0'*(5-len(snum))\n",
    "        rfile = ofile+'/part-'+zeros+snum\n",
    "        grep_output = !hdfs dfs -cat $rfile | grep '*total_nodes'\n",
    "        if len(grep_output) == 2:\n",
    "            _, total_nodes = grep_output[1].strip().split('\\t')\n",
    "            break\n",
    "    return float(total_nodes)\n",
    "\n",
    "# Set the damping factor\n",
    "damping_factor = 0.85\n",
    "iterations = 1.0\n",
    "\n",
    "ofile ='HW9Results/PageRankInit_test'\n",
    "\n",
    "total_nodes = get_total_nodes('/user/hadoop/'+ofile)\n",
    "print total_nodes\n",
    "\n",
    "# Run the PageRank MRJob\n",
    "ifile = 'hdfs:///user/hadoop/'+ofile\n",
    "ofile ='HW9Output/PageRank'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "mr_job = PageRank.MRPageRank(args=['hdfs:///user/hadoop/HW9Results/PageRankInit_test/part-00000', '-r', 'hadoop', '--damping_factor', damping_factor, '--total_nodes', total_nodes, '--output-dir', ofile])\n",
    "    \n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "    \n",
    "print 'all done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "# Set stop condition to False\n",
    "Stop = False\n",
    "while(False):\n",
    "#while(Stop == False):\n",
    "#while(i < 40):\n",
    "    work_table = {}\n",
    "    # Store the previous page ranks to compare of the stop conditions\n",
    "    old_page_ranks = page_ranks\n",
    "    page_ranks = []\n",
    "    # Print the iteration number\n",
    "    print('Iteration %d' % (i))\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            #print key, value\n",
    "            work_table[key] = value\n",
    "            page_ranks.append(value[1])\n",
    "        \n",
    "        # Update work_table for the next iteration\n",
    "        with open(work_filename, 'w') as f:\n",
    "            for key, value in work_table.iteritems():\n",
    "                f.write(key+'\\t'+json.dumps(value)+'\\n')\n",
    "        \n",
    "        # Check the stop critera against the threshold epsilon\n",
    "        Stop = stop_criterion_reached(old_page_ranks, page_ranks, epsilon)\n",
    "        #print('Sum of page ranks: %f' % (sum(page_ranks)))\n",
    "    i += 1\n",
    "\n",
    "# Print the final results\n",
    "#print page_ranks\n",
    "#print('Sum of page ranks: %f' % (sum(page_ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_criterion_reached(old_page_ranks, page_ranks, epsilon):\n",
    "    Stop = False\n",
    "    total_error = 0.0\n",
    "    for pr1, pr2 in zip(old_page_ranks, page_ranks):\n",
    "        total_error += abs(pr1-pr2)\n",
    "    print 'Total Error: ', total_error\n",
    "    if total_error < epsilon:\n",
    "        Stop = True\n",
    "    return Stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
