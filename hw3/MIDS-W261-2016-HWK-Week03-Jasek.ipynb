{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  3\n",
    "* **Date:**  5/31/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**HW3.0.**\n",
    "How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "What is  a combiner function in the context of Hadoop? \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.1** Consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress.  \n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n",
    "\n",
    "??add screen shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 00:04:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 18 items\n",
      "-rw-r--r--   3 hadoop supergroup   50906486 2016-05-26 23:08 /user/hadoop/Consumer_Complaints.csv\n",
      "-rw-r--r--   3 hadoop supergroup      16478 2016-05-27 00:04 /user/hadoop/Consumer_Complaints_small.csv\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:20 /user/hadoop/outputHW2-0-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 14:06 /user/hadoop/outputHW2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 23:36 /user/hadoop/outputHW2-2\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 22:39 /user/hadoop/outputHW2-3\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 11:35 /user/hadoop/outputHW2-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 11:36 /user/hadoop/outputHW2-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:41 /user/hadoop/outputHW2-4b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 23:56 /user/hadoop/outputHW3-1\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n",
      "-rw-r--r--   3 hadoop supergroup         44 2016-05-26 09:20 /user/hadoop/strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-26 12:52 /user/hadoop/word_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the Consumer_Complaints.csv input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal Consumer_Complaints.csv /user/hadoop\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal Consumer_Complaints_small.csv /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 12:42:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 19 items\n",
      "-rw-r--r--   3 hadoop supergroup   50906486 2016-05-26 23:08 /user/hadoop/Consumer_Complaints.csv\n",
      "-rw-r--r--   3 hadoop supergroup      16478 2016-05-27 00:04 /user/hadoop/Consumer_Complaints_small.csv\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "-rw-r--r--   3 hadoop supergroup         31 2016-05-27 12:42 /user/hadoop/hw3-2_strings.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:54 /user/hadoop/outputHW2-0-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:56 /user/hadoop/outputHW2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 23:36 /user/hadoop/outputHW2-2\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 22:39 /user/hadoop/outputHW2-3\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:00 /user/hadoop/outputHW2-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:01 /user/hadoop/outputHW2-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:41 /user/hadoop/outputHW2-4b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:36 /user/hadoop/outputHW3-1\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n",
      "-rw-r--r--   3 hadoop supergroup         44 2016-05-26 09:20 /user/hadoop/strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-26 12:52 /user/hadoop/word_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# List out what is in the /user/hadoop directory in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.1\n",
    "## The function takes the Consumer Complaints file as input and reads the Product column and\n",
    "## counts the different types of products.  The categories are:  'debt collection', 'mortgage' and 'others'\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line on commas and store the product column value in the 'product' variable\n",
    "    items = line.split(',')\n",
    "    product = items[1].lower()\n",
    "    # Skip the column header\n",
    "    if product == \"product\":\n",
    "        continue\n",
    "    elif product == \"debt collection\":\n",
    "        # Create a counter in the EDA Counters group called Debt that counts the products\n",
    "        # in the debt collection group\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Debt,1\\n\")\n",
    "    elif product == \"mortgage\":\n",
    "        # Create a counter in the EDA Counters group called Mortgage that counts the products\n",
    "        # in the debt collection group\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Mortgage,1\\n\")\n",
    "    else:\n",
    "        # Create a counter in the EDA Counters group called Others that counts the products\n",
    "        # in all other groups\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Others,1\\n\")\n",
    "    # Print the product with a count of 1\n",
    "    print('%s\\t%d' % (product, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.1\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 14:43:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 14:43:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-1/_SUCCESS\n",
      "16/05/27 14:43:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-1/part-00000\n",
      "16/05/27 14:43:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 14:44:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar6987847830147326519/] [] /tmp/streamjob5627728082777250317.jar tmpDir=null\n",
      "16/05/27 14:44:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 14:44:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 14:44:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 14:44:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/27 14:44:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0150\n",
      "16/05/27 14:44:02 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0150\n",
      "16/05/27 14:44:02 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0150/\n",
      "16/05/27 14:44:02 INFO mapreduce.Job: Running job: job_1463787494457_0150\n",
      "16/05/27 14:44:06 INFO mapreduce.Job: Job job_1463787494457_0150 running in uber mode : false\n",
      "16/05/27 14:44:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/27 14:44:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 14:44:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 14:44:19 INFO mapreduce.Job: Job job_1463787494457_0150 completed successfully\n",
      "16/05/27 14:44:20 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5504142\n",
      "\t\tFILE: Number of bytes written=11364110\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=184\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10814\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2758\n",
      "\t\tTotal time spent by all map tasks (ms)=10814\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2758\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10814\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2758\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11073536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2824192\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=4878312\n",
      "\t\tMap output materialized bytes=5504148\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=5504148\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=336\n",
      "\t\tCPU time spent (ms)=5240\n",
      "\t\tPhysical memory (bytes) snapshot=674217984\n",
      "\t\tVirtual memory (bytes) snapshot=6298443776\n",
      "\t\tTotal committed heap usage (bytes)=500695040\n",
      "\tEDA Counters\n",
      "\t\tDebt=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOthers=142788\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=184\n",
      "16/05/27 14:44:20 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-1\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-1\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 12:36:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 12:36 /user/hadoop/outputHW3-1/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        184 2016-05-27 12:36 /user/hadoop/outputHW3-1/part-00000\n",
      "16/05/27 12:36:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bank account or service\t38073\n",
      "consumer loan\t9387\n",
      "credit card\t41563\n",
      "credit reporting\t41214\n",
      "debt collection\t44372\n",
      "money transfers\t1540\n",
      "mortgage\t125752\n",
      "payday loan\t1579\n",
      "student loan\t9432\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-1\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 3.2** Analyze the performance of your Mappers, Combiners and Reducers using Counters.\n",
    "\n",
    "**HW 3.2A** For this brief study the Input file will be one record (the next line only):  \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "ANSWER:  ??my counts are 2 and 2\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "\n",
    "**HW 3.2B** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.  \n",
    "ANSWER:  Using 2 mappers and 2 reducers and NO combiner.  See hadoop output below marked 'OUTPUT FOR HW3.2B'.  Command used is as follows:  \n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\  \n",
    "-D mapreduce.job.maps=2 \\  \n",
    "-D mapred.reduce.tasks=2 \\  \n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\  \n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2b     \n",
    "\n",
    "\tMapper Counters  \n",
    "\t\tCalls=2  \n",
    "\tReducer Counters  \n",
    "\t\tCalls=2  \n",
    "\n",
    "**HW 3.2C** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.  \n",
    "ANSWER:  Using 2 mappers and 2 reducers and specifying a combiner.  See hadoop output below marked 'OUTPUT FOR HW3.2C'.  Command used is as follows:  \n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\  \n",
    "-D mapreduce.job.maps=2 \\  \n",
    "-D mapred.reduce.tasks=2 \\  \n",
    "-files mapper.py,reducer.py,combiner.py -mapper mapper.py -reducer reducer.py -combiner combiner.py \\  \n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2c  \n",
    "\n",
    "\tCombiner Counters\n",
    "\t\tCalls=4\n",
    "\tMapper Counters\n",
    "\t\tCalls=2\n",
    "\tReducer Counters\n",
    "\t\tCalls=2\n",
    "\n",
    "**HW 3.2D** Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency.  If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).  \n",
    "ANSWER:  See hadoop output below marked 'OUTPUT FOR HW3.2D'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:35:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 13:35:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/hw3-2_strings.txt\n",
      "16/05/27 13:35:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Read the required input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/hw3-2_strings.txt\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal hw3-2_strings.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:35:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "foo foo quux labs foo bar quux\n"
     ]
    }
   ],
   "source": [
    "# List out what is in the /user/hadoop directory in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/hw3-2_strings.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2A\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Break up each line by space delimiter and print each word with a count of 1.\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print('%s\\t%d' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2A\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:34:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 13:34:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2a/_SUCCESS\n",
      "16/05/27 13:34:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2a/part-00000\n",
      "16/05/27 13:34:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2a/part-00001\n",
      "16/05/27 13:34:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 13:34:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar6497032136524940383/] [] /tmp/streamjob7139756651106569084.jar tmpDir=null\n",
      "16/05/27 13:34:43 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 13:34:43 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 13:34:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 13:34:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/27 13:34:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0148\n",
      "16/05/27 13:34:44 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0148\n",
      "16/05/27 13:34:44 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0148/\n",
      "16/05/27 13:34:44 INFO mapreduce.Job: Running job: job_1463787494457_0148\n",
      "16/05/27 13:34:49 INFO mapreduce.Job: Job job_1463787494457_0148 running in uber mode : false\n",
      "16/05/27 13:34:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/27 13:34:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 13:35:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 13:35:01 INFO mapreduce.Job: Job job_1463787494457_0148 completed successfully\n",
      "16/05/27 13:35:01 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=65\n",
      "\t\tFILE: Number of bytes written=355941\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=238\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6414\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2874\n",
      "\t\tTotal time spent by all map tasks (ms)=6414\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2874\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6414\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2874\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6567936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2942976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=71\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=71\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=225\n",
      "\t\tCPU time spent (ms)=1230\n",
      "\t\tPhysical memory (bytes) snapshot=647430144\n",
      "\t\tVirtual memory (bytes) snapshot=6291394560\n",
      "\t\tTotal committed heap usage (bytes)=505413632\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=48\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/05/27 13:35:01 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2a\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2a\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/hw3-2_strings.txt -output /user/hadoop/outputHW3-2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:23:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 5 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 13:22 /user/hadoop/outputHW3-2a/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup          7 2016-05-27 13:22 /user/hadoop/outputHW3-2a/part-00000\n",
      "-rw-r--r--   3 hadoop supergroup          6 2016-05-27 13:22 /user/hadoop/outputHW3-2a/part-00001\n",
      "-rw-r--r--   3 hadoop supergroup          6 2016-05-27 13:22 /user/hadoop/outputHW3-2a/part-00002\n",
      "-rw-r--r--   3 hadoop supergroup          7 2016-05-27 13:22 /user/hadoop/outputHW3-2a/part-00003\n",
      "16/05/27 13:23:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "quux\t2\n",
      "16/05/27 13:23:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "foo\t3\n",
      "16/05/27 13:23:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bar\t1\n",
      "16/05/27 13:23:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2a\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00001\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00002\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2B, HW3.2C\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    items = line.split(',')\n",
    "    issue = items[3].lower()\n",
    "    # If this is the column header, then do not count it.\n",
    "    if issue == \"issue\":\n",
    "        continue\n",
    "    for word in issue.split():\n",
    "        print('%s\\t%d' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/env python\n",
    "## combiner.py\n",
    "## Author: Megan Jasek\n",
    "## Description: combiner code for HW3.2C\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Combiner Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    items = line.split('\\t')\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2B, HW3.2C\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 17:58:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/_SUCCESS\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/part-00000\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/part-00001\n",
      "16/05/27 17:58:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 17:58:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar2229384560138214939/] [] /tmp/streamjob5233022461206413562.jar tmpDir=null\n",
      "16/05/27 17:58:09 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 17:58:09 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 17:58:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 17:58:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/27 17:58:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/27 17:58:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0162\n",
      "16/05/27 17:58:10 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0162\n",
      "16/05/27 17:58:10 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0162/\n",
      "16/05/27 17:58:10 INFO mapreduce.Job: Running job: job_1463787494457_0162\n",
      "16/05/27 17:58:14 INFO mapreduce.Job: Job job_1463787494457_0162 running in uber mode : false\n",
      "16/05/27 17:58:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/27 17:58:22 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 17:58:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 17:58:30 INFO mapreduce.Job: Job job_1463787494457_0162 completed successfully\n",
      "16/05/27 17:58:30 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11386163\n",
      "\t\tFILE: Number of bytes written=23246776\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=2169\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11664\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10286\n",
      "\t\tTotal time spent by all map tasks (ms)=11664\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10286\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11664\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10286\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11943936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=10532864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978633\n",
      "\t\tMap output bytes=9428885\n",
      "\t\tMap output materialized bytes=11386175\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce shuffle bytes=11386175\n",
      "\t\tReduce input records=978633\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=1957266\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=285\n",
      "\t\tCPU time spent (ms)=6890\n",
      "\t\tPhysical memory (bytes) snapshot=831930368\n",
      "\t\tVirtual memory (bytes) snapshot=8398528512\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2169\n",
      "16/05/27 17:58:30 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2b\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2B ########################\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2b\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 17:58:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 17:58 /user/hadoop/outputHW3-2b/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 17:58 /user/hadoop/outputHW3-2b/part-00000\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 17:58 /user/hadoop/outputHW3-2b/part-00001\n",
      "16/05/27 17:58:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"account\t16205\n",
      "/\t12386\n",
      "a\t3503\n",
      "account\t4476\n",
      "acct\t163\n",
      "an\t2505\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "company's\t4858\n",
      "cont'd\t11848\n",
      "convenience\t75\n",
      "credit\t50894\n",
      "debt\t19309\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "disclosure\t5214\n",
      "disputes\t6938\n",
      "expect\t807\n",
      "false\t2508\n",
      "fees\t807\n",
      "for\t929\n",
      "i\t925\n",
      "incorrect\t29069\n",
      "increase/decrease\t1149\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "or\t22533\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t2508\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t5214\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "16/05/27 17:58:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"application\t8625\n",
      "\"loan\t107254\n",
      "\"making/receiving\t3226\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t243\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "balance\t597\n",
      "bank\t202\n",
      "card\t4405\n",
      "charged\t976\n",
      "closing/cancelling\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "contact\t3053\n",
      "costs\t4350\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "decision\t2774\n",
      "didn't\t925\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "embezzlement\t3276\n",
      "fee\t3198\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect/missing\t64\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t12237\n",
      "loan/did\t139\n",
      "low\t5663\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "opening\t16205\n",
      "other\t7886\n",
      "out\t1242\n",
      "pay\t3821\n",
      "payment\t92\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "rate\t3431\n",
      "report\t30546\n",
      "report/credit\t4357\n",
      "reporting\t6559\n",
      "rewards\t1002\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statements\t2508\n",
      "stop\t131\n",
      "taking\t1242\n",
      "taking/threatening\t2505\n",
      "transaction\t1485\n",
      "underwriting\t2774\n",
      "using\t2422\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2b\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2b/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2b/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 13:00:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/_SUCCESS\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/part-00000\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/part-00001\n",
      "16/05/28 13:00:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 13:00:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3607205272937069079/] [] /tmp/streamjob6827866800821188715.jar tmpDir=null\n",
      "16/05/28 13:00:19 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 13:00:19 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 13:00:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 13:00:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/28 13:00:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/28 13:00:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0189\n",
      "16/05/28 13:00:21 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0189\n",
      "16/05/28 13:00:21 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0189/\n",
      "16/05/28 13:00:21 INFO mapreduce.Job: Running job: job_1463787494457_0189\n",
      "16/05/28 13:00:26 INFO mapreduce.Job: Job job_1463787494457_0189 running in uber mode : false\n",
      "16/05/28 13:00:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 13:00:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/05/28 13:00:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 13:00:38 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/28 13:00:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 13:00:39 INFO mapreduce.Job: Job job_1463787494457_0189 completed successfully\n",
      "16/05/28 13:00:39 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4598\n",
      "\t\tFILE: Number of bytes written=486162\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=2169\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12469\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5695\n",
      "\t\tTotal time spent by all map tasks (ms)=12469\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5695\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12469\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5695\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12768256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5831680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978633\n",
      "\t\tMap output bytes=9428885\n",
      "\t\tMap output materialized bytes=4610\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=978633\n",
      "\t\tCombine output records=317\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce shuffle bytes=4610\n",
      "\t\tReduce input records=317\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=634\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=360\n",
      "\t\tCPU time spent (ms)=5180\n",
      "\t\tPhysical memory (bytes) snapshot=823820288\n",
      "\t\tVirtual memory (bytes) snapshot=8400056320\n",
      "\t\tTotal committed heap usage (bytes)=595066880\n",
      "\tCombiner Counters\n",
      "\t\tCalls=4\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2169\n",
      "16/05/28 13:00:39 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2c\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2C ########################\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2c/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2c\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py,combiner.py -mapper mapper.py -reducer reducer.py -combiner combiner.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:02:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 18:01 /user/hadoop/outputHW3-2c/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 18:01 /user/hadoop/outputHW3-2c/part-00000\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 18:01 /user/hadoop/outputHW3-2c/part-00001\n",
      "16/05/27 18:02:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"account\t16205\n",
      "/\t12386\n",
      "a\t3503\n",
      "account\t4476\n",
      "acct\t163\n",
      "an\t2505\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "company's\t4858\n",
      "cont'd\t11848\n",
      "convenience\t75\n",
      "credit\t50894\n",
      "debt\t19309\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "disclosure\t5214\n",
      "disputes\t6938\n",
      "expect\t807\n",
      "false\t2508\n",
      "fees\t807\n",
      "for\t929\n",
      "i\t925\n",
      "incorrect\t29069\n",
      "increase/decrease\t1149\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "or\t22533\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t2508\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t5214\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "16/05/27 18:02:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"application\t8625\n",
      "\"loan\t107254\n",
      "\"making/receiving\t3226\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t243\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "balance\t597\n",
      "bank\t202\n",
      "card\t4405\n",
      "charged\t976\n",
      "closing/cancelling\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "contact\t3053\n",
      "costs\t4350\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "decision\t2774\n",
      "didn't\t925\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "embezzlement\t3276\n",
      "fee\t3198\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect/missing\t64\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t12237\n",
      "loan/did\t139\n",
      "low\t5663\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "opening\t16205\n",
      "other\t7886\n",
      "out\t1242\n",
      "pay\t3821\n",
      "payment\t92\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "rate\t3431\n",
      "report\t30546\n",
      "report/credit\t4357\n",
      "reporting\t6559\n",
      "rewards\t1002\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statements\t2508\n",
      "stop\t131\n",
      "taking\t1242\n",
      "taking/threatening\t2505\n",
      "transaction\t1485\n",
      "underwriting\t2774\n",
      "using\t2422\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2c\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2c/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2c/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:05:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:05:04 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the output of the Issues Column Word Count to separate files to use as input for sorting\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-2c/part-00000 /user/hadoop/cc_wordcount_1\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-2c/part-00001 /user/hadoop/cc_wordcount_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2D\n",
    "## This function passes the words and counts to the reducer.  It also sums up two other values:\n",
    "## total word count (count_words) and total unique word count (count_unique).  And passes those\n",
    "## values on to the reducer using special marker values 1000000000 and 2000000000.  This\n",
    "## technique is called order inversion.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "count_unique = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Sum up the total word count and total word count of unique words\n",
    "# Print each word and its count for the reducer\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    word, value = line.split('\\t')\n",
    "    count_words += int(value)\n",
    "    count_unique += 1\n",
    "    print('%s\\t%d' % (word, int(value)))\n",
    "\n",
    "# Print the special values 1000000000 and 2000000000 to mark the total count of words\n",
    "# and the total unique count of words\n",
    "print('%d\\t1000000000' % (count_words))\n",
    "print('%d\\t2000000000' % (count_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2D\n",
    "## This function expects keys to come in sorted.  Special values from the mapper, values 1000000000\n",
    "## and 2000000000 are used here to mark any total word count counts or unique word count counts.\n",
    "## Because these values are so high, they will come in first from the mappers, so the total word\n",
    "## count and total unique word count can be calculated first and then used for other calculations\n",
    "## in the function.  The reducer can quickly calculate the relative frequency of a word\n",
    "## by dividing by the total word count.  The top most frequent words are printed and the 10 least\n",
    "## frequent words are printed.  The reducer uses the total unique word count to understand which\n",
    "## words will be the 10 last in the sorted list.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "count_unique = 0\n",
    "i = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = int(items[1])\n",
    "    # If the value equals the special number 1000000000, this indicates this is a total \n",
    "    # word count value and it should be added to the count_words total\n",
    "    if value == 1000000000:\n",
    "        count_words += float(key)\n",
    "    # If the value equals the special number 2000000000, this indicates this is a unique \n",
    "    # word count value and it should be added to the count_unique total\n",
    "    elif value == 2000000000:\n",
    "        count_unique += float(key)\n",
    "    else:\n",
    "        # Print only the 50 most frequent and the 10 least frequent words\n",
    "        if (i < 50) or (i >= (count_unique-10)):\n",
    "            print('%s\\t%s\\t%f' % (key, value, value/count_words))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 12:50:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 12:50:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2d/_SUCCESS\n",
      "16/05/28 12:50:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2d/part-00000\n",
      "16/05/28 12:50:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 12:50:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3612921558270034290/] [] /tmp/streamjob7234082046083478946.jar tmpDir=null\n",
      "16/05/28 12:50:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 12:50:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 12:50:29 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/05/28 12:50:29 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/05/28 12:50:29 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/28 12:50:29 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/28 12:50:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0188\n",
      "16/05/28 12:50:29 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0188\n",
      "16/05/28 12:50:29 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0188/\n",
      "16/05/28 12:50:29 INFO mapreduce.Job: Running job: job_1463787494457_0188\n",
      "16/05/28 12:50:33 INFO mapreduce.Job: Job job_1463787494457_0188 running in uber mode : false\n",
      "16/05/28 12:50:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 12:50:40 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/05/28 12:50:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 12:50:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 12:50:45 INFO mapreduce.Job: Job job_1463787494457_0188 completed successfully\n",
      "16/05/28 12:50:45 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2800\n",
      "\t\tFILE: Number of bytes written=482669\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2566\n",
      "\t\tHDFS: Number of bytes written=1308\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14205\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1928\n",
      "\t\tTotal time spent by all map tasks (ms)=14205\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1928\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14205\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1928\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14545920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1974272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=171\n",
      "\t\tMap output records=177\n",
      "\t\tMap output bytes=2440\n",
      "\t\tMap output materialized bytes=2812\n",
      "\t\tInput split bytes=276\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=177\n",
      "\t\tReduce shuffle bytes=2812\n",
      "\t\tReduce input records=177\n",
      "\t\tReduce output records=60\n",
      "\t\tSpilled Records=354\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=433\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tPhysical memory (bytes) snapshot=892223488\n",
      "\t\tVirtual memory (bytes) snapshot=8390533120\n",
      "\t\tTotal committed heap usage (bytes)=706740224\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2290\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1308\n",
      "16/05/28 12:50:45 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2d\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2d/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2d\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/cc_wordcount_1,/user/hadoop/cc_wordcount_2 -output /user/hadoop/outputHW3-2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 12:56:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"loan\t107254\t0.109596\n",
      "modification\t70487\t0.072026\n",
      "credit\t50894\t0.052005\n",
      "servicing\t36767\t0.037570\n",
      "report\t30546\t0.031213\n",
      "incorrect\t29069\t0.029704\n",
      "information\t29069\t0.029704\n",
      "on\t29069\t0.029704\n",
      "or\t22533\t0.023025\n",
      "debt\t19309\t0.019731\n",
      "and\t16448\t0.016807\n",
      "\"account\t16205\t0.016559\n",
      "opening\t16205\t0.016559\n",
      "club\t12545\t0.012819\n",
      "health\t12545\t0.012819\n",
      "/\t12386\t0.012656\n",
      "not\t12353\t0.012623\n",
      "loan\t12237\t0.012504\n",
      "attempts\t11848\t0.012107\n",
      "collect\t11848\t0.012107\n",
      "cont'd\t11848\t0.012107\n",
      "owed\t11848\t0.012107\n",
      "of\t10885\t0.011123\n",
      "my\t10731\t0.010965\n",
      "deposits\t10555\t0.010785\n",
      "withdrawals\t10555\t0.010785\n",
      "problems\t9484\t0.009691\n",
      "\"application\t8625\t0.008813\n",
      "to\t8401\t0.008584\n",
      "unable\t8178\t0.008357\n",
      "billing\t8158\t0.008336\n",
      "other\t7886\t0.008058\n",
      "disputes\t6938\t0.007089\n",
      "communication\t6920\t0.007071\n",
      "tactics\t6920\t0.007071\n",
      "reporting\t6559\t0.006702\n",
      "lease\t6337\t0.006475\n",
      "the\t6248\t0.006384\n",
      "being\t5663\t0.005787\n",
      "by\t5663\t0.005787\n",
      "caused\t5663\t0.005787\n",
      "funds\t5663\t0.005787\n",
      "low\t5663\t0.005787\n",
      "process\t5505\t0.005625\n",
      "disclosure\t5214\t0.005328\n",
      "verification\t5214\t0.005328\n",
      "managing\t5006\t0.005115\n",
      "company's\t4858\t0.004964\n",
      "investigation\t4858\t0.004964\n",
      "identity\t4729\t0.004832\n",
      "apply\t118\t0.000121\n",
      "amount\t98\t0.000100\n",
      "credited\t92\t0.000094\n",
      "payment\t92\t0.000094\n",
      "checks\t75\t0.000077\n",
      "convenience\t75\t0.000077\n",
      "amt\t71\t0.000073\n",
      "day\t71\t0.000073\n",
      "disclosures\t64\t0.000065\n",
      "incorrect/missing\t64\t0.000065\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2D ########################\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2d/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.2.1**  Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2.1D\n",
    "## This function passes the words and counts to the reducer.  It also sums up the\n",
    "## total word count (count_words).  And passes it on to the reducer using special marker\n",
    "## value 1000000000.  This technique is called order inversion.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Sum up the total word count and total word count of unique words\n",
    "# Print each word and its count for the reducer\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    word, value = line.split('\\t')\n",
    "    if int(value) < 3000:\n",
    "        label = \"a\"\n",
    "    else:\n",
    "        label = \"b\"\n",
    "    count_words += int(value)\n",
    "    print('%s\\t%s\\t%d' % (label, word, int(value)))\n",
    "\n",
    "# Print the special value 1000000000 to mark the total count of words\n",
    "print('a\\t%d\\t1000000000' % (count_words))\n",
    "print('b\\t%d\\t1000000000' % (count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2.1D\n",
    "## This function expects keys to come in sorted.  Special values from the mapper, value 1000000000\n",
    "## is used here to mark any total word count counts.\n",
    "## Because this values is so high, it will come in first from the mappers, so the total word\n",
    "## count and total unique word count can be calculated first and then used for other calculations\n",
    "## in the function.  The reducer can quickly calculate the relative frequency of a word\n",
    "## by dividing by the total word count.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd and 3rd items as label and key and value\n",
    "    label = items[0]\n",
    "    key = items[1]\n",
    "    value = int(items[2])\n",
    "    # If the value equals the special number 1000000000, this indicates this is a total \n",
    "    # word count value and it should be added to the count_words total\n",
    "    if value == 1000000000:\n",
    "        count_words += float(key)\n",
    "    else:\n",
    "        print('%s\\t%s\\t%f' % (key, value, value/count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 16:27:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/outputHW3-2-1/*': No such file or directory\n",
      "16/05/28 16:27:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 16:27:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar7883164916772480020/] [] /tmp/streamjob8177213495908193641.jar tmpDir=null\n",
      "16/05/28 16:27:54 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 16:27:54 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 16:27:55 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/05/28 16:27:55 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/28 16:27:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0211\n",
      "16/05/28 16:27:56 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0211\n",
      "16/05/28 16:27:56 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0211/\n",
      "16/05/28 16:27:56 INFO mapreduce.Job: Running job: job_1463787494457_0211\n",
      "16/05/28 16:28:01 INFO mapreduce.Job: Job job_1463787494457_0211 running in uber mode : false\n",
      "16/05/28 16:28:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 16:28:07 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/05/28 16:28:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 16:28:13 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/28 16:28:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 16:28:14 INFO mapreduce.Job: Job job_1463787494457_0211 completed successfully\n",
      "16/05/28 16:28:14 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3172\n",
      "\t\tFILE: Number of bytes written=604420\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2566\n",
      "\t\tHDFS: Number of bytes written=3708\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6362\n",
      "\t\tTotal time spent by all map tasks (ms)=14400\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6362\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14400\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6362\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14745600\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6514688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=171\n",
      "\t\tMap output records=177\n",
      "\t\tMap output bytes=2806\n",
      "\t\tMap output materialized bytes=3196\n",
      "\t\tInput split bytes=276\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=177\n",
      "\t\tReduce shuffle bytes=3196\n",
      "\t\tReduce input records=177\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=354\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=624\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tPhysical memory (bytes) snapshot=1053052928\n",
      "\t\tVirtual memory (bytes) snapshot=10490544128\n",
      "\t\tTotal committed heap usage (bytes)=808452096\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2290\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3708\n",
      "16/05/28 16:28:14 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2-1\n"
     ]
    }
   ],
   "source": [
    "# -combiner reducer.py\n",
    "#-combiner Ireducer.py\n",
    "#-D stream.num.map.output.key.field=2 \\\n",
    "#-D stream.map.output.field.separator=\"\\t\" \\\n",
    "#-D mapreduce.partition.keypartitioner.options=-k2,2 \\\n",
    "#-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "#-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "\n",
    "\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2-1\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/cc_wordcount_1,/user/hadoop/cc_wordcount_2 -output /user/hadoop/outputHW3-2-1 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 16:29:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"loan\t107254\t0.109596\n",
      "modification\t70487\t0.072026\n",
      "credit\t50894\t0.052005\n",
      "servicing\t36767\t0.037570\n",
      "report\t30546\t0.031213\n",
      "incorrect\t29069\t0.029704\n",
      "information\t29069\t0.029704\n",
      "on\t29069\t0.029704\n",
      "or\t22533\t0.023025\n",
      "debt\t19309\t0.019731\n",
      "and\t16448\t0.016807\n",
      "\"account\t16205\t0.016559\n",
      "opening\t16205\t0.016559\n",
      "club\t12545\t0.012819\n",
      "health\t12545\t0.012819\n",
      "/\t12386\t0.012656\n",
      "not\t12353\t0.012623\n",
      "loan\t12237\t0.012504\n",
      "attempts\t11848\t0.012107\n",
      "collect\t11848\t0.012107\n",
      "cont'd\t11848\t0.012107\n",
      "owed\t11848\t0.012107\n",
      "of\t10885\t0.011123\n",
      "my\t10731\t0.010965\n",
      "deposits\t10555\t0.010785\n",
      "withdrawals\t10555\t0.010785\n",
      "problems\t9484\t0.009691\n",
      "\"application\t8625\t0.008813\n",
      "to\t8401\t0.008584\n",
      "unable\t8178\t0.008357\n",
      "billing\t8158\t0.008336\n",
      "other\t7886\t0.008058\n",
      "disputes\t6938\t0.007089\n",
      "communication\t6920\t0.007071\n",
      "tactics\t6920\t0.007071\n",
      "reporting\t6559\t0.006702\n",
      "lease\t6337\t0.006475\n",
      "the\t6248\t0.006384\n",
      "being\t5663\t0.005787\n",
      "by\t5663\t0.005787\n",
      "caused\t5663\t0.005787\n",
      "funds\t5663\t0.005787\n",
      "low\t5663\t0.005787\n",
      "process\t5505\t0.005625\n",
      "disclosure\t5214\t0.005328\n",
      "verification\t5214\t0.005328\n",
      "managing\t5006\t0.005115\n",
      "company's\t4858\t0.004964\n",
      "investigation\t4858\t0.004964\n",
      "identity\t4729\t0.004832\n",
      "account\t4476\t0.004574\n",
      "card\t4405\t0.004501\n",
      "get\t4357\t0.004452\n",
      "report/credit\t4357\t0.004452\n",
      "score\t4357\t0.004452\n",
      "costs\t4350\t0.004445\n",
      "settlement\t4350\t0.004445\n",
      "improper\t4309\t0.004403\n",
      "interest\t4238\t0.004331\n",
      "protection\t4139\t0.004229\n",
      "when\t4095\t0.004184\n",
      "repaying\t3844\t0.003928\n",
      "your\t3844\t0.003928\n",
      "fraud\t3842\t0.003926\n",
      "are\t3821\t0.003904\n",
      "pay\t3821\t0.003904\n",
      "you\t3821\t0.003904\n",
      "a\t3503\t0.003579\n",
      "apr\t3431\t0.003506\n",
      "rate\t3431\t0.003506\n",
      "embezzlement\t3276\t0.003348\n",
      "theft\t3276\t0.003348\n",
      "\"making/receiving\t3226\t0.003296\n",
      "payments\t3226\t0.003296\n",
      "fee\t3198\t0.003268\n",
      "contact\t3053\t0.003120\n",
      "16/05/28 16:29:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "info\t2896\t0.002959\n",
      "sharing\t2832\t0.002894\n",
      "closing/cancelling\t2795\t0.002856\n",
      "decision\t2774\t0.002835\n",
      "underwriting\t2774\t0.002835\n",
      "customer\t2734\t0.002794\n",
      "false\t2508\t0.002563\n",
      "representation\t2508\t0.002563\n",
      "statements\t2508\t0.002563\n",
      "action\t2505\t0.002560\n",
      "an\t2505\t0.002560\n",
      "illegal\t2505\t0.002560\n",
      "taking/threatening\t2505\t0.002560\n",
      "atm\t2422\t0.002475\n",
      "debit\t2422\t0.002475\n",
      "using\t2422\t0.002475\n",
      "lender\t2165\t0.002212\n",
      "can't\t1999\t0.002043\n",
      "dealing\t1944\t0.001986\n",
      "servicer\t1944\t0.001986\n",
      "with\t1944\t0.001986\n",
      "collection\t1907\t0.001949\n",
      "late\t1797\t0.001836\n",
      "line\t1732\t0.001770\n",
      "repay\t1647\t0.001683\n",
      "service\t1518\t0.001551\n",
      "determination\t1490\t0.001523\n",
      "transaction\t1485\t0.001517\n",
      "use\t1477\t0.001509\n",
      "monitoring\t1453\t0.001485\n",
      "relations\t1367\t0.001397\n",
      "out\t1242\t0.001269\n",
      "taking\t1242\t0.001269\n",
      "statement\t1220\t0.001247\n",
      "advertising\t1193\t0.001219\n",
      "marketing\t1193\t0.001219\n",
      "payoff\t1155\t0.001180\n",
      "increase/decrease\t1149\t0.001174\n",
      "issue\t1098\t0.001122\n",
      "delinquent\t1061\t0.001084\n",
      "practices\t1003\t0.001025\n",
      "rewards\t1002\t0.001024\n",
      "charged\t976\t0.000997\n",
      "for\t929\t0.000949\n",
      "didn't\t925\t0.000945\n",
      "i\t925\t0.000945\n",
      "dispute\t904\t0.000924\n",
      "expect\t807\t0.000825\n",
      "fees\t807\t0.000825\n",
      "shopping\t672\t0.000687\n",
      "issuance\t640\t0.000654\n",
      "unsolicited\t640\t0.000654\n",
      "balance\t597\t0.000610\n",
      "transfer\t597\t0.000610\n",
      "scam\t566\t0.000578\n",
      "issues\t538\t0.000550\n",
      "money\t413\t0.000422\n",
      "changes\t350\t0.000358\n",
      "forbearance\t350\t0.000358\n",
      "plans\t350\t0.000358\n",
      "terms\t350\t0.000358\n",
      "workout\t350\t0.000358\n",
      "getting\t291\t0.000297\n",
      "available\t274\t0.000280\n",
      "promised\t274\t0.000280\n",
      "was\t274\t0.000280\n",
      "application\t243\t0.000248\n",
      "delay\t243\t0.000248\n",
      "processing\t243\t0.000248\n",
      "advance\t240\t0.000245\n",
      "cash\t240\t0.000245\n",
      "privacy\t240\t0.000245\n",
      "bankruptcy\t222\t0.000227\n",
      "received\t216\t0.000221\n",
      "bank\t202\t0.000206\n",
      "wrong\t169\t0.000173\n",
      "arbitration\t168\t0.000172\n",
      "acct\t163\t0.000167\n",
      "applied\t139\t0.000142\n",
      "loan/did\t139\t0.000142\n",
      "receive\t139\t0.000142\n",
      "sale\t139\t0.000142\n",
      "charges\t131\t0.000134\n",
      "stop\t131\t0.000134\n",
      "overlimit\t127\t0.000130\n",
      "apply\t118\t0.000121\n",
      "amount\t98\t0.000100\n",
      "credited\t92\t0.000094\n",
      "payment\t92\t0.000094\n",
      "checks\t75\t0.000077\n",
      "convenience\t75\t0.000077\n",
      "amt\t71\t0.000073\n",
      "day\t71\t0.000073\n",
      "disclosures\t64\t0.000065\n",
      "incorrect/missing\t64\t0.000065\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2.1D ########################\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2-1/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2-1/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Bank account or service\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Consumer loan\t1\n",
      "Debt collection\t1\n",
      "Consumer loan\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Mortgage\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Mortgage\t1\n",
      "Debt collection\t1\n",
      "Bank account or service\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Consumer loan\t1\n",
      "Credit reporting\t1\n",
      "Bank account or service\t1\n",
      "Debt collection\t1\n",
      "Bank account or service\t1\n",
      "Mortgage\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Bank account or service\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit card\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit card\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Mortgage\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit card\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Bank account or service\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Bank account or service\t1\n",
      "Credit reporting\t1\n",
      "Bank account or service\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Mortgage\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Debt collection\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n",
      "Credit reporting\t1\n"
     ]
    }
   ],
   "source": [
    "### Testing cell\n",
    "filename = \"Consumer_Complaints_small.csv\"\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        items = line.split(',')\n",
    "        product = items[1]\n",
    "        if product == \"Debt collection\":\n",
    "            pass\n",
    "        elif product == \"Mortgage\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        print '%s\\t%d' % (product, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 16:59:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Read the Consumer_Complaints.csv input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal test_strings.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Imapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Imapper.py\n",
    "#!/usr/bin/env python\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Identity Calls,1\\n\")\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    print(\"%s\" % (line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Ireducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Ireducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Identity Calls,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    print(\"%s\" % (line.strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
