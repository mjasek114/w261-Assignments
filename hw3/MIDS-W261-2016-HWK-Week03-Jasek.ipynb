{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  3\n",
    "* **Date:**  5/31/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**HW3.0.**  \n",
    "\n",
    "**HW3.0A**How do you merge two sorted lists/arrays of records of the form [key, value]? Where is this used in Hadoop MapReduce? [Hint within the shuffle].  \n",
    "ANSWER:  \n",
    "\n",
    "Sorted lists are merged using 3 pointers.  \n",
    "1. at the start of 1st sorted list\n",
    "2. at the start of 2nd sorted list\n",
    "3. at the start of the merged list.\n",
    "The algorithm keeps picking off the smallest element from the beginning of the lists and adding it to the merged list until both of the original lists are empty.  Here is the algorithm:\n",
    "\n",
    "def mergeSortedLists(a, b):  \n",
    "L = []  \n",
    "while a and b:  \n",
    "if a[0] < b[0]:  \n",
    "L.append(a.pop(0))  \n",
    "else:  \n",
    "L.append(b.pop(0))  \n",
    "return L + a + b  \n",
    "\n",
    "Hadoop uses this algorithm when merging lists of sorted keys during the Hadoop shuffle that happens after the mappers are finished but before the reducers start.\n",
    "\n",
    "**HW3.0B** What is  a combiner function in the context of Hadoop?  Give an example where it can be used and justify why it should be used in the context of this problem.  \n",
    "ANSWER:  \n",
    "A combiner function is a function that aggregates outputs with the same key.  Combiners can be used on the output of a mapper function.  They can combine the output from the same key.  This is done to reduce network traffic from the mapper nodes to the reducer nodes.  This can save time in the mapreduce framework.\n",
    "\n",
    "**HW3.0C** What is the Hadoop shuffle?  \n",
    "ANSWER:  \n",
    "The Hadoop shuffle is the process by which the mapreduce framework performs the sort and transfers the map outputs to the reducers as inputs.  The Hadoop shuffle can be summarized as follows:  \n",
    "* On mapper node:  1) Partition the mapper output by key, so that every key is assigned a reducer node, 2) Sort the data by key, 3) Combine (aggregate) the data by key.\n",
    "* On mapper node:  mergesort (repeat while output)\n",
    "* Transmit data to reducer node\n",
    "* On reducer node:  Combine (aggregate) the data by key.\n",
    "* On reducer node:  mergesort partition files\n",
    "* On reducer node:  send output to reducer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.1** Consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress.  \n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n",
    "\n",
    "??add screen shot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image(filename=\"mapper_reducer_counts_3_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 00:04:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 18 items\n",
      "-rw-r--r--   3 hadoop supergroup   50906486 2016-05-26 23:08 /user/hadoop/Consumer_Complaints.csv\n",
      "-rw-r--r--   3 hadoop supergroup      16478 2016-05-27 00:04 /user/hadoop/Consumer_Complaints_small.csv\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:20 /user/hadoop/outputHW2-0-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 14:06 /user/hadoop/outputHW2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 23:36 /user/hadoop/outputHW2-2\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 22:39 /user/hadoop/outputHW2-3\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 11:35 /user/hadoop/outputHW2-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 11:36 /user/hadoop/outputHW2-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:41 /user/hadoop/outputHW2-4b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 23:56 /user/hadoop/outputHW3-1\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n",
      "-rw-r--r--   3 hadoop supergroup         44 2016-05-26 09:20 /user/hadoop/strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-26 12:52 /user/hadoop/word_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the Consumer_Complaints.csv input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal Consumer_Complaints.csv /user/hadoop\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal Consumer_Complaints_small.csv /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 14:24:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 36 items\n",
      "-rw-r--r--   3 hadoop supergroup   50906486 2016-05-26 23:08 /user/hadoop/Consumer_Complaints.csv\n",
      "-rw-r--r--   3 hadoop supergroup      16478 2016-05-27 00:04 /user/hadoop/Consumer_Complaints_small.csv\n",
      "-rw-r--r--   3 hadoop supergroup    3458517 2016-05-28 17:14 /user/hadoop/ProductPurchaseData.txt\n",
      "-rw-r--r--   3 hadoop supergroup      10918 2016-05-28 20:17 /user/hadoop/ProductPurchaseData_small.txt\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 18:04 /user/hadoop/cc_wordcount_1\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 18:05 /user/hadoop/cc_wordcount_2\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "-rw-r--r--   3 hadoop supergroup         31 2016-05-27 13:35 /user/hadoop/hw3-2_strings.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:54 /user/hadoop/outputHW2-0-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:56 /user/hadoop/outputHW2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 23:36 /user/hadoop/outputHW2-2\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 22:39 /user/hadoop/outputHW2-3\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:00 /user/hadoop/outputHW2-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:01 /user/hadoop/outputHW2-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:41 /user/hadoop/outputHW2-4b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 14:44 /user/hadoop/outputHW3-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 16:28 /user/hadoop/outputHW3-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 17:45 /user/hadoop/outputHW3-2a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 17:58 /user/hadoop/outputHW3-2b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 13:00 /user/hadoop/outputHW3-2c\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 12:50 /user/hadoop/outputHW3-2d\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 19:05 /user/hadoop/outputHW3-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 19:16 /user/hadoop/outputHW3-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 22:19 /user/hadoop/outputHW3-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 21:17 /user/hadoop/outputHW3-4b\n",
      "-rw-r--r--   3 hadoop supergroup     142676 2016-05-28 18:07 /user/hadoop/product_counts\n",
      "-rw-r--r--   3 hadoop supergroup      32057 2016-05-28 20:49 /user/hadoop/product_pairs_output\n",
      "-rw-r--r--   3 hadoop supergroup   19335926 2016-05-28 20:34 /user/hadoop/product_pairs_output_full\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n",
      "-rw-r--r--   3 hadoop supergroup         44 2016-05-26 09:20 /user/hadoop/strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup         78 2016-05-27 16:59 /user/hadoop/test_strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-26 12:52 /user/hadoop/word_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# List out what is in the /user/hadoop directory in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.1\n",
    "## The function takes the Consumer Complaints file as input and reads the Product column and\n",
    "## counts the different types of products.  The categories are:  'debt collection', 'mortgage' and 'others'\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line on commas and store the product column value in the 'product' variable\n",
    "    items = line.split(',')\n",
    "    product = items[1].lower()\n",
    "    # Skip the column header\n",
    "    if product == \"product\":\n",
    "        continue\n",
    "    elif product == \"debt collection\":\n",
    "        # Create a counter in the EDA Counters group called Debt that counts the products\n",
    "        # in the debt collection group\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Debt,1\\n\")\n",
    "    elif product == \"mortgage\":\n",
    "        # Create a counter in the EDA Counters group called Mortgage that counts the products\n",
    "        # in the debt collection group\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Mortgage,1\\n\")\n",
    "    else:\n",
    "        # Create a counter in the EDA Counters group called Others that counts the products\n",
    "        # in all other groups\n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Others,1\\n\")\n",
    "    # Print the product with a count of 1\n",
    "    print('%s\\t%d' % (product, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.1\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 14:43:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 14:43:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-1/_SUCCESS\n",
      "16/05/27 14:43:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-1/part-00000\n",
      "16/05/27 14:43:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 14:44:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar6987847830147326519/] [] /tmp/streamjob5627728082777250317.jar tmpDir=null\n",
      "16/05/27 14:44:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 14:44:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 14:44:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 14:44:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/27 14:44:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0150\n",
      "16/05/27 14:44:02 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0150\n",
      "16/05/27 14:44:02 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0150/\n",
      "16/05/27 14:44:02 INFO mapreduce.Job: Running job: job_1463787494457_0150\n",
      "16/05/27 14:44:06 INFO mapreduce.Job: Job job_1463787494457_0150 running in uber mode : false\n",
      "16/05/27 14:44:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/27 14:44:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 14:44:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 14:44:19 INFO mapreduce.Job: Job job_1463787494457_0150 completed successfully\n",
      "16/05/27 14:44:20 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5504142\n",
      "\t\tFILE: Number of bytes written=11364110\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=184\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10814\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2758\n",
      "\t\tTotal time spent by all map tasks (ms)=10814\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2758\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10814\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2758\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11073536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2824192\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=4878312\n",
      "\t\tMap output materialized bytes=5504148\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=5504148\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=336\n",
      "\t\tCPU time spent (ms)=5240\n",
      "\t\tPhysical memory (bytes) snapshot=674217984\n",
      "\t\tVirtual memory (bytes) snapshot=6298443776\n",
      "\t\tTotal committed heap usage (bytes)=500695040\n",
      "\tEDA Counters\n",
      "\t\tDebt=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOthers=142788\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=184\n",
      "16/05/27 14:44:20 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-1\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-1\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 12:36:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 12:36 /user/hadoop/outputHW3-1/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        184 2016-05-27 12:36 /user/hadoop/outputHW3-1/part-00000\n",
      "16/05/27 12:36:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bank account or service\t38073\n",
      "consumer loan\t9387\n",
      "credit card\t41563\n",
      "credit reporting\t41214\n",
      "debt collection\t44372\n",
      "money transfers\t1540\n",
      "mortgage\t125752\n",
      "payday loan\t1579\n",
      "student loan\t9432\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-1\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 3.2** Analyze the performance of your Mappers, Combiners and Reducers using Counters.\n",
    "\n",
    "**HW 3.2A** For this brief study the Input file will be one record (the next line only):  \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "ANSWER:  ??my counts are 2 and 2\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "\n",
    "**HW 3.2B** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.  \n",
    "ANSWER:  Using 2 mappers and 2 reducers and NO combiner.  See hadoop output below marked 'OUTPUT FOR HW3.2B'.  Command used is as follows:  \n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\  \n",
    "-D mapreduce.job.maps=2 \\  \n",
    "-D mapred.reduce.tasks=2 \\  \n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\  \n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2b     \n",
    "\n",
    "\tMapper Counters  \n",
    "\t\tCalls=2  \n",
    "\tReducer Counters  \n",
    "\t\tCalls=2  \n",
    "\n",
    "**HW 3.2C** Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.  \n",
    "ANSWER:  Using 2 mappers and 2 reducers and specifying a combiner.  See hadoop output below marked 'OUTPUT FOR HW3.2C'.  Command used is as follows:  \n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\  \n",
    "-D mapreduce.job.maps=2 \\  \n",
    "-D mapred.reduce.tasks=2 \\  \n",
    "-files mapper.py,reducer.py,combiner.py -mapper mapper.py -reducer reducer.py -combiner combiner.py \\  \n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2c  \n",
    "\n",
    "\tCombiner Counters\n",
    "\t\tCalls=4\n",
    "\tMapper Counters\n",
    "\t\tCalls=2\n",
    "\tReducer Counters\n",
    "\t\tCalls=2\n",
    "\n",
    "**HW 3.2D** Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency.  If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).  \n",
    "ANSWER:  See hadoop output below marked 'OUTPUT FOR HW3.2D'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:35:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 13:35:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/hw3-2_strings.txt\n",
      "16/05/27 13:35:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Read the required input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/hw3-2_strings.txt\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal hw3-2_strings.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 13:35:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "foo foo quux labs foo bar quux\n"
     ]
    }
   ],
   "source": [
    "# List out what is in the /user/hadoop directory in HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/hw3-2_strings.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount_mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2A\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Break up each line by space delimiter and print each word with a count of 1.\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print('%s\\t%d' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount_reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2A\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 17:45:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 17:45:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2a/_SUCCESS\n",
      "16/05/28 17:45:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2a/part-00000\n",
      "16/05/28 17:45:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 17:45:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar554923918886206609/] [] /tmp/streamjob4830429812404749691.jar tmpDir=null\n",
      "16/05/28 17:45:12 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 17:45:12 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 17:45:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 17:45:13 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/28 17:45:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0212\n",
      "16/05/28 17:45:13 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0212\n",
      "16/05/28 17:45:13 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0212/\n",
      "16/05/28 17:45:13 INFO mapreduce.Job: Running job: job_1463787494457_0212\n",
      "16/05/28 17:45:18 INFO mapreduce.Job: Job job_1463787494457_0212 running in uber mode : false\n",
      "16/05/28 17:45:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 17:45:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 17:45:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 17:45:30 INFO mapreduce.Job: Job job_1463787494457_0212 completed successfully\n",
      "16/05/28 17:45:30 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=65\n",
      "\t\tFILE: Number of bytes written=356252\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=237\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6373\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3301\n",
      "\t\tTotal time spent by all map tasks (ms)=6373\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3301\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6373\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3301\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6525952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3380224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=71\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=71\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=169\n",
      "\t\tCPU time spent (ms)=1270\n",
      "\t\tPhysical memory (bytes) snapshot=643870720\n",
      "\t\tVirtual memory (bytes) snapshot=6292185088\n",
      "\t\tTotal committed heap usage (bytes)=508559360\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=47\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/05/28 17:45:30 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2a\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2a\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files wordcount_mapper.py,wordcount_reducer.py -mapper wordcount_mapper.py -reducer wordcount_reducer.py \\\n",
    "-input /user/hadoop/hw3-2_strings.txt -output /user/hadoop/outputHW3-2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 17:45:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-28 17:45 /user/hadoop/outputHW3-2a/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup         26 2016-05-28 17:45 /user/hadoop/outputHW3-2a/part-00000\n",
      "16/05/28 17:45:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2a\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00000\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00001\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00002\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2a/part-00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2B, HW3.2C\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    items = line.split(',')\n",
    "    issue = items[3].lower()\n",
    "    # If this is the column header, then do not count it.\n",
    "    if issue == \"issue\":\n",
    "        continue\n",
    "    for word in issue.split():\n",
    "        print('%s\\t%d' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/env python\n",
    "## combiner.py\n",
    "## Author: Megan Jasek\n",
    "## Description: combiner code for HW3.2C\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Combiner Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    items = line.split('\\t')\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2B, HW3.2C\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 17:58:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/_SUCCESS\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/part-00000\n",
      "16/05/27 17:58:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2b/part-00001\n",
      "16/05/27 17:58:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 17:58:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar2229384560138214939/] [] /tmp/streamjob5233022461206413562.jar tmpDir=null\n",
      "16/05/27 17:58:09 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 17:58:09 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/27 17:58:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/27 17:58:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/27 17:58:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/27 17:58:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0162\n",
      "16/05/27 17:58:10 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0162\n",
      "16/05/27 17:58:10 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0162/\n",
      "16/05/27 17:58:10 INFO mapreduce.Job: Running job: job_1463787494457_0162\n",
      "16/05/27 17:58:14 INFO mapreduce.Job: Job job_1463787494457_0162 running in uber mode : false\n",
      "16/05/27 17:58:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/27 17:58:22 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/27 17:58:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/27 17:58:30 INFO mapreduce.Job: Job job_1463787494457_0162 completed successfully\n",
      "16/05/27 17:58:30 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11386163\n",
      "\t\tFILE: Number of bytes written=23246776\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=2169\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11664\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10286\n",
      "\t\tTotal time spent by all map tasks (ms)=11664\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10286\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11664\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10286\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11943936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=10532864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978633\n",
      "\t\tMap output bytes=9428885\n",
      "\t\tMap output materialized bytes=11386175\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce shuffle bytes=11386175\n",
      "\t\tReduce input records=978633\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=1957266\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=285\n",
      "\t\tCPU time spent (ms)=6890\n",
      "\t\tPhysical memory (bytes) snapshot=831930368\n",
      "\t\tVirtual memory (bytes) snapshot=8398528512\n",
      "\t\tTotal committed heap usage (bytes)=603979776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2169\n",
      "16/05/27 17:58:30 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2b\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2B ########################\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2b\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 17:58:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 17:58 /user/hadoop/outputHW3-2b/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 17:58 /user/hadoop/outputHW3-2b/part-00000\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 17:58 /user/hadoop/outputHW3-2b/part-00001\n",
      "16/05/27 17:58:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"account\t16205\n",
      "/\t12386\n",
      "a\t3503\n",
      "account\t4476\n",
      "acct\t163\n",
      "an\t2505\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "company's\t4858\n",
      "cont'd\t11848\n",
      "convenience\t75\n",
      "credit\t50894\n",
      "debt\t19309\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "disclosure\t5214\n",
      "disputes\t6938\n",
      "expect\t807\n",
      "false\t2508\n",
      "fees\t807\n",
      "for\t929\n",
      "i\t925\n",
      "incorrect\t29069\n",
      "increase/decrease\t1149\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "or\t22533\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t2508\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t5214\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "16/05/27 17:58:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"application\t8625\n",
      "\"loan\t107254\n",
      "\"making/receiving\t3226\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t243\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "balance\t597\n",
      "bank\t202\n",
      "card\t4405\n",
      "charged\t976\n",
      "closing/cancelling\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "contact\t3053\n",
      "costs\t4350\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "decision\t2774\n",
      "didn't\t925\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "embezzlement\t3276\n",
      "fee\t3198\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect/missing\t64\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t12237\n",
      "loan/did\t139\n",
      "low\t5663\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "opening\t16205\n",
      "other\t7886\n",
      "out\t1242\n",
      "pay\t3821\n",
      "payment\t92\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "rate\t3431\n",
      "report\t30546\n",
      "report/credit\t4357\n",
      "reporting\t6559\n",
      "rewards\t1002\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statements\t2508\n",
      "stop\t131\n",
      "taking\t1242\n",
      "taking/threatening\t2505\n",
      "transaction\t1485\n",
      "underwriting\t2774\n",
      "using\t2422\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2b\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2b/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2b/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 13:00:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/_SUCCESS\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/part-00000\n",
      "16/05/28 13:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2c/part-00001\n",
      "16/05/28 13:00:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 13:00:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3607205272937069079/] [] /tmp/streamjob6827866800821188715.jar tmpDir=null\n",
      "16/05/28 13:00:19 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 13:00:19 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 13:00:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 13:00:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/28 13:00:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/28 13:00:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0189\n",
      "16/05/28 13:00:21 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0189\n",
      "16/05/28 13:00:21 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0189/\n",
      "16/05/28 13:00:21 INFO mapreduce.Job: Running job: job_1463787494457_0189\n",
      "16/05/28 13:00:26 INFO mapreduce.Job: Job job_1463787494457_0189 running in uber mode : false\n",
      "16/05/28 13:00:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 13:00:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/05/28 13:00:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 13:00:38 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/28 13:00:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 13:00:39 INFO mapreduce.Job: Job job_1463787494457_0189 completed successfully\n",
      "16/05/28 13:00:39 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4598\n",
      "\t\tFILE: Number of bytes written=486162\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910085\n",
      "\t\tHDFS: Number of bytes written=2169\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12469\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5695\n",
      "\t\tTotal time spent by all map tasks (ms)=12469\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5695\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12469\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5695\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12768256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5831680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978633\n",
      "\t\tMap output bytes=9428885\n",
      "\t\tMap output materialized bytes=4610\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=978633\n",
      "\t\tCombine output records=317\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce shuffle bytes=4610\n",
      "\t\tReduce input records=317\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=634\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=360\n",
      "\t\tCPU time spent (ms)=5180\n",
      "\t\tPhysical memory (bytes) snapshot=823820288\n",
      "\t\tVirtual memory (bytes) snapshot=8400056320\n",
      "\t\tTotal committed heap usage (bytes)=595066880\n",
      "\tCombiner Counters\n",
      "\t\tCalls=4\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2169\n",
      "16/05/28 13:00:39 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2c\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2C ########################\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2c/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2c\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapreduce.job.maps=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py,combiner.py -mapper mapper.py -reducer reducer.py -combiner combiner.py \\\n",
    "-input /user/hadoop/Consumer_Complaints.csv -output /user/hadoop/outputHW3-2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:02:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-27 18:01 /user/hadoop/outputHW3-2c/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 18:01 /user/hadoop/outputHW3-2c/part-00000\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 18:01 /user/hadoop/outputHW3-2c/part-00001\n",
      "16/05/27 18:02:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"account\t16205\n",
      "/\t12386\n",
      "a\t3503\n",
      "account\t4476\n",
      "acct\t163\n",
      "an\t2505\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "company's\t4858\n",
      "cont'd\t11848\n",
      "convenience\t75\n",
      "credit\t50894\n",
      "debt\t19309\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "disclosure\t5214\n",
      "disputes\t6938\n",
      "expect\t807\n",
      "false\t2508\n",
      "fees\t807\n",
      "for\t929\n",
      "i\t925\n",
      "incorrect\t29069\n",
      "increase/decrease\t1149\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "or\t22533\n",
      "overlimit\t127\n",
      "owed\t11848\n",
      "payments\t3226\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t2508\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t5214\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "16/05/27 18:02:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"application\t8625\n",
      "\"loan\t107254\n",
      "\"making/receiving\t3226\n",
      "action\t2505\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t243\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t11848\n",
      "balance\t597\n",
      "bank\t202\n",
      "card\t4405\n",
      "charged\t976\n",
      "closing/cancelling\t2795\n",
      "club\t12545\n",
      "collect\t11848\n",
      "collection\t1907\n",
      "communication\t6920\n",
      "contact\t3053\n",
      "costs\t4350\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "decision\t2774\n",
      "didn't\t925\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "embezzlement\t3276\n",
      "fee\t3198\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "health\t12545\n",
      "identity\t4729\n",
      "illegal\t2505\n",
      "improper\t4309\n",
      "incorrect/missing\t64\n",
      "info\t2896\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t12237\n",
      "loan/did\t139\n",
      "low\t5663\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "modification\t70487\n",
      "money\t413\n",
      "monitoring\t1453\n",
      "my\t10731\n",
      "opening\t16205\n",
      "other\t7886\n",
      "out\t1242\n",
      "pay\t3821\n",
      "payment\t92\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "rate\t3431\n",
      "report\t30546\n",
      "report/credit\t4357\n",
      "reporting\t6559\n",
      "rewards\t1002\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "sharing\t2832\n",
      "shopping\t672\n",
      "statements\t2508\n",
      "stop\t131\n",
      "taking\t1242\n",
      "taking/threatening\t2505\n",
      "transaction\t1485\n",
      "underwriting\t2774\n",
      "using\t2422\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW3-2c\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2c/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2c/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/27 18:05:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/27 18:05:04 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the output of the Issues Column Word Count to separate files to use as input for sorting\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-2c/part-00000 /user/hadoop/cc_wordcount_1\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-2c/part-00001 /user/hadoop/cc_wordcount_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2D\n",
    "## This function passes the words and counts to the reducer.  It also sums up two other values:\n",
    "## total word count (count_words) and total unique word count (count_unique).  And passes those\n",
    "## values on to the reducer using special marker values 1000000000 and 2000000000.  This\n",
    "## technique is called order inversion.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "count_unique = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Sum up the total word count and total word count of unique words\n",
    "# Print each word and its count for the reducer\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    word, value = line.split('\\t')\n",
    "    count_words += int(value)\n",
    "    count_unique += 1\n",
    "    print('%s\\t%d' % (word, int(value)))\n",
    "\n",
    "# Print the special values 1000000000 and 2000000000 to mark the total count of words\n",
    "# and the total unique count of words\n",
    "print('%d\\t1000000000' % (count_words))\n",
    "print('%d\\t2000000000' % (count_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2D\n",
    "## This function expects keys to come in sorted.  Special values from the mapper, values 1000000000\n",
    "## and 2000000000 are used here to mark any total word count counts or unique word count counts.\n",
    "## Because these values are so high, they will come in first from the mappers, so the total word\n",
    "## count and total unique word count can be calculated first and then used for other calculations\n",
    "## in the function.  The reducer can quickly calculate the relative frequency of a word\n",
    "## by dividing by the total word count.  The top most frequent words are printed and the 10 least\n",
    "## frequent words are printed.  The reducer uses the total unique word count to understand which\n",
    "## words will be the 10 last in the sorted list.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "count_unique = 0\n",
    "i = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = int(items[1])\n",
    "    # If the value equals the special number 1000000000, this indicates this is a total \n",
    "    # word count value and it should be added to the count_words total\n",
    "    if value == 1000000000:\n",
    "        count_words += float(key)\n",
    "    # If the value equals the special number 2000000000, this indicates this is a unique \n",
    "    # word count value and it should be added to the count_unique total\n",
    "    elif value == 2000000000:\n",
    "        count_unique += float(key)\n",
    "    else:\n",
    "        # Print only the 50 most frequent and the 10 least frequent words\n",
    "        if (i < 50) or (i >= (count_unique-10)):\n",
    "            print('%s\\t%s\\t%f' % (key, value, value/count_words))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 12:50:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 12:50:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2d/_SUCCESS\n",
      "16/05/28 12:50:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-2d/part-00000\n",
      "16/05/28 12:50:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 12:50:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3612921558270034290/] [] /tmp/streamjob7234082046083478946.jar tmpDir=null\n",
      "16/05/28 12:50:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 12:50:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 12:50:29 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/05/28 12:50:29 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/05/28 12:50:29 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/28 12:50:29 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/28 12:50:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0188\n",
      "16/05/28 12:50:29 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0188\n",
      "16/05/28 12:50:29 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0188/\n",
      "16/05/28 12:50:29 INFO mapreduce.Job: Running job: job_1463787494457_0188\n",
      "16/05/28 12:50:33 INFO mapreduce.Job: Job job_1463787494457_0188 running in uber mode : false\n",
      "16/05/28 12:50:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 12:50:40 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/05/28 12:50:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 12:50:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 12:50:45 INFO mapreduce.Job: Job job_1463787494457_0188 completed successfully\n",
      "16/05/28 12:50:45 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2800\n",
      "\t\tFILE: Number of bytes written=482669\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2566\n",
      "\t\tHDFS: Number of bytes written=1308\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14205\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1928\n",
      "\t\tTotal time spent by all map tasks (ms)=14205\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1928\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14205\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1928\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14545920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1974272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=171\n",
      "\t\tMap output records=177\n",
      "\t\tMap output bytes=2440\n",
      "\t\tMap output materialized bytes=2812\n",
      "\t\tInput split bytes=276\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=177\n",
      "\t\tReduce shuffle bytes=2812\n",
      "\t\tReduce input records=177\n",
      "\t\tReduce output records=60\n",
      "\t\tSpilled Records=354\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=433\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tPhysical memory (bytes) snapshot=892223488\n",
      "\t\tVirtual memory (bytes) snapshot=8390533120\n",
      "\t\tTotal committed heap usage (bytes)=706740224\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2290\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1308\n",
      "16/05/28 12:50:45 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2d\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2d/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2d\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/cc_wordcount_1,/user/hadoop/cc_wordcount_2 -output /user/hadoop/outputHW3-2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 12:56:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"loan\t107254\t0.109596\n",
      "modification\t70487\t0.072026\n",
      "credit\t50894\t0.052005\n",
      "servicing\t36767\t0.037570\n",
      "report\t30546\t0.031213\n",
      "incorrect\t29069\t0.029704\n",
      "information\t29069\t0.029704\n",
      "on\t29069\t0.029704\n",
      "or\t22533\t0.023025\n",
      "debt\t19309\t0.019731\n",
      "and\t16448\t0.016807\n",
      "\"account\t16205\t0.016559\n",
      "opening\t16205\t0.016559\n",
      "club\t12545\t0.012819\n",
      "health\t12545\t0.012819\n",
      "/\t12386\t0.012656\n",
      "not\t12353\t0.012623\n",
      "loan\t12237\t0.012504\n",
      "attempts\t11848\t0.012107\n",
      "collect\t11848\t0.012107\n",
      "cont'd\t11848\t0.012107\n",
      "owed\t11848\t0.012107\n",
      "of\t10885\t0.011123\n",
      "my\t10731\t0.010965\n",
      "deposits\t10555\t0.010785\n",
      "withdrawals\t10555\t0.010785\n",
      "problems\t9484\t0.009691\n",
      "\"application\t8625\t0.008813\n",
      "to\t8401\t0.008584\n",
      "unable\t8178\t0.008357\n",
      "billing\t8158\t0.008336\n",
      "other\t7886\t0.008058\n",
      "disputes\t6938\t0.007089\n",
      "communication\t6920\t0.007071\n",
      "tactics\t6920\t0.007071\n",
      "reporting\t6559\t0.006702\n",
      "lease\t6337\t0.006475\n",
      "the\t6248\t0.006384\n",
      "being\t5663\t0.005787\n",
      "by\t5663\t0.005787\n",
      "caused\t5663\t0.005787\n",
      "funds\t5663\t0.005787\n",
      "low\t5663\t0.005787\n",
      "process\t5505\t0.005625\n",
      "disclosure\t5214\t0.005328\n",
      "verification\t5214\t0.005328\n",
      "managing\t5006\t0.005115\n",
      "company's\t4858\t0.004964\n",
      "investigation\t4858\t0.004964\n",
      "identity\t4729\t0.004832\n",
      "apply\t118\t0.000121\n",
      "amount\t98\t0.000100\n",
      "credited\t92\t0.000094\n",
      "payment\t92\t0.000094\n",
      "checks\t75\t0.000077\n",
      "convenience\t75\t0.000077\n",
      "amt\t71\t0.000073\n",
      "day\t71\t0.000073\n",
      "disclosures\t64\t0.000065\n",
      "incorrect/missing\t64\t0.000065\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2D ########################\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2d/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.2.1**  Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.2.1D\n",
    "## This function passes the words and counts to the reducer.  It also sums up the\n",
    "## total word count (count_words).  And passes it on to the reducer using special marker\n",
    "## value 1000000000.  This technique is called order inversion.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Sum up the total word count and total word count of unique words\n",
    "# Print each word and its count for the reducer\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    word, value = line.split('\\t')\n",
    "    if int(value) < 3000:\n",
    "        label = \"a\"\n",
    "    else:\n",
    "        label = \"b\"\n",
    "    count_words += int(value)\n",
    "    print('%s\\t%s\\t%d' % (label, word, int(value)))\n",
    "\n",
    "# Print the special value 1000000000 to mark the total count of words\n",
    "print('a\\t%d\\t1000000000' % (count_words))\n",
    "print('b\\t%d\\t1000000000' % (count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.2.1D\n",
    "## This function expects keys to come in sorted.  Special values from the mapper, value 1000000000\n",
    "## is used here to mark any total word count counts.\n",
    "## Because this values is so high, it will come in first from the mappers, so the total word\n",
    "## count and total unique word count can be calculated first and then used for other calculations\n",
    "## in the function.  The reducer can quickly calculate the relative frequency of a word\n",
    "## by dividing by the total word count.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "count_words = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd and 3rd items as label and key and value\n",
    "    label = items[0]\n",
    "    key = items[1]\n",
    "    value = int(items[2])\n",
    "    # If the value equals the special number 1000000000, this indicates this is a total \n",
    "    # word count value and it should be added to the count_words total\n",
    "    if value == 1000000000:\n",
    "        count_words += float(key)\n",
    "    else:\n",
    "        print('%s\\t%s\\t%f' % (key, value, value/count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 16:27:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/outputHW3-2-1/*': No such file or directory\n",
      "16/05/28 16:27:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 16:27:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar7883164916772480020/] [] /tmp/streamjob8177213495908193641.jar tmpDir=null\n",
      "16/05/28 16:27:54 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 16:27:54 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 16:27:55 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/05/28 16:27:55 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/05/28 16:27:55 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/28 16:27:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0211\n",
      "16/05/28 16:27:56 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0211\n",
      "16/05/28 16:27:56 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0211/\n",
      "16/05/28 16:27:56 INFO mapreduce.Job: Running job: job_1463787494457_0211\n",
      "16/05/28 16:28:01 INFO mapreduce.Job: Job job_1463787494457_0211 running in uber mode : false\n",
      "16/05/28 16:28:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 16:28:07 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/05/28 16:28:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 16:28:13 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/05/28 16:28:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 16:28:14 INFO mapreduce.Job: Job job_1463787494457_0211 completed successfully\n",
      "16/05/28 16:28:14 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3172\n",
      "\t\tFILE: Number of bytes written=604420\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2566\n",
      "\t\tHDFS: Number of bytes written=3708\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6362\n",
      "\t\tTotal time spent by all map tasks (ms)=14400\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6362\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14400\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6362\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14745600\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6514688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=171\n",
      "\t\tMap output records=177\n",
      "\t\tMap output bytes=2806\n",
      "\t\tMap output materialized bytes=3196\n",
      "\t\tInput split bytes=276\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=177\n",
      "\t\tReduce shuffle bytes=3196\n",
      "\t\tReduce input records=177\n",
      "\t\tReduce output records=171\n",
      "\t\tSpilled Records=354\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=624\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tPhysical memory (bytes) snapshot=1053052928\n",
      "\t\tVirtual memory (bytes) snapshot=10490544128\n",
      "\t\tTotal committed heap usage (bytes)=808452096\n",
      "\tMapper Counters\n",
      "\t\tCalls=3\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2290\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3708\n",
      "16/05/28 16:28:14 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-2-1\n"
     ]
    }
   ],
   "source": [
    "# -combiner reducer.py\n",
    "#-combiner Ireducer.py\n",
    "\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-2-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-2-1\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/cc_wordcount_1,/user/hadoop/cc_wordcount_2 -output /user/hadoop/outputHW3-2-1 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 16:29:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"loan\t107254\t0.109596\n",
      "modification\t70487\t0.072026\n",
      "credit\t50894\t0.052005\n",
      "servicing\t36767\t0.037570\n",
      "report\t30546\t0.031213\n",
      "incorrect\t29069\t0.029704\n",
      "information\t29069\t0.029704\n",
      "on\t29069\t0.029704\n",
      "or\t22533\t0.023025\n",
      "debt\t19309\t0.019731\n",
      "and\t16448\t0.016807\n",
      "\"account\t16205\t0.016559\n",
      "opening\t16205\t0.016559\n",
      "club\t12545\t0.012819\n",
      "health\t12545\t0.012819\n",
      "/\t12386\t0.012656\n",
      "not\t12353\t0.012623\n",
      "loan\t12237\t0.012504\n",
      "attempts\t11848\t0.012107\n",
      "collect\t11848\t0.012107\n",
      "cont'd\t11848\t0.012107\n",
      "owed\t11848\t0.012107\n",
      "of\t10885\t0.011123\n",
      "my\t10731\t0.010965\n",
      "deposits\t10555\t0.010785\n",
      "withdrawals\t10555\t0.010785\n",
      "problems\t9484\t0.009691\n",
      "\"application\t8625\t0.008813\n",
      "to\t8401\t0.008584\n",
      "unable\t8178\t0.008357\n",
      "billing\t8158\t0.008336\n",
      "other\t7886\t0.008058\n",
      "disputes\t6938\t0.007089\n",
      "communication\t6920\t0.007071\n",
      "tactics\t6920\t0.007071\n",
      "reporting\t6559\t0.006702\n",
      "lease\t6337\t0.006475\n",
      "the\t6248\t0.006384\n",
      "being\t5663\t0.005787\n",
      "by\t5663\t0.005787\n",
      "caused\t5663\t0.005787\n",
      "funds\t5663\t0.005787\n",
      "low\t5663\t0.005787\n",
      "process\t5505\t0.005625\n",
      "disclosure\t5214\t0.005328\n",
      "verification\t5214\t0.005328\n",
      "managing\t5006\t0.005115\n",
      "company's\t4858\t0.004964\n",
      "investigation\t4858\t0.004964\n",
      "identity\t4729\t0.004832\n",
      "account\t4476\t0.004574\n",
      "card\t4405\t0.004501\n",
      "get\t4357\t0.004452\n",
      "report/credit\t4357\t0.004452\n",
      "score\t4357\t0.004452\n",
      "costs\t4350\t0.004445\n",
      "settlement\t4350\t0.004445\n",
      "improper\t4309\t0.004403\n",
      "interest\t4238\t0.004331\n",
      "protection\t4139\t0.004229\n",
      "when\t4095\t0.004184\n",
      "repaying\t3844\t0.003928\n",
      "your\t3844\t0.003928\n",
      "fraud\t3842\t0.003926\n",
      "are\t3821\t0.003904\n",
      "pay\t3821\t0.003904\n",
      "you\t3821\t0.003904\n",
      "a\t3503\t0.003579\n",
      "apr\t3431\t0.003506\n",
      "rate\t3431\t0.003506\n",
      "embezzlement\t3276\t0.003348\n",
      "theft\t3276\t0.003348\n",
      "\"making/receiving\t3226\t0.003296\n",
      "payments\t3226\t0.003296\n",
      "fee\t3198\t0.003268\n",
      "contact\t3053\t0.003120\n",
      "16/05/28 16:29:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "info\t2896\t0.002959\n",
      "sharing\t2832\t0.002894\n",
      "closing/cancelling\t2795\t0.002856\n",
      "decision\t2774\t0.002835\n",
      "underwriting\t2774\t0.002835\n",
      "customer\t2734\t0.002794\n",
      "false\t2508\t0.002563\n",
      "representation\t2508\t0.002563\n",
      "statements\t2508\t0.002563\n",
      "action\t2505\t0.002560\n",
      "an\t2505\t0.002560\n",
      "illegal\t2505\t0.002560\n",
      "taking/threatening\t2505\t0.002560\n",
      "atm\t2422\t0.002475\n",
      "debit\t2422\t0.002475\n",
      "using\t2422\t0.002475\n",
      "lender\t2165\t0.002212\n",
      "can't\t1999\t0.002043\n",
      "dealing\t1944\t0.001986\n",
      "servicer\t1944\t0.001986\n",
      "with\t1944\t0.001986\n",
      "collection\t1907\t0.001949\n",
      "late\t1797\t0.001836\n",
      "line\t1732\t0.001770\n",
      "repay\t1647\t0.001683\n",
      "service\t1518\t0.001551\n",
      "determination\t1490\t0.001523\n",
      "transaction\t1485\t0.001517\n",
      "use\t1477\t0.001509\n",
      "monitoring\t1453\t0.001485\n",
      "relations\t1367\t0.001397\n",
      "out\t1242\t0.001269\n",
      "taking\t1242\t0.001269\n",
      "statement\t1220\t0.001247\n",
      "advertising\t1193\t0.001219\n",
      "marketing\t1193\t0.001219\n",
      "payoff\t1155\t0.001180\n",
      "increase/decrease\t1149\t0.001174\n",
      "issue\t1098\t0.001122\n",
      "delinquent\t1061\t0.001084\n",
      "practices\t1003\t0.001025\n",
      "rewards\t1002\t0.001024\n",
      "charged\t976\t0.000997\n",
      "for\t929\t0.000949\n",
      "didn't\t925\t0.000945\n",
      "i\t925\t0.000945\n",
      "dispute\t904\t0.000924\n",
      "expect\t807\t0.000825\n",
      "fees\t807\t0.000825\n",
      "shopping\t672\t0.000687\n",
      "issuance\t640\t0.000654\n",
      "unsolicited\t640\t0.000654\n",
      "balance\t597\t0.000610\n",
      "transfer\t597\t0.000610\n",
      "scam\t566\t0.000578\n",
      "issues\t538\t0.000550\n",
      "money\t413\t0.000422\n",
      "changes\t350\t0.000358\n",
      "forbearance\t350\t0.000358\n",
      "plans\t350\t0.000358\n",
      "terms\t350\t0.000358\n",
      "workout\t350\t0.000358\n",
      "getting\t291\t0.000297\n",
      "available\t274\t0.000280\n",
      "promised\t274\t0.000280\n",
      "was\t274\t0.000280\n",
      "application\t243\t0.000248\n",
      "delay\t243\t0.000248\n",
      "processing\t243\t0.000248\n",
      "advance\t240\t0.000245\n",
      "cash\t240\t0.000245\n",
      "privacy\t240\t0.000245\n",
      "bankruptcy\t222\t0.000227\n",
      "received\t216\t0.000221\n",
      "bank\t202\t0.000206\n",
      "wrong\t169\t0.000173\n",
      "arbitration\t168\t0.000172\n",
      "acct\t163\t0.000167\n",
      "applied\t139\t0.000142\n",
      "loan/did\t139\t0.000142\n",
      "receive\t139\t0.000142\n",
      "sale\t139\t0.000142\n",
      "charges\t131\t0.000134\n",
      "stop\t131\t0.000134\n",
      "overlimit\t127\t0.000130\n",
      "apply\t118\t0.000121\n",
      "amount\t98\t0.000100\n",
      "credited\t92\t0.000094\n",
      "payment\t92\t0.000094\n",
      "checks\t75\t0.000077\n",
      "convenience\t75\t0.000077\n",
      "amt\t71\t0.000073\n",
      "day\t71\t0.000073\n",
      "disclosures\t64\t0.000065\n",
      "incorrect/missing\t64\t0.000065\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.2.1D ########################\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2-1/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-2-1/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.3.** Shopping Cart Analysis.  Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.  \n",
    "\n",
    "ANSWER:  See hadoop output labeled 'OUTPUT FOR HW3.3'\n",
    "\n",
    "Number of Unique Products: 12592  \n",
    "Largest Basket: 37\t\n",
    "\n",
    "| Product ID | Frequency | Relative frequency |  \n",
    "| - | - | - |  \n",
    "| DAI62779 | 6667 | 0.017507 |  \n",
    "| FRO40251 | 3881 | 0.010191 |  \n",
    "| ELE17451 | 3875 | 0.010175 |  \n",
    "| GRO73461 | 3602 | 0.009458 |  \n",
    "| SNA80324 | 3044 | 0.007993 |  \n",
    "| ELE32164 | 2851 | 0.007486 |  \n",
    "| DAI75645 | 2736 | 0.007184 |  \n",
    "| SNA45677 | 2455 | 0.006447 |  \n",
    "| FRO31317 | 2330 | 0.006118 |  \n",
    "| DAI85309 | 2293 | 0.006021 |  \n",
    "\n",
    "...for the remaining products: See hadoop output labeled 'OUTPUT FOR HW3.3' below>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 20:18:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 32 items\n",
      "-rw-r--r--   3 hadoop supergroup   50906486 2016-05-26 23:08 /user/hadoop/Consumer_Complaints.csv\n",
      "-rw-r--r--   3 hadoop supergroup      16478 2016-05-27 00:04 /user/hadoop/Consumer_Complaints_small.csv\n",
      "-rw-r--r--   3 hadoop supergroup    3458517 2016-05-28 17:14 /user/hadoop/ProductPurchaseData.txt\n",
      "-rw-r--r--   3 hadoop supergroup      10918 2016-05-28 20:17 /user/hadoop/ProductPurchaseData_small.txt\n",
      "-rw-r--r--   3 hadoop supergroup        964 2016-05-27 18:04 /user/hadoop/cc_wordcount_1\n",
      "-rw-r--r--   3 hadoop supergroup       1205 2016-05-27 18:05 /user/hadoop/cc_wordcount_2\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "-rw-r--r--   3 hadoop supergroup         31 2016-05-27 13:35 /user/hadoop/hw3-2_strings.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:54 /user/hadoop/outputHW2-0-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 11:56 /user/hadoop/outputHW2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-25 23:36 /user/hadoop/outputHW2-2\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 22:39 /user/hadoop/outputHW2-3\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:00 /user/hadoop/outputHW2-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 12:01 /user/hadoop/outputHW2-3b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 16:41 /user/hadoop/outputHW2-4b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 14:44 /user/hadoop/outputHW3-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 16:28 /user/hadoop/outputHW3-2-1\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 17:45 /user/hadoop/outputHW3-2a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-27 17:58 /user/hadoop/outputHW3-2b\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 13:00 /user/hadoop/outputHW3-2c\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 12:50 /user/hadoop/outputHW3-2d\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 19:05 /user/hadoop/outputHW3-3a\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-28 19:16 /user/hadoop/outputHW3-3b\n",
      "-rw-r--r--   3 hadoop supergroup     142676 2016-05-28 18:07 /user/hadoop/product_counts\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n",
      "-rw-r--r--   3 hadoop supergroup         44 2016-05-26 09:20 /user/hadoop/strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup         78 2016-05-27 16:59 /user/hadoop/test_strings.txt\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-26 12:52 /user/hadoop/word_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the ProductPurchaseData.txt input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal ProductPurchaseData.txt /user/hadoop\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal ProductPurchaseData_small.txt /user/hadoop\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing max_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile max_mapper.py\n",
    "#!/usr/bin/env python\n",
    "## max_mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.3A\n",
    "## Print the count of 1 for each unique product in the input file and print the\n",
    "## largest basket size with a special marker: *largestbasket\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "largest_basket = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Break up each line by space delimiter and print each product with a count of 1.\n",
    "# Keep track of the largest basket size and print it with a special marker:  *largestbasket\n",
    "# so that it is passed to the reducer\n",
    "for line in sys.stdin:\n",
    "    basket_size = 0\n",
    "    for product in line.split():\n",
    "        basket_size += 1\n",
    "        print('%s\\t%d' % (product, 1))\n",
    "    if basket_size > largest_basket:\n",
    "        largest_basket = basket_size\n",
    "print('*largestbasket\\t%d' % (largest_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting max_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile max_reducer.py\n",
    "#!/usr/bin/env python\n",
    "## max_reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.3\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        if key == \"*largestbasket\":\n",
    "            cur_count = max(cur_count, int(value))\n",
    "        else:\n",
    "            cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 18:48:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:48:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-3a/_SUCCESS\n",
      "16/05/29 18:48:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-3a/part-00000\n",
      "16/05/29 18:48:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:48:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5454150486823369875/] [] /tmp/streamjob2241486386734461250.jar tmpDir=null\n",
      "16/05/29 18:48:51 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 18:48:51 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 18:48:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 18:48:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 18:48:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0251\n",
      "16/05/29 18:48:52 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0251\n",
      "16/05/29 18:48:52 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0251/\n",
      "16/05/29 18:48:52 INFO mapreduce.Job: Running job: job_1463787494457_0251\n",
      "16/05/29 18:48:57 INFO mapreduce.Job: Job job_1463787494457_0251 running in uber mode : false\n",
      "16/05/29 18:48:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 18:49:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 18:49:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 18:49:09 INFO mapreduce.Job: Job job_1463787494457_0251 completed successfully\n",
      "16/05/29 18:49:09 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950758\n",
      "\t\tFILE: Number of bytes written=10257468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462069\n",
      "\t\tHDFS: Number of bytes written=142676\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8811\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3103\n",
      "\t\tTotal time spent by all map tasks (ms)=8811\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3103\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8811\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3103\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9022464\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3177472\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=4189100\n",
      "\t\tMap output materialized bytes=4950764\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=4950764\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=221\n",
      "\t\tCPU time spent (ms)=3870\n",
      "\t\tPhysical memory (bytes) snapshot=658784256\n",
      "\t\tVirtual memory (bytes) snapshot=6294740992\n",
      "\t\tTotal committed heap usage (bytes)=498597888\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142676\n",
      "16/05/29 18:49:09 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-3a\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Run a product count of the products in the ProductPurchaseData.txt file\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-3a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-3a\n",
    "# Run a Hadoop streaming job.  The input file 'ProductPurchaseData.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files max_mapper.py,max_reducer.py -mapper max_mapper.py -reducer max_reducer.py \\\n",
    "-input /user/hadoop/ProductPurchaseData.txt -output /user/hadoop/outputHW3-3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 18:49:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:49:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/product_counts\n",
      "16/05/29 18:49:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:49:59 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/05/29 18:50:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:50:00 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#################### HW3.3 PRODUCT WORD COUNTS ########################\n",
    "# Print the output file\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-3a/part-00000\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/product_counts\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-3a/part-00000 /user/hadoop/product_counts\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/product_counts product_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.3\n",
    "## This function passes the products and counts to the reducer.  It also sums up the\n",
    "## total product count (count_products) and total unique products (count_unique) and stores the \n",
    "## largest basket size.  It passes these values on to the reducer using special marker\n",
    "## value 1000000000, 2000000000 and 3000000000.  This technique is called order inversion.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize the counts to 0\n",
    "count_products = 0\n",
    "largest_basket = 0\n",
    "count_unique = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Sum up the total product count and total count of unique products\n",
    "# and pass the largestbasket value through to the reducer\n",
    "# Print each product and its count for the reducer\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    product, count = line.split('\\t')\n",
    "    if product == \"*largestbasket\":\n",
    "        largest_basket = int(count)\n",
    "    else:\n",
    "        count_products += int(count)\n",
    "        count_unique += 1\n",
    "        print('%s\\t%d' % (product, int(count)))\n",
    "\n",
    "# Print the special values 1000000000, 2000000000 and 3000000000 to mark the total counts\n",
    "# and largest basket size\n",
    "print('%d\\t1000000000' % (count_products))\n",
    "print('%d\\t2000000000' % (largest_basket))\n",
    "print('%d\\t3000000000' % (count_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.3\n",
    "## This function expects keys to come in sorted.  Special values from the mapper, value 1000000000,\n",
    "## 2000000000 and 3000000000 are used here to mark total product counts and the largest basket size.\n",
    "## Because these values are so high, it will come in first from the mappers and used in the rest of \n",
    "## the function.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize the counters\n",
    "count_products = 0\n",
    "largest_basket = 0\n",
    "count_unique = 0\n",
    "i = 0\n",
    "\n",
    "READING_COUNTERS = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd and items as key and value\n",
    "    key = items[0]\n",
    "    value = int(items[1])\n",
    "    # If the value equals the special number 1000000000, this indicates this is a total \n",
    "    # product count value and it should be added to the count_products total\n",
    "    if value == 1000000000:\n",
    "        count_products += float(key)\n",
    "    # If the value equals the special number 2000000000, this indicates this is a \n",
    "    # largest basket marker and the max of the current largest basket should be compared\n",
    "    # to it and the max of those values set to the current largest basket\n",
    "    elif value == 2000000000:\n",
    "        largest_basket = max(largest_basket, int(key))\n",
    "    # If the value equals the special number 3000000000, this indicates this is a total \n",
    "    # unique product count value and it should be added to the count_unique total\n",
    "    elif value == 3000000000:\n",
    "        count_unique += int(key)\n",
    "    else:\n",
    "        # If its done reading the counters, then print the counters as reducer output\n",
    "        if READING_COUNTERS:\n",
    "            print('# Unique Products: %d' % (count_unique))\n",
    "            print('Largest Basket: %d' % (largest_basket))\n",
    "            READING_COUNTERS = False\n",
    "        # Only print the top 50 most frequent products\n",
    "        if i < 50:\n",
    "            print('%s\\t%s\\t%f' % (key, value, value/count_products))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 18:50:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:50:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-3b/_SUCCESS\n",
      "16/05/29 18:50:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-3b/part-00000\n",
      "16/05/29 18:50:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 18:50:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar8323011921678963961/] [] /tmp/streamjob6646349792872948167.jar tmpDir=null\n",
      "16/05/29 18:51:00 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 18:51:00 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 18:51:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 18:51:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 18:51:00 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/29 18:51:00 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/29 18:51:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0252\n",
      "16/05/29 18:51:00 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0252\n",
      "16/05/29 18:51:00 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0252/\n",
      "16/05/29 18:51:00 INFO mapreduce.Job: Running job: job_1463787494457_0252\n",
      "16/05/29 18:51:05 INFO mapreduce.Job: Job job_1463787494457_0252 running in uber mode : false\n",
      "16/05/29 18:51:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 18:51:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 18:51:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 18:51:16 INFO mapreduce.Job: Job job_1463787494457_0252 completed successfully\n",
      "16/05/29 18:51:16 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180553\n",
      "\t\tFILE: Number of bytes written=718762\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=145250\n",
      "\t\tHDFS: Number of bytes written=1196\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7254\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2014\n",
      "\t\tTotal time spent by all map tasks (ms)=7254\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2014\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7254\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2014\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7428096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2062336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=12598\n",
      "\t\tMap output bytes=155351\n",
      "\t\tMap output materialized bytes=180559\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12598\n",
      "\t\tReduce shuffle bytes=180559\n",
      "\t\tReduce input records=12598\n",
      "\t\tReduce output records=52\n",
      "\t\tSpilled Records=25196\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=230\n",
      "\t\tCPU time spent (ms)=2050\n",
      "\t\tPhysical memory (bytes) snapshot=671121408\n",
      "\t\tVirtual memory (bytes) snapshot=6298066944\n",
      "\t\tTotal committed heap usage (bytes)=499122176\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=145066\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1196\n",
      "16/05/29 18:51:16 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-3b\n"
     ]
    }
   ],
   "source": [
    "####################### HW3.3 ######################\n",
    "# Run a job to find the number of unique products, the largest basket size and the to 50 most frequently bought products.\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-3b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-3b\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/product_counts -output /user/hadoop/outputHW3-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 18:51:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "# Unique Products: 12592\t\n",
      "Largest Basket: 37\t\n",
      "DAI62779\t6667\t0.017507\n",
      "FRO40251\t3881\t0.010191\n",
      "ELE17451\t3875\t0.010175\n",
      "GRO73461\t3602\t0.009458\n",
      "SNA80324\t3044\t0.007993\n",
      "ELE32164\t2851\t0.007486\n",
      "DAI75645\t2736\t0.007184\n",
      "SNA45677\t2455\t0.006447\n",
      "FRO31317\t2330\t0.006118\n",
      "DAI85309\t2293\t0.006021\n",
      "ELE26917\t2292\t0.006019\n",
      "FRO80039\t2233\t0.005864\n",
      "GRO21487\t2115\t0.005554\n",
      "SNA99873\t2083\t0.005470\n",
      "GRO59710\t2004\t0.005262\n",
      "GRO71621\t1920\t0.005042\n",
      "FRO85978\t1918\t0.005036\n",
      "GRO30386\t1840\t0.004832\n",
      "ELE74009\t1816\t0.004769\n",
      "GRO56726\t1784\t0.004685\n",
      "DAI63921\t1773\t0.004656\n",
      "GRO46854\t1756\t0.004611\n",
      "ELE66600\t1713\t0.004498\n",
      "DAI83733\t1712\t0.004496\n",
      "FRO32293\t1702\t0.004469\n",
      "ELE66810\t1697\t0.004456\n",
      "SNA55762\t1646\t0.004322\n",
      "DAI22177\t1627\t0.004272\n",
      "FRO78087\t1531\t0.004020\n",
      "ELE99737\t1516\t0.003981\n",
      "ELE34057\t1489\t0.003910\n",
      "GRO94758\t1489\t0.003910\n",
      "FRO35904\t1436\t0.003771\n",
      "FRO53271\t1420\t0.003729\n",
      "SNA93860\t1407\t0.003695\n",
      "SNA90094\t1390\t0.003650\n",
      "GRO38814\t1352\t0.003550\n",
      "ELE56788\t1345\t0.003532\n",
      "GRO61133\t1321\t0.003469\n",
      "DAI88807\t1316\t0.003456\n",
      "ELE74482\t1316\t0.003456\n",
      "ELE59935\t1311\t0.003443\n",
      "SNA96271\t1295\t0.003401\n",
      "DAI43223\t1290\t0.003387\n",
      "ELE91337\t1289\t0.003385\n",
      "GRO15017\t1275\t0.003348\n",
      "DAI31081\t1261\t0.003311\n",
      "GRO81087\t1220\t0.003204\n",
      "DAI22896\t1219\t0.003201\n",
      "GRO85051\t1214\t0.003188\n"
     ]
    }
   ],
   "source": [
    "#################### OUTPUT FOR HW3.3 ########################\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-3b/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.4** (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a map-reduce program to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "**HW3.4A**  List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset) in decreasing order of support  for frequent (100>count) itemsets of size 2.  Use the Pairs pattern (lecture 3) to extract these frequent itemsets of size 2. Free free to use combiners if they bring value.  Please output records of the following form for the top 50 pairs (itemsets of size 2):  \n",
    "\n",
    "      item1, item2, support count, support  \n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), and break ties in support (between pairs, if any exist) by taking the first ones in lexicographically increasing order.  \n",
    "\n",
    "ANSWER:  See the Hadoop output marked 'OUTPUT FOR HW3.4'.  Below are the 10 most frequent pairs.  The full list of 50 pairs is in the output section marked 'OUTPUT FOR HW3.4' below.  \n",
    "\n",
    "| Product 1 | Product 2 | Pair Frequency | Pair Relative frequency |  \n",
    "| - | - | - | - |  \n",
    "| DAI62779 | ELE17451 | 1592 | 0.051188 |  \t\n",
    "| FRO40251 | SNA80324 | 1412 | 0.045400 |  \n",
    "| DAI75645 | FRO40251 | 1254 | 0.040320 |  \n",
    "| FRO40251 | GRO85051 | 1213 | 0.039002 |  \n",
    "| DAI62779 | GRO73461 | 1139 | 0.036623 |  \n",
    "| DAI75645 | SNA80324 | 1130 | 0.036333 |  \n",
    "| DAI62779 | FRO40251 | 1070 | 0.034404 |  \n",
    "| DAI62779 | SNA80324 | 923 | 0.029678 |  \n",
    "| DAI62779 | DAI85309 | 918 | 0.029517 |  \n",
    "| ELE32164 | GRO59710 | 911 | 0.029292 |  \n",
    "\n",
    "**HW3.4B**  Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)  \n",
    "\n",
    "Computational setup:  singler server, dual core, 4G RAM, Centos 7.0  \n",
    "\n",
    "Times for Step 1 - Wordcount MR job:  \n",
    "Mappers:  2, Reducers:  1  \n",
    "\n",
    "\t\tTotal time spent by all map tasks (ms)=21,150  \n",
    "\t\tTotal time spent by all reduce tasks (ms)=4,049  \n",
    "        Total time spent by all tasks (ms)=25,199\n",
    "\n",
    "Times for Step 2 - Sorting MR job:  \n",
    "Mappers:  2, Reducers:  1  \n",
    "\n",
    "\t\tTotal time spent by all map tasks (ms)=6,594\n",
    "\t\tTotal time spent by all reduce tasks (ms)=1,908\n",
    "        Total time spent by all tasks (ms)=8,502\n",
    "\n",
    "**HW3.4C**  Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.  \n",
    "Counters for Step 1 - Wordcount MR job:  \n",
    "\n",
    "\tCombiner Counters\n",
    "\t\tCalls=2\n",
    "\tMapper Counters\n",
    "\t\tCalls=2\n",
    "\tReducer Counters\n",
    "\t\tCalls=1\n",
    "\n",
    "Counters for Step 2 - Sorting MR job:  \n",
    "\n",
    "\tMapper Counters\n",
    "\t\tIdentity Calls=2\n",
    "\tReducer Counters\n",
    "\t\tCalls=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pair_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pair_mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.4A\n",
    "## Print the count of 1 for each unique product pair in each basket in the input file and print the\n",
    "## basket count with the special marker: (*basketcount,*basketcount)\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "count_basket = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "# Break up each line by space delimiter and print each product pair in each basket with a count of 1.\n",
    "# Keep track of the basket count and print it with a special marker: (*basketcount,*basketcount)\n",
    "# so that it is passed to the reducer\n",
    "for line in sys.stdin:\n",
    "    products = line.split()\n",
    "    for i in range(len(products)-1):\n",
    "        pi = products[i]\n",
    "        for pj in products[i+1:]:\n",
    "            if pi > pj:\n",
    "                print('(%s,%s)\\t%d' % (pj, pi, 1))\n",
    "            else:\n",
    "                print('(%s,%s)\\t%d' % (pi, pj, 1))\n",
    "    count_basket += 1\n",
    "\n",
    "print('(*basketcount,*basketcount)\\t%d' % (count_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pair_combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pair_combiner.py\n",
    "#!/usr/bin/env python\n",
    "## pair_combiner.py\n",
    "## Author: Megan Jasek\n",
    "## Description: combiner code for HW3.4A\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Combiner Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pair_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pair_reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.4A\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each key with its total sum of all of its values.  It only outputs keys with\n",
    "## values of at least 100.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    value = items[1]\n",
    "    # If this key is the same as the previous key add the value to the count\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            if cur_count >= 100:\n",
    "                print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "if cur_count >= 100:\n",
    "    print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 22:18:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 22:18:49 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-4a/_SUCCESS\n",
      "16/05/28 22:18:49 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-4a/part-00000\n",
      "16/05/28 22:18:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 22:18:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar6491861718591967964/] [] /tmp/streamjob9127244375347921682.jar tmpDir=null\n",
      "16/05/28 22:18:51 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 22:18:52 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 22:18:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 22:18:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/28 22:18:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0240\n",
      "16/05/28 22:18:53 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0240\n",
      "16/05/28 22:18:53 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0240/\n",
      "16/05/28 22:18:53 INFO mapreduce.Job: Running job: job_1463787494457_0240\n",
      "16/05/28 22:18:57 INFO mapreduce.Job: Job job_1463787494457_0240 running in uber mode : false\n",
      "16/05/28 22:18:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 22:19:08 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/05/28 22:19:09 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/05/28 22:19:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 22:19:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 22:19:17 INFO mapreduce.Job: Job job_1463787494457_0240 completed successfully\n",
      "16/05/28 22:19:17 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=24677715\n",
      "\t\tFILE: Number of bytes written=49713386\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462069\n",
      "\t\tHDFS: Number of bytes written=32057\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21150\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4049\n",
      "\t\tTotal time spent by all map tasks (ms)=21150\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4049\n",
      "\t\tTotal vcore-seconds taken by all map tasks=21150\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4049\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=21657600\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4146176\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534059\n",
      "\t\tMap output bytes=55749322\n",
      "\t\tMap output materialized bytes=24677721\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=2534059\n",
      "\t\tCombine output records=1026712\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=24677721\n",
      "\t\tReduce input records=1026712\n",
      "\t\tReduce output records=1335\n",
      "\t\tSpilled Records=2053424\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=284\n",
      "\t\tCPU time spent (ms)=11810\n",
      "\t\tPhysical memory (bytes) snapshot=671203328\n",
      "\t\tVirtual memory (bytes) snapshot=6297927680\n",
      "\t\tTotal committed heap usage (bytes)=507510784\n",
      "\tCombiner Counters\n",
      "\t\tCalls=2\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32057\n",
      "16/05/28 22:19:17 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-4a\n"
     ]
    }
   ],
   "source": [
    "####### HW3.4\n",
    "# Step 1: Run a count of the product pairs in each basket in the ProductPurchaseData.txt file\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-4a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-4a\n",
    "# Run a Hadoop streaming job.  The input file 'ProductPurchaseData.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files pair_mapper.py,pair_reducer.py,pair_combiner.py -mapper pair_mapper.py -reducer pair_reducer.py -combiner pair_combiner.py \\\n",
    "-input /user/hadoop/ProductPurchaseData.txt -output /user/hadoop/outputHW3-4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 20:49:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(*basketcount,*basketcount)\t31101\n",
      "(DAI16732,FRO78087)\t106\n",
      "(DAI18527,SNA44451)\t102\n",
      "(DAI22177,DAI31081)\t127\n",
      "(DAI22177,DAI62779)\t382\n",
      "(DAI22177,DAI63921)\t136\n",
      "(DAI22177,DAI75645)\t123\n",
      "(DAI22177,DAI83733)\t126\n",
      "(DAI22177,DAI85309)\t172\n",
      "(DAI22177,ELE17451)\t203\n",
      "(DAI22177,ELE26917)\t134\n",
      "(DAI22177,ELE32164)\t155\n",
      "(DAI22177,ELE34057)\t107\n",
      "(DAI22177,ELE56788)\t134\n",
      "(DAI22177,ELE66600)\t101\n",
      "(DAI22177,ELE66810)\t105\n",
      "(DAI22177,ELE74009)\t108\n",
      "(DAI22177,ELE91337)\t150\n",
      "(DAI22177,FRO31317)\t160\n",
      "(DAI22177,FRO32293)\t128\n",
      "(DAI22177,FRO40251)\t181\n",
      "(DAI22177,FRO66272)\t130\n",
      "(DAI22177,FRO78087)\t107\n",
      "(DAI22177,FRO80039)\t152\n",
      "(DAI22177,FRO85978)\t156\n",
      "(DAI22177,GRO21487)\t122\n",
      "(DAI22177,GRO30386)\t106\n",
      "(DAI22177,GRO46854)\t160\n",
      "(DAI22177,GRO59710)\t120\n",
      "(DAI22177,GRO71621)\t132\n",
      "(DAI22177,GRO73461)\t248\n",
      "(DAI22177,SNA45677)\t145\n",
      "(DAI22177,SNA55762)\t102\n",
      "(DAI22177,SNA80324)\t140\n",
      "(DAI22177,SNA99873)\t148\n",
      "(DAI22240,DAI62779)\t151\n",
      "(DAI22240,ELE17451)\t105\n",
      "(DAI22240,ELE37048)\t219\n",
      "(DAI22240,ELE74482)\t103\n",
      "(DAI22240,FRO40251)\t112\n",
      "(DAI22534,DAI62779)\t123\n",
      "(DAI22896,DAI62779)\t297\n",
      "(DAI22896,DAI75645)\t215\n",
      "(DAI22896,ELE17451)\t193\n",
      "(DAI22896,ELE32164)\t107\n",
      "(DAI22896,ELE74009)\t165\n",
      "(DAI22896,FRO31317)\t167\n",
      "(DAI22896,FRO40251)\t154\n",
      "(DAI22896,FRO53271)\t123\n",
      "(DAI22896,GRO21487)\t114\n",
      "(DAI22896,GRO30386)\t102\n",
      "(DAI22896,GRO38814)\t223\n",
      "(DAI22896,GRO46854)\t114\n",
      "(DAI22896,GRO61133)\t110\n",
      "(DAI22896,GRO73461)\t304\n",
      "(DAI22896,SNA72163)\t227\n",
      "(DAI22896,SNA80324)\t195\n",
      "(DAI23334,DAI62779)\t273\n",
      "(DAI23334,ELE17451)\t100\n",
      "(DAI23334,ELE92920)\t143\n",
      "(DAI29159,DAI62779)\t119\n",
      "(DAI31081,DAI43223)\t123\n",
      "(DAI31081,DAI62779)\t364\n",
      "(DAI31081,DAI75645)\t206\n",
      "(DAI31081,DAI88807)\t134\n",
      "(DAI31081,ELE17451)\t230\n",
      "(DAI31081,ELE32164)\t312\n",
      "(DAI31081,FRO31317)\t109\n",
      "(DAI31081,FRO40251)\t280\n",
      "(DAI31081,FRO53271)\t161\n",
      "(DAI31081,GRO21487)\t133\n",
      "(DAI31081,GRO56989)\t103\n",
      "(DAI31081,GRO59710)\t187\n",
      "(DAI31081,GRO69543)\t174\n",
      "(DAI31081,GRO73461)\t283\n",
      "(DAI31081,GRO85051)\t102\n",
      "(DAI31081,SNA80324)\t189\n",
      "(DAI33336,DAI62779)\t102\n",
      "(DAI35347,DAI62779)\t226\n",
      "(DAI35347,DAI85309)\t100\n",
      "(DAI35347,ELE17451)\t158\n",
      "(DAI35347,ELE26917)\t111\n",
      "(DAI35347,ELE32164)\t123\n",
      "(DAI35347,ELE66810)\t311\n",
      "(DAI35347,FRO31317)\t325\n",
      "(DAI35347,FRO80039)\t113\n",
      "(DAI37288,ELE32164)\t117\n",
      "(DAI38969,DAI62779)\t190\n",
      "(DAI42083,DAI62779)\t117\n",
      "(DAI42083,DAI92600)\t256\n",
      "(DAI42083,ELE17451)\t185\n",
      "(DAI42083,GRO59710)\t113\n",
      "(DAI42083,SNA59903)\t112\n",
      "(DAI42493,DAI62779)\t309\n",
      "(DAI42493,DAI75645)\t167\n",
      "(DAI42493,DAI85309)\t117\n",
      "(DAI42493,DAI88807)\t118\n",
      "(DAI42493,ELE17451)\t192\n",
      "(DAI42493,ELE32164)\t266\n",
      "(DAI42493,ELE74009)\t136\n",
      "(DAI42493,ELE92920)\t142\n",
      "(DAI42493,FRO31317)\t122\n",
      "(DAI42493,FRO40251)\t235\n",
      "(DAI42493,FRO53271)\t103\n",
      "(DAI42493,GRO21487)\t108\n",
      "(DAI42493,GRO59710)\t236\n",
      "(DAI42493,GRO73461)\t144\n",
      "(DAI42493,SNA18336)\t130\n",
      "(DAI42493,SNA80324)\t125\n",
      "(DAI43223,DAI62779)\t459\n",
      "(DAI43223,DAI85309)\t126\n",
      "(DAI43223,ELE17451)\t326\n",
      "(DAI43223,ELE32164)\t711\n",
      "(DAI43223,ELE74009)\t103\n",
      "(DAI43223,ELE92920)\t107\n",
      "(DAI43223,GRO38814)\t102\n",
      "(DAI43223,GRO59710)\t512\n",
      "(DAI43223,GRO73461)\t184\n",
      "(DAI43223,GRO81087)\t111\n",
      "(DAI43223,GRO94758)\t151\n",
      "(DAI43223,SNA45677)\t130\n",
      "(DAI43868,SNA82528)\t288\n",
      "(DAI46755,FRO81176)\t148\n",
      "(DAI48126,DAI62779)\t142\n",
      "(DAI48891,DAI62779)\t219\n",
      "(DAI48891,DAI63921)\t110\n",
      "(DAI48891,DAI83733)\t116\n",
      "(DAI48891,ELE17451)\t121\n",
      "(DAI48891,ELE78169)\t113\n",
      "(DAI48891,GRO36567)\t128\n",
      "(DAI48891,GRO73461)\t117\n",
      "(DAI48891,GRO94758)\t114\n",
      "(DAI48891,SNA45677)\t194\n",
      "(DAI49199,DAI62779)\t179\n",
      "(DAI49199,FRO40251)\t111\n",
      "(DAI49199,SNA99873)\t102\n",
      "(DAI50913,GRO33815)\t137\n",
      "(DAI50921,DAI62779)\t106\n",
      "(DAI51880,ELE26917)\t107\n",
      "(DAI53152,FRO40251)\t140\n",
      "(DAI55148,DAI62779)\t526\n",
      "(DAI55148,DAI75645)\t299\n",
      "(DAI55148,DAI85309)\t131\n",
      "(DAI55148,ELE17451)\t305\n",
      "(DAI55148,ELE32164)\t134\n",
      "(DAI55148,FRO40251)\t343\n",
      "(DAI55148,FRO92469)\t108\n",
      "(DAI55148,GRO73461)\t135\n",
      "(DAI55148,SNA80324)\t339\n",
      "(DAI55911,DAI62779)\t259\n",
      "(DAI55911,DAI75645)\t186\n",
      "(DAI55911,ELE17451)\t127\n",
      "(DAI55911,ELE26917)\t113\n",
      "(DAI55911,ELE32164)\t119\n",
      "(DAI55911,FRO40251)\t232\n",
      "(DAI55911,GRO73461)\t116\n",
      "(DAI55911,GRO85051)\t133\n",
      "(DAI55911,SNA80324)\t165\n",
      "(DAI59508,DAI62779)\t143\n",
      "(DAI62779,DAI63921)\t353\n",
      "(DAI62779,DAI67189)\t136\n",
      "(DAI62779,DAI67621)\t109\n",
      "(DAI62779,DAI73122)\t110\n",
      "(DAI62779,DAI75645)\t882\n",
      "(DAI62779,DAI83733)\t586\n",
      "(DAI62779,DAI83948)\t190\n",
      "(DAI62779,DAI85309)\t918\n",
      "(DAI62779,DAI87448)\t260\n",
      "(DAI62779,DAI88079)\t117\n",
      "(DAI62779,DAI88352)\t158\n",
      "(DAI62779,DAI88807)\t261\n",
      "(DAI62779,DAI91290)\t353\n",
      "(DAI62779,DAI92600)\t177\n",
      "(DAI62779,DAI94679)\t118\n",
      "(DAI62779,DAI95741)\t195\n",
      "(DAI62779,ELE11111)\t232\n",
      "(DAI62779,ELE12792)\t317\n",
      "(DAI62779,ELE12845)\t123\n",
      "(DAI62779,ELE14480)\t230\n",
      "(DAI62779,ELE17451)\t1592\n",
      "(DAI62779,ELE20196)\t103\n",
      "(DAI62779,ELE20398)\t113\n",
      "(DAI62779,ELE20847)\t275\n",
      "(DAI62779,ELE21353)\t216\n",
      "(DAI62779,ELE24630)\t132\n",
      "(DAI62779,ELE25077)\t135\n",
      "(DAI62779,ELE26917)\t650\n",
      "(DAI62779,ELE32164)\t832\n",
      "(DAI62779,ELE34057)\t224\n",
      "(DAI62779,ELE36909)\t107\n",
      "(DAI62779,ELE37048)\t180\n",
      "(DAI62779,ELE37770)\t153\n",
      "(DAI62779,ELE49801)\t148\n",
      "(DAI62779,ELE53126)\t170\n",
      "(DAI62779,ELE56788)\t355\n",
      "(DAI62779,ELE59028)\t370\n",
      "(DAI62779,ELE59935)\t351\n",
      "(DAI62779,ELE60381)\t104\n",
      "(DAI62779,ELE62598)\t228\n",
      "(DAI62779,ELE66600)\t317\n",
      "(DAI62779,ELE66810)\t279\n",
      "(DAI62779,ELE68605)\t175\n",
      "(DAI62779,ELE69552)\t134\n",
      "(DAI62779,ELE74009)\t432\n",
      "(DAI62779,ELE74482)\t254\n",
      "(DAI62779,ELE78169)\t213\n",
      "(DAI62779,ELE81534)\t120\n",
      "(DAI62779,ELE85027)\t119\n",
      "(DAI62779,ELE86561)\t104\n",
      "(DAI62779,ELE87456)\t151\n",
      "(DAI62779,ELE88583)\t147\n",
      "(DAI62779,ELE91337)\t208\n",
      "(DAI62779,ELE92920)\t877\n",
      "(DAI62779,ELE99737)\t401\n",
      "(DAI62779,FRO16142)\t252\n",
      "(DAI62779,FRO19221)\t462\n",
      "(DAI62779,FRO24098)\t133\n",
      "(DAI62779,FRO31317)\t376\n",
      "(DAI62779,FRO32293)\t299\n",
      "(DAI62779,FRO35904)\t291\n",
      "(DAI62779,FRO40251)\t1070\n",
      "(DAI62779,FRO43226)\t107\n",
      "(DAI62779,FRO53271)\t237\n",
      "(DAI62779,FRO61354)\t167\n",
      "(DAI62779,FRO66272)\t276\n",
      "(DAI62779,FRO73056)\t104\n",
      "(DAI62779,FRO75586)\t220\n",
      "(DAI62779,FRO78087)\t482\n",
      "(DAI62779,FRO78994)\t201\n",
      "(DAI62779,FRO79022)\t249\n",
      "(DAI62779,FRO80039)\t550\n",
      "(DAI62779,FRO85978)\t434\n",
      "(DAI62779,FRO89565)\t158\n",
      "(DAI62779,FRO92261)\t223\n",
      "(DAI62779,FRO92469)\t242\n",
      "(DAI62779,FRO98184)\t118\n",
      "(DAI62779,FRO98729)\t147\n",
      "(DAI62779,FRO99372)\t170\n",
      "(DAI62779,FRO99783)\t210\n",
      "(DAI62779,GRO15017)\t391\n",
      "(DAI62779,GRO17075)\t100\n",
      "(DAI62779,GRO18367)\t133\n",
      "(DAI62779,GRO21487)\t471\n",
      "(DAI62779,GRO23573)\t133\n",
      "(DAI62779,GRO24246)\t125\n",
      "(DAI62779,GRO29598)\t102\n",
      "(DAI62779,GRO30386)\t709\n",
      "(DAI62779,GRO30912)\t103\n",
      "(DAI62779,GRO32086)\t156\n",
      "(DAI62779,GRO35122)\t163\n",
      "(DAI62779,GRO36567)\t182\n",
      "(DAI62779,GRO38814)\t389\n",
      "(DAI62779,GRO38983)\t182\n",
      "(DAI62779,GRO44993)\t192\n",
      "(DAI62779,GRO46854)\t461\n",
      "(DAI62779,GRO50832)\t150\n",
      "(DAI62779,GRO56726)\t302\n",
      "(DAI62779,GRO56989)\t155\n",
      "(DAI62779,GRO59710)\t561\n",
      "(DAI62779,GRO61133)\t300\n",
      "(DAI62779,GRO64900)\t205\n",
      "(DAI62779,GRO68067)\t159\n",
      "(DAI62779,GRO69543)\t167\n",
      "(DAI62779,GRO71621)\t595\n",
      "(DAI62779,GRO73461)\t1139\n",
      "(DAI62779,GRO81087)\t396\n",
      "(DAI62779,GRO81647)\t124\n",
      "(DAI62779,GRO82670)\t104\n",
      "(DAI62779,GRO83463)\t186\n",
      "(DAI62779,GRO85051)\t382\n",
      "(DAI62779,GRO88324)\t237\n",
      "(DAI62779,GRO88505)\t147\n",
      "(DAI62779,GRO88511)\t175\n",
      "(DAI62779,GRO89148)\t111\n",
      "(DAI62779,GRO94758)\t479\n",
      "(DAI62779,GRO95519)\t130\n",
      "(DAI62779,GRO99222)\t237\n",
      "(DAI62779,SNA12663)\t164\n",
      "(DAI62779,SNA18336)\t506\n",
      "(DAI62779,SNA20554)\t137\n",
      "(DAI62779,SNA24799)\t216\n",
      "(DAI62779,SNA29014)\t122\n",
      "(DAI62779,SNA30755)\t112\n",
      "(DAI62779,SNA31619)\t101\n",
      "(DAI62779,SNA38068)\t329\n",
      "(DAI62779,SNA40408)\t115\n",
      "(DAI62779,SNA40784)\t207\n",
      "(DAI62779,SNA42528)\t115\n",
      "(DAI62779,SNA43319)\t132\n",
      "(DAI62779,SNA44190)\t106\n",
      "(DAI62779,SNA45677)\t604\n",
      "(DAI62779,SNA47306)\t221\n",
      "(DAI62779,SNA49107)\t118\n",
      "(DAI62779,SNA50789)\t149\n",
      "(DAI62779,SNA53220)\t248\n",
      "(DAI62779,SNA55617)\t101\n",
      "(DAI62779,SNA55762)\t593\n",
      "(DAI62779,SNA55952)\t208\n",
      "(DAI62779,SNA57865)\t152\n",
      "(DAI62779,SNA59061)\t140\n",
      "(DAI62779,SNA59903)\t342\n",
      "(DAI62779,SNA62128)\t174\n",
      "(DAI62779,SNA69641)\t141\n",
      "(DAI62779,SNA72163)\t279\n",
      "(DAI62779,SNA74022)\t101\n",
      "(DAI62779,SNA80324)\t923\n",
      "(DAI62779,SNA82528)\t101\n",
      "(DAI62779,SNA88283)\t178\n",
      "(DAI62779,SNA90094)\t408\n",
      "(DAI62779,SNA90258)\t114\n",
      "(DAI62779,SNA93730)\t154\n",
      "(DAI62779,SNA93860)\t537\n",
      "(DAI62779,SNA95666)\t228\n",
      "(DAI62779,SNA96271)\t442\n",
      "(DAI62779,SNA96466)\t186\n",
      "(DAI62779,SNA99873)\t406\n",
      "(DAI63921,DAI83733)\t142\n",
      "(DAI63921,DAI85309)\t180\n",
      "(DAI63921,DAI91290)\t120\n",
      "(DAI63921,ELE11160)\t100\n",
      "(DAI63921,ELE17451)\t188\n",
      "(DAI63921,ELE26917)\t120\n",
      "(DAI63921,ELE34057)\t107\n",
      "(DAI63921,ELE59935)\t189\n",
      "(DAI63921,ELE66600)\t159\n",
      "(DAI63921,ELE66810)\t108\n",
      "(DAI63921,ELE86561)\t106\n",
      "(DAI63921,ELE91337)\t155\n",
      "(DAI63921,ELE99737)\t108\n",
      "(DAI63921,FRO31317)\t153\n",
      "(DAI63921,FRO32293)\t132\n",
      "(DAI63921,FRO35904)\t121\n",
      "(DAI63921,FRO66272)\t122\n",
      "(DAI63921,FRO80039)\t126\n",
      "(DAI63921,FRO85978)\t262\n",
      "(DAI63921,GRO21487)\t163\n",
      "(DAI63921,GRO44993)\t115\n",
      "(DAI63921,GRO46854)\t192\n",
      "(DAI63921,GRO56726)\t126\n",
      "(DAI63921,GRO61133)\t122\n",
      "(DAI63921,GRO71621)\t144\n",
      "(DAI63921,GRO73461)\t219\n",
      "(DAI63921,SNA12663)\t108\n",
      "(DAI63921,SNA45677)\t178\n",
      "(DAI63921,SNA55762)\t120\n",
      "(DAI63921,SNA55952)\t110\n",
      "(DAI63921,SNA99873)\t137\n",
      "(DAI70456,ELE37770)\t165\n",
      "(DAI73122,ELE32164)\t125\n",
      "(DAI73122,GRO73461)\t146\n",
      "(DAI75645,DAI83733)\t155\n",
      "(DAI75645,DAI83948)\t157\n",
      "(DAI75645,DAI85309)\t212\n",
      "(DAI75645,DAI88079)\t149\n",
      "(DAI75645,DAI88807)\t170\n",
      "(DAI75645,DAI91290)\t129\n",
      "(DAI75645,DAI95741)\t106\n",
      "(DAI75645,ELE11111)\t186\n",
      "(DAI75645,ELE12792)\t109\n",
      "(DAI75645,ELE14480)\t111\n",
      "(DAI75645,ELE17451)\t547\n",
      "(DAI75645,ELE20847)\t306\n",
      "(DAI75645,ELE26917)\t278\n",
      "(DAI75645,ELE34057)\t110\n",
      "(DAI75645,ELE56788)\t130\n",
      "(DAI75645,ELE66600)\t145\n",
      "(DAI75645,ELE66810)\t120\n",
      "(DAI75645,ELE74009)\t286\n",
      "(DAI75645,ELE74482)\t137\n",
      "(DAI75645,ELE92920)\t164\n",
      "(DAI75645,FRO16142)\t166\n",
      "(DAI75645,FRO31317)\t195\n",
      "(DAI75645,FRO32293)\t147\n",
      "(DAI75645,FRO35904)\t117\n",
      "(DAI75645,FRO40251)\t1254\n",
      "(DAI75645,FRO47962)\t189\n",
      "(DAI75645,FRO53271)\t210\n",
      "(DAI75645,FRO78087)\t171\n",
      "(DAI75645,FRO80039)\t175\n",
      "(DAI75645,FRO85978)\t122\n",
      "(DAI75645,FRO92469)\t256\n",
      "(DAI75645,GRO15017)\t228\n",
      "(DAI75645,GRO21487)\t213\n",
      "(DAI75645,GRO30386)\t239\n",
      "(DAI75645,GRO32086)\t105\n",
      "(DAI75645,GRO35122)\t111\n",
      "(DAI75645,GRO38814)\t244\n",
      "(DAI75645,GRO44993)\t120\n",
      "(DAI75645,GRO46854)\t190\n",
      "(DAI75645,GRO56726)\t145\n",
      "(DAI75645,GRO61133)\t204\n",
      "(DAI75645,GRO64900)\t104\n",
      "(DAI75645,GRO69543)\t158\n",
      "(DAI75645,GRO71621)\t211\n",
      "(DAI75645,GRO73461)\t712\n",
      "(DAI75645,GRO81087)\t206\n",
      "(DAI75645,GRO85051)\t395\n",
      "(DAI75645,GRO94758)\t203\n",
      "(DAI75645,SNA18336)\t101\n",
      "(DAI75645,SNA38068)\t124\n",
      "(DAI75645,SNA45677)\t245\n",
      "(DAI75645,SNA47306)\t118\n",
      "(DAI75645,SNA55762)\t255\n",
      "(DAI75645,SNA72163)\t131\n",
      "(DAI75645,SNA80324)\t1130\n",
      "(DAI75645,SNA90094)\t215\n",
      "(DAI75645,SNA93860)\t177\n",
      "(DAI75645,SNA96271)\t175\n",
      "(DAI75645,SNA99873)\t189\n",
      "(DAI83031,DAI94679)\t141\n",
      "(DAI83733,DAI85309)\t293\n",
      "(DAI83733,DAI87448)\t119\n",
      "(DAI83733,ELE17451)\t296\n",
      "(DAI83733,ELE26917)\t143\n",
      "(DAI83733,ELE32164)\t140\n",
      "(DAI83733,ELE59935)\t130\n",
      "(DAI83733,ELE66600)\t105\n",
      "(DAI83733,ELE74009)\t130\n",
      "(DAI83733,ELE91337)\t139\n",
      "(DAI83733,ELE92920)\t111\n",
      "(DAI83733,FRO31317)\t159\n",
      "(DAI83733,FRO32293)\t109\n",
      "(DAI83733,FRO35904)\t107\n",
      "(DAI83733,FRO40251)\t203\n",
      "(DAI83733,FRO79022)\t113\n",
      "(DAI83733,FRO80039)\t156\n",
      "(DAI83733,FRO85978)\t165\n",
      "(DAI83733,GRO21487)\t154\n",
      "(DAI83733,GRO38814)\t130\n",
      "(DAI83733,GRO46854)\t189\n",
      "(DAI83733,GRO56726)\t158\n",
      "(DAI83733,GRO71621)\t136\n",
      "(DAI83733,GRO73461)\t225\n",
      "(DAI83733,SNA45677)\t174\n",
      "(DAI83733,SNA55762)\t108\n",
      "(DAI83733,SNA80324)\t196\n",
      "(DAI83733,SNA99873)\t155\n",
      "(DAI83948,ELE26917)\t108\n",
      "(DAI83948,ELE32164)\t109\n",
      "(DAI83948,FRO40251)\t202\n",
      "(DAI83948,GRO73461)\t156\n",
      "(DAI83948,GRO88511)\t131\n",
      "(DAI83948,SNA80324)\t139\n",
      "(DAI85309,DAI87448)\t134\n",
      "(DAI85309,DAI91290)\t116\n",
      "(DAI85309,DAI95741)\t153\n",
      "(DAI85309,ELE14480)\t100\n",
      "(DAI85309,ELE17451)\t482\n",
      "(DAI85309,ELE26917)\t185\n",
      "(DAI85309,ELE32164)\t259\n",
      "(DAI85309,ELE34057)\t155\n",
      "(DAI85309,ELE56788)\t149\n",
      "(DAI85309,ELE59935)\t146\n",
      "(DAI85309,ELE66600)\t133\n",
      "(DAI85309,ELE66810)\t146\n",
      "(DAI85309,ELE74009)\t157\n",
      "(DAI85309,ELE74482)\t103\n",
      "(DAI85309,ELE91337)\t134\n",
      "(DAI85309,ELE92920)\t201\n",
      "(DAI85309,ELE99737)\t659\n",
      "(DAI85309,FRO19221)\t168\n",
      "(DAI85309,FRO31317)\t199\n",
      "(DAI85309,FRO32293)\t106\n",
      "(DAI85309,FRO35904)\t114\n",
      "(DAI85309,FRO40251)\t284\n",
      "(DAI85309,FRO66272)\t138\n",
      "(DAI85309,FRO78087)\t156\n",
      "(DAI85309,FRO79022)\t194\n",
      "(DAI85309,FRO80039)\t181\n",
      "(DAI85309,FRO85978)\t183\n",
      "(DAI85309,GRO15017)\t122\n",
      "(DAI85309,GRO21487)\t167\n",
      "(DAI85309,GRO30386)\t139\n",
      "(DAI85309,GRO38814)\t119\n",
      "(DAI85309,GRO46854)\t346\n",
      "(DAI85309,GRO56726)\t213\n",
      "(DAI85309,GRO59710)\t146\n",
      "(DAI85309,GRO61133)\t142\n",
      "(DAI85309,GRO71621)\t270\n",
      "(DAI85309,GRO73461)\t338\n",
      "(DAI85309,GRO81087)\t121\n",
      "(DAI85309,GRO94758)\t238\n",
      "(DAI85309,SNA18336)\t173\n",
      "(DAI85309,SNA40784)\t121\n",
      "(DAI85309,SNA45677)\t283\n",
      "(DAI85309,SNA55762)\t211\n",
      "(DAI85309,SNA80324)\t187\n",
      "(DAI85309,SNA90094)\t214\n",
      "(DAI85309,SNA93860)\t143\n",
      "(DAI85309,SNA96271)\t139\n",
      "(DAI85309,SNA99873)\t193\n",
      "(DAI86167,FRO43226)\t118\n",
      "(DAI87448,ELE17451)\t111\n",
      "(DAI87448,ELE30933)\t120\n",
      "(DAI87448,ELE34057)\t106\n",
      "(DAI87448,ELE91337)\t101\n",
      "(DAI87448,FRO40251)\t105\n",
      "(DAI87448,FRO85978)\t115\n",
      "(DAI87448,GRO21487)\t101\n",
      "(DAI87448,GRO46854)\t153\n",
      "(DAI87448,GRO56726)\t156\n",
      "(DAI87448,GRO73461)\t231\n",
      "(DAI88079,ELE17451)\t124\n",
      "(DAI88079,FRO40251)\t446\n",
      "(DAI88079,GRO73461)\t145\n",
      "(DAI88079,SNA80324)\t142\n",
      "(DAI88088,ELE38289)\t180\n",
      "(DAI88352,ELE17451)\t105\n",
      "(DAI88807,ELE17451)\t291\n",
      "(DAI88807,ELE32164)\t194\n",
      "(DAI88807,ELE74009)\t123\n",
      "(DAI88807,FRO16142)\t118\n",
      "(DAI88807,FRO31317)\t104\n",
      "(DAI88807,FRO32293)\t105\n",
      "(DAI88807,FRO40251)\t146\n",
      "(DAI88807,FRO53271)\t139\n",
      "(DAI88807,FRO80039)\t129\n",
      "(DAI88807,GRO21487)\t140\n",
      "(DAI88807,GRO38814)\t102\n",
      "(DAI88807,GRO59710)\t139\n",
      "(DAI88807,GRO61133)\t111\n",
      "(DAI88807,GRO69543)\t109\n",
      "(DAI88807,GRO73461)\t313\n",
      "(DAI88807,SNA59903)\t328\n",
      "(DAI88807,SNA72163)\t394\n",
      "(DAI88807,SNA80324)\t143\n",
      "(DAI88807,SNA93860)\t109\n",
      "(DAI91290,ELE17451)\t219\n",
      "(DAI91290,ELE26917)\t110\n",
      "(DAI91290,ELE32164)\t139\n",
      "(DAI91290,FRO40251)\t185\n",
      "(DAI91290,GRO30386)\t154\n",
      "(DAI91290,GRO59710)\t103\n",
      "(DAI91290,GRO73461)\t161\n",
      "(DAI91290,GRO81087)\t116\n",
      "(DAI91290,SNA80324)\t155\n",
      "(DAI91290,SNA90094)\t102\n",
      "(DAI92600,ELE17451)\t204\n",
      "(DAI92600,ELE32164)\t135\n",
      "(DAI92600,FRO40251)\t121\n",
      "(DAI92600,FRO80039)\t111\n",
      "(DAI92600,GRO21487)\t107\n",
      "(DAI92600,GRO59710)\t127\n",
      "(DAI92600,GRO73461)\t254\n",
      "(DAI92600,SNA59903)\t113\n",
      "(DAI93865,FRO40251)\t208\n",
      "(DAI95741,ELE17451)\t102\n",
      "(DAI95741,FRO40251)\t122\n",
      "(DAI95741,GRO46854)\t155\n",
      "(DAI95741,GRO56726)\t104\n",
      "(DAI95741,GRO73461)\t121\n",
      "(DAI95741,SNA90094)\t120\n",
      "(ELE11111,ELE17451)\t121\n",
      "(ELE11111,ELE32164)\t123\n",
      "(ELE11111,FRO40251)\t157\n",
      "(ELE11111,GRO59710)\t100\n",
      "(ELE11111,GRO73461)\t158\n",
      "(ELE11111,GRO94758)\t101\n",
      "(ELE11111,SNA80324)\t157\n",
      "(ELE12792,ELE17451)\t159\n",
      "(ELE12792,FRO40251)\t122\n",
      "(ELE12792,FRO80039)\t101\n",
      "(ELE12792,GRO73461)\t116\n",
      "(ELE12792,SNA69641)\t161\n",
      "(ELE12792,SNA80324)\t174\n",
      "(ELE12845,FRO35904)\t174\n",
      "(ELE12951,FRO40251)\t105\n",
      "(ELE14480,ELE17451)\t146\n",
      "(ELE14480,ELE32164)\t135\n",
      "(ELE14480,ELE34057)\t108\n",
      "(ELE14480,FRO40251)\t148\n",
      "(ELE14480,FRO80039)\t110\n",
      "(ELE14480,GRO24246)\t102\n",
      "(ELE14480,GRO73461)\t196\n",
      "(ELE14480,SNA62128)\t313\n",
      "(ELE14480,SNA80324)\t100\n",
      "(ELE17451,ELE20398)\t111\n",
      "(ELE17451,ELE20847)\t141\n",
      "(ELE17451,ELE26917)\t314\n",
      "(ELE17451,ELE32164)\t511\n",
      "(ELE17451,ELE34057)\t168\n",
      "(ELE17451,ELE37048)\t135\n",
      "(ELE17451,ELE37770)\t100\n",
      "(ELE17451,ELE53126)\t151\n",
      "(ELE17451,ELE56788)\t208\n",
      "(ELE17451,ELE59028)\t118\n",
      "(ELE17451,ELE59935)\t181\n",
      "(ELE17451,ELE62598)\t153\n",
      "(ELE17451,ELE66600)\t168\n",
      "(ELE17451,ELE66810)\t154\n",
      "(ELE17451,ELE68605)\t115\n",
      "(ELE17451,ELE74009)\t231\n",
      "(ELE17451,ELE74482)\t169\n",
      "(ELE17451,ELE92920)\t384\n",
      "(ELE17451,ELE99737)\t151\n",
      "(ELE17451,FRO16142)\t152\n",
      "(ELE17451,FRO31317)\t359\n",
      "(ELE17451,FRO32293)\t219\n",
      "(ELE17451,FRO35904)\t148\n",
      "(ELE17451,FRO40251)\t697\n",
      "(ELE17451,FRO53271)\t182\n",
      "(ELE17451,FRO66272)\t117\n",
      "(ELE17451,FRO75586)\t151\n",
      "(ELE17451,FRO78087)\t218\n",
      "(ELE17451,FRO80039)\t356\n",
      "(ELE17451,FRO85978)\t173\n",
      "(ELE17451,FRO89565)\t104\n",
      "(ELE17451,FRO92261)\t127\n",
      "(ELE17451,FRO92469)\t165\n",
      "(ELE17451,FRO98184)\t112\n",
      "(ELE17451,FRO99372)\t115\n",
      "(ELE17451,FRO99783)\t108\n",
      "(ELE17451,GRO15017)\t171\n",
      "(ELE17451,GRO21487)\t236\n",
      "(ELE17451,GRO30386)\t468\n",
      "(ELE17451,GRO32086)\t108\n",
      "(ELE17451,GRO35122)\t154\n",
      "(ELE17451,GRO38814)\t160\n",
      "(ELE17451,GRO46854)\t209\n",
      "(ELE17451,GRO56726)\t218\n",
      "(ELE17451,GRO56989)\t129\n",
      "(ELE17451,GRO59710)\t408\n",
      "(ELE17451,GRO61133)\t209\n",
      "(ELE17451,GRO64900)\t113\n",
      "(ELE17451,GRO69543)\t128\n",
      "(ELE17451,GRO71621)\t290\n",
      "(ELE17451,GRO73461)\t580\n",
      "(ELE17451,GRO81087)\t262\n",
      "(ELE17451,GRO83463)\t106\n",
      "(ELE17451,GRO85051)\t217\n",
      "(ELE17451,GRO88324)\t117\n",
      "(ELE17451,GRO94758)\t227\n",
      "(ELE17451,GRO99222)\t148\n",
      "(ELE17451,SNA18336)\t267\n",
      "(ELE17451,SNA30755)\t111\n",
      "(ELE17451,SNA38068)\t185\n",
      "(ELE17451,SNA40784)\t104\n",
      "(ELE17451,SNA45677)\t304\n",
      "(ELE17451,SNA55762)\t303\n",
      "(ELE17451,SNA55952)\t123\n",
      "(ELE17451,SNA57865)\t101\n",
      "(ELE17451,SNA59061)\t100\n",
      "(ELE17451,SNA59903)\t351\n",
      "(ELE17451,SNA62128)\t186\n",
      "(ELE17451,SNA72163)\t272\n",
      "(ELE17451,SNA80324)\t597\n",
      "(ELE17451,SNA88283)\t169\n",
      "(ELE17451,SNA90094)\t182\n",
      "(ELE17451,SNA90258)\t113\n",
      "(ELE17451,SNA93860)\t182\n",
      "(ELE17451,SNA96271)\t203\n",
      "(ELE17451,SNA99873)\t270\n",
      "(ELE20398,GRO73461)\t111\n",
      "(ELE20847,ELE26917)\t110\n",
      "(ELE20847,FRO40251)\t434\n",
      "(ELE20847,FRO75586)\t118\n",
      "(ELE20847,FRO92469)\t122\n",
      "(ELE20847,GRO73461)\t187\n",
      "(ELE20847,GRO85051)\t139\n",
      "(ELE20847,SNA80324)\t410\n",
      "(ELE20847,SNA96271)\t184\n",
      "(ELE21353,FRO19221)\t124\n",
      "(ELE24630,ELE26917)\t127\n",
      "(ELE25077,GRO89004)\t215\n",
      "(ELE26917,ELE32164)\t148\n",
      "(ELE26917,ELE34057)\t111\n",
      "(ELE26917,ELE56788)\t115\n",
      "(ELE26917,ELE59935)\t107\n",
      "(ELE26917,ELE66600)\t189\n",
      "(ELE26917,ELE66810)\t131\n",
      "(ELE26917,ELE74009)\t205\n",
      "(ELE26917,ELE92629)\t114\n",
      "(ELE26917,ELE92920)\t106\n",
      "(ELE26917,FRO31317)\t169\n",
      "(ELE26917,FRO32293)\t126\n",
      "(ELE26917,FRO40251)\t346\n",
      "(ELE26917,FRO78087)\t144\n",
      "(ELE26917,FRO80039)\t181\n",
      "(ELE26917,FRO85978)\t152\n",
      "(ELE26917,FRO98729)\t101\n",
      "(ELE26917,GRO15017)\t164\n",
      "(ELE26917,GRO21487)\t156\n",
      "(ELE26917,GRO30386)\t138\n",
      "(ELE26917,GRO38814)\t104\n",
      "(ELE26917,GRO38983)\t179\n",
      "(ELE26917,GRO44993)\t109\n",
      "(ELE26917,GRO46854)\t116\n",
      "(ELE26917,GRO56726)\t104\n",
      "(ELE26917,GRO59710)\t132\n",
      "(ELE26917,GRO64900)\t112\n",
      "(ELE26917,GRO71621)\t162\n",
      "(ELE26917,GRO73461)\t255\n",
      "(ELE26917,GRO81087)\t121\n",
      "(ELE26917,GRO85051)\t146\n",
      "(ELE26917,GRO94758)\t124\n",
      "(ELE26917,GRO99222)\t192\n",
      "(ELE26917,SNA45677)\t234\n",
      "(ELE26917,SNA47306)\t120\n",
      "(ELE26917,SNA55762)\t138\n",
      "(ELE26917,SNA80324)\t267\n",
      "(ELE26917,SNA96271)\t198\n",
      "(ELE26917,SNA99873)\t194\n",
      "(ELE28189,FRO17734)\t164\n",
      "(ELE30182,SNA24799)\t114\n",
      "(ELE30933,GRO73461)\t119\n",
      "(ELE32164,ELE34057)\t154\n",
      "(ELE32164,ELE37048)\t125\n",
      "(ELE32164,ELE53126)\t105\n",
      "(ELE32164,ELE56788)\t146\n",
      "(ELE32164,ELE62598)\t228\n",
      "(ELE32164,ELE66600)\t138\n",
      "(ELE32164,ELE66810)\t186\n",
      "(ELE32164,ELE68605)\t102\n",
      "(ELE32164,ELE74009)\t258\n",
      "(ELE32164,ELE74482)\t176\n",
      "(ELE32164,ELE92920)\t224\n",
      "(ELE32164,ELE99737)\t162\n",
      "(ELE32164,FRO16142)\t184\n",
      "(ELE32164,FRO31317)\t186\n",
      "(ELE32164,FRO32293)\t177\n",
      "(ELE32164,FRO35904)\t134\n",
      "(ELE32164,FRO40251)\t152\n",
      "(ELE32164,FRO43226)\t121\n",
      "(ELE32164,FRO53271)\t254\n",
      "(ELE32164,FRO78087)\t202\n",
      "(ELE32164,FRO80039)\t207\n",
      "(ELE32164,FRO89565)\t137\n",
      "(ELE32164,GRO15017)\t127\n",
      "(ELE32164,GRO21487)\t230\n",
      "(ELE32164,GRO30386)\t204\n",
      "(ELE32164,GRO32086)\t107\n",
      "(ELE32164,GRO35122)\t100\n",
      "(ELE32164,GRO38814)\t216\n",
      "(ELE32164,GRO46854)\t103\n",
      "(ELE32164,GRO59710)\t911\n",
      "(ELE32164,GRO61133)\t186\n",
      "(ELE32164,GRO69543)\t176\n",
      "(ELE32164,GRO71621)\t182\n",
      "(ELE32164,GRO73461)\t486\n",
      "(ELE32164,GRO81087)\t233\n",
      "(ELE32164,GRO83463)\t104\n",
      "(ELE32164,GRO94758)\t204\n",
      "(ELE32164,SNA18336)\t124\n",
      "(ELE32164,SNA38068)\t138\n",
      "(ELE32164,SNA40784)\t118\n",
      "(ELE32164,SNA45677)\t224\n",
      "(ELE32164,SNA55762)\t140\n",
      "(ELE32164,SNA59903)\t120\n",
      "(ELE32164,SNA62128)\t131\n",
      "(ELE32164,SNA71332)\t102\n",
      "(ELE32164,SNA72163)\t143\n",
      "(ELE32164,SNA80324)\t115\n",
      "(ELE32164,SNA90094)\t159\n",
      "(ELE32164,SNA93860)\t154\n",
      "(ELE32164,SNA96271)\t101\n",
      "(ELE32164,SNA99873)\t135\n",
      "(ELE32244,ELE66600)\t219\n",
      "(ELE34057,ELE66810)\t101\n",
      "(ELE34057,ELE91337)\t126\n",
      "(ELE34057,ELE99737)\t106\n",
      "(ELE34057,FRO31317)\t143\n",
      "(ELE34057,FRO40251)\t131\n",
      "(ELE34057,FRO80039)\t124\n",
      "(ELE34057,FRO85978)\t143\n",
      "(ELE34057,GRO21487)\t124\n",
      "(ELE34057,GRO46854)\t113\n",
      "(ELE34057,GRO59710)\t108\n",
      "(ELE34057,GRO71621)\t115\n",
      "(ELE34057,GRO73461)\t217\n",
      "(ELE34057,SNA45677)\t158\n",
      "(ELE34057,SNA55762)\t113\n",
      "(ELE34057,SNA99873)\t145\n",
      "(ELE37048,ELE53126)\t190\n",
      "(ELE37048,ELE74482)\t115\n",
      "(ELE37048,FRO40251)\t146\n",
      "(ELE37048,SNA80324)\t106\n",
      "(ELE37048,SNA99873)\t106\n",
      "(ELE37770,ELE53126)\t120\n",
      "(ELE37770,ELE74482)\t235\n",
      "(ELE37770,FRO40251)\t103\n",
      "(ELE37798,GRO73461)\t108\n",
      "(ELE38289,SNA66979)\t124\n",
      "(ELE49801,GRO46854)\t131\n",
      "(ELE49801,GRO73461)\t122\n",
      "(ELE49801,SNA93860)\t115\n",
      "(ELE53126,ELE74482)\t109\n",
      "(ELE53126,FRO24098)\t146\n",
      "(ELE53126,FRO31317)\t218\n",
      "(ELE53126,FRO40251)\t129\n",
      "(ELE53126,FRO53271)\t199\n",
      "(ELE53126,GRO32086)\t134\n",
      "(ELE53126,GRO59710)\t103\n",
      "(ELE53126,SNA80324)\t101\n",
      "(ELE53126,SNA99873)\t156\n",
      "(ELE55848,GRO32086)\t105\n",
      "(ELE56788,ELE66810)\t117\n",
      "(ELE56788,FRO31317)\t112\n",
      "(ELE56788,FRO32293)\t106\n",
      "(ELE56788,FRO40251)\t169\n",
      "(ELE56788,FRO78087)\t107\n",
      "(ELE56788,FRO80039)\t114\n",
      "(ELE56788,GRO30386)\t106\n",
      "(ELE56788,GRO59710)\t115\n",
      "(ELE56788,GRO71621)\t130\n",
      "(ELE56788,GRO73461)\t102\n",
      "(ELE56788,SNA45677)\t123\n",
      "(ELE56788,SNA80324)\t148\n",
      "(ELE59028,FRO85978)\t167\n",
      "(ELE59028,SNA93860)\t161\n",
      "(ELE59935,FRO40251)\t134\n",
      "(ELE59935,FRO80039)\t182\n",
      "(ELE59935,GRO30386)\t142\n",
      "(ELE59935,GRO46854)\t149\n",
      "(ELE59935,GRO73461)\t116\n",
      "(ELE59935,SNA45677)\t101\n",
      "(ELE59935,SNA80324)\t105\n",
      "(ELE62598,FRO40251)\t154\n",
      "(ELE62598,GRO59710)\t106\n",
      "(ELE62598,GRO73461)\t121\n",
      "(ELE62598,SNA38068)\t122\n",
      "(ELE66600,ELE66810)\t132\n",
      "(ELE66600,ELE74009)\t128\n",
      "(ELE66600,ELE91337)\t140\n",
      "(ELE66600,FRO31317)\t124\n",
      "(ELE66600,FRO40251)\t196\n",
      "(ELE66600,FRO78087)\t263\n",
      "(ELE66600,FRO80039)\t126\n",
      "(ELE66600,FRO85978)\t165\n",
      "(ELE66600,FRO98729)\t100\n",
      "(ELE66600,GRO15017)\t105\n",
      "(ELE66600,GRO21487)\t141\n",
      "(ELE66600,GRO38983)\t133\n",
      "(ELE66600,GRO46854)\t112\n",
      "(ELE66600,GRO71621)\t122\n",
      "(ELE66600,GRO73461)\t218\n",
      "(ELE66600,SNA45677)\t154\n",
      "(ELE66600,SNA55762)\t130\n",
      "(ELE66600,SNA80324)\t129\n",
      "(ELE66600,SNA95666)\t103\n",
      "(ELE66600,SNA96271)\t144\n",
      "(ELE66600,SNA99873)\t152\n",
      "(ELE66810,ELE74009)\t110\n",
      "(ELE66810,FRO31317)\t164\n",
      "(ELE66810,FRO32293)\t109\n",
      "(ELE66810,FRO35729)\t131\n",
      "(ELE66810,FRO40251)\t168\n",
      "(ELE66810,FRO41939)\t234\n",
      "(ELE66810,FRO80039)\t150\n",
      "(ELE66810,GRO21487)\t136\n",
      "(ELE66810,GRO59710)\t145\n",
      "(ELE66810,GRO73461)\t228\n",
      "(ELE66810,SNA45677)\t119\n",
      "(ELE66810,SNA66979)\t121\n",
      "(ELE66810,SNA80324)\t117\n",
      "(ELE66810,SNA99873)\t177\n",
      "(ELE68605,FRO40251)\t118\n",
      "(ELE68605,GRO73461)\t113\n",
      "(ELE69552,FRO19221)\t136\n",
      "(ELE74009,ELE74482)\t114\n",
      "(ELE74009,ELE92920)\t130\n",
      "(ELE74009,FRO31317)\t160\n",
      "(ELE74009,FRO35904)\t106\n",
      "(ELE74009,FRO40251)\t272\n",
      "(ELE74009,FRO53271)\t129\n",
      "(ELE74009,FRO78087)\t145\n",
      "(ELE74009,FRO80039)\t120\n",
      "(ELE74009,FRO85978)\t118\n",
      "(ELE74009,GRO15017)\t153\n",
      "(ELE74009,GRO21487)\t159\n",
      "(ELE74009,GRO30386)\t123\n",
      "(ELE74009,GRO38814)\t141\n",
      "(ELE74009,GRO46854)\t138\n",
      "(ELE74009,GRO59710)\t192\n",
      "(ELE74009,GRO61133)\t137\n",
      "(ELE74009,GRO64900)\t142\n",
      "(ELE74009,GRO71621)\t136\n",
      "(ELE74009,GRO73461)\t345\n",
      "(ELE74009,GRO81087)\t113\n",
      "(ELE74009,GRO94758)\t142\n",
      "(ELE74009,SNA45677)\t233\n",
      "(ELE74009,SNA55762)\t128\n",
      "(ELE74009,SNA72163)\t145\n",
      "(ELE74009,SNA80324)\t224\n",
      "(ELE74009,SNA90094)\t122\n",
      "(ELE74009,SNA93860)\t136\n",
      "(ELE74009,SNA95666)\t213\n",
      "(ELE74009,SNA99873)\t174\n",
      "(ELE74482,FRO31317)\t317\n",
      "(ELE74482,FRO40251)\t179\n",
      "(ELE74482,GRO21487)\t101\n",
      "(ELE74482,GRO30386)\t142\n",
      "(ELE74482,GRO59710)\t133\n",
      "(ELE74482,GRO73461)\t168\n",
      "(ELE74482,SNA45677)\t113\n",
      "(ELE74482,SNA80324)\t114\n",
      "(ELE74482,SNA99873)\t357\n",
      "(ELE78169,GRO94758)\t252\n",
      "(ELE78169,SNA45677)\t206\n",
      "(ELE86561,SNA45677)\t100\n",
      "(ELE87456,GRO73461)\t104\n",
      "(ELE88583,GRO30912)\t105\n",
      "(ELE88583,SNA24799)\t190\n",
      "(ELE91337,FRO35904)\t104\n",
      "(ELE91337,FRO80039)\t120\n",
      "(ELE91337,FRO85978)\t214\n",
      "(ELE91337,GRO21487)\t138\n",
      "(ELE91337,GRO46854)\t147\n",
      "(ELE91337,GRO64900)\t141\n",
      "(ELE91337,GRO71621)\t109\n",
      "(ELE91337,GRO73461)\t174\n",
      "(ELE91337,SNA45677)\t334\n",
      "(ELE91337,SNA55762)\t120\n",
      "(ELE91337,SNA99873)\t125\n",
      "(ELE92920,FRO31317)\t101\n",
      "(ELE92920,FRO40251)\t213\n",
      "(ELE92920,GRO15017)\t161\n",
      "(ELE92920,GRO46854)\t101\n",
      "(ELE92920,GRO59710)\t141\n",
      "(ELE92920,GRO71621)\t102\n",
      "(ELE92920,GRO73461)\t108\n",
      "(ELE92920,GRO81087)\t140\n",
      "(ELE92920,SNA18336)\t455\n",
      "(ELE92920,SNA45677)\t104\n",
      "(ELE92920,SNA80324)\t118\n",
      "(ELE99737,FRO19221)\t133\n",
      "(ELE99737,FRO40251)\t141\n",
      "(ELE99737,FRO80039)\t146\n",
      "(ELE99737,FRO85978)\t110\n",
      "(ELE99737,GRO56726)\t119\n",
      "(ELE99737,GRO73461)\t155\n",
      "(ELE99737,GRO94758)\t150\n",
      "(ELE99737,SNA45677)\t178\n",
      "(ELE99737,SNA55762)\t130\n",
      "(ELE99737,SNA80324)\t104\n",
      "(ELE99737,SNA90094)\t102\n",
      "(FRO16142,FRO40251)\t170\n",
      "(FRO16142,GRO38814)\t114\n",
      "(FRO16142,GRO59710)\t122\n",
      "(FRO16142,GRO73461)\t197\n",
      "(FRO16142,SNA80324)\t172\n",
      "(FRO19221,GRO71621)\t109\n",
      "(FRO19221,GRO73461)\t202\n",
      "(FRO19221,SNA53220)\t155\n",
      "(FRO19221,SNA93860)\t151\n",
      "(FRO24098,FRO40251)\t106\n",
      "(FRO24098,GRO73461)\t112\n",
      "(FRO31317,FRO32293)\t147\n",
      "(FRO31317,FRO35904)\t101\n",
      "(FRO31317,FRO40251)\t292\n",
      "(FRO31317,FRO53271)\t257\n",
      "(FRO31317,FRO78087)\t102\n",
      "(FRO31317,FRO80039)\t187\n",
      "(FRO31317,FRO85978)\t154\n",
      "(FRO31317,GRO15017)\t144\n",
      "(FRO31317,GRO21487)\t205\n",
      "(FRO31317,GRO30386)\t203\n",
      "(FRO31317,GRO32086)\t143\n",
      "(FRO31317,GRO38814)\t121\n",
      "(FRO31317,GRO46854)\t114\n",
      "(FRO31317,GRO56726)\t281\n",
      "(FRO31317,GRO59710)\t149\n",
      "(FRO31317,GRO61133)\t114\n",
      "(FRO31317,GRO69543)\t120\n",
      "(FRO31317,GRO71621)\t133\n",
      "(FRO31317,GRO73461)\t395\n",
      "(FRO31317,GRO81087)\t100\n",
      "(FRO31317,SNA45677)\t202\n",
      "(FRO31317,SNA55762)\t117\n",
      "(FRO31317,SNA80324)\t158\n",
      "(FRO31317,SNA90094)\t118\n",
      "(FRO31317,SNA93860)\t139\n",
      "(FRO31317,SNA96271)\t126\n",
      "(FRO31317,SNA99873)\t277\n",
      "(FRO32293,FRO40251)\t226\n",
      "(FRO32293,FRO53271)\t106\n",
      "(FRO32293,FRO80039)\t130\n",
      "(FRO32293,GRO21487)\t109\n",
      "(FRO32293,GRO30386)\t106\n",
      "(FRO32293,GRO46854)\t101\n",
      "(FRO32293,GRO56726)\t102\n",
      "(FRO32293,GRO59710)\t136\n",
      "(FRO32293,GRO73461)\t200\n",
      "(FRO32293,SNA45677)\t117\n",
      "(FRO32293,SNA80324)\t156\n",
      "(FRO32293,SNA99873)\t135\n",
      "(FRO35904,FRO40251)\t153\n",
      "(FRO35904,FRO78087)\t136\n",
      "(FRO35904,FRO85978)\t160\n",
      "(FRO35904,FRO99783)\t122\n",
      "(FRO35904,GRO21487)\t115\n",
      "(FRO35904,GRO46854)\t128\n",
      "(FRO35904,GRO71621)\t104\n",
      "(FRO35904,GRO73461)\t162\n",
      "(FRO35904,SNA45677)\t143\n",
      "(FRO35904,SNA55762)\t102\n",
      "(FRO35904,SNA80324)\t135\n",
      "(FRO35904,SNA96271)\t113\n",
      "(FRO35904,SNA99873)\t108\n",
      "(FRO40251,FRO43226)\t109\n",
      "(FRO40251,FRO53271)\t309\n",
      "(FRO40251,FRO61354)\t126\n",
      "(FRO40251,FRO74605)\t107\n",
      "(FRO40251,FRO75586)\t129\n",
      "(FRO40251,FRO78087)\t215\n",
      "(FRO40251,FRO80039)\t249\n",
      "(FRO40251,FRO85978)\t138\n",
      "(FRO40251,FRO89565)\t110\n",
      "(FRO40251,FRO92261)\t113\n",
      "(FRO40251,FRO92469)\t835\n",
      "(FRO40251,FRO98729)\t107\n",
      "(FRO40251,GRO15017)\t197\n",
      "(FRO40251,GRO21487)\t375\n",
      "(FRO40251,GRO30386)\t224\n",
      "(FRO40251,GRO32086)\t165\n",
      "(FRO40251,GRO35122)\t127\n",
      "(FRO40251,GRO36567)\t113\n",
      "(FRO40251,GRO38636)\t106\n",
      "(FRO40251,GRO38814)\t295\n",
      "(FRO40251,GRO38983)\t153\n",
      "(FRO40251,GRO44993)\t131\n",
      "(FRO40251,GRO46854)\t210\n",
      "(FRO40251,GRO50832)\t100\n",
      "(FRO40251,GRO56726)\t247\n",
      "(FRO40251,GRO56989)\t100\n",
      "(FRO40251,GRO61133)\t211\n",
      "(FRO40251,GRO64900)\t149\n",
      "(FRO40251,GRO69543)\t227\n",
      "(FRO40251,GRO71621)\t288\n",
      "(FRO40251,GRO73461)\t882\n",
      "(FRO40251,GRO81087)\t223\n",
      "(FRO40251,GRO81647)\t113\n",
      "(FRO40251,GRO85051)\t1213\n",
      "(FRO40251,GRO88511)\t113\n",
      "(FRO40251,GRO94758)\t230\n",
      "(FRO40251,GRO99222)\t142\n",
      "(FRO40251,SNA12663)\t105\n",
      "(FRO40251,SNA18336)\t143\n",
      "(FRO40251,SNA38068)\t158\n",
      "(FRO40251,SNA42528)\t110\n",
      "(FRO40251,SNA45677)\t309\n",
      "(FRO40251,SNA47306)\t139\n",
      "(FRO40251,SNA50789)\t113\n",
      "(FRO40251,SNA55762)\t258\n",
      "(FRO40251,SNA55952)\t115\n",
      "(FRO40251,SNA62128)\t125\n",
      "(FRO40251,SNA72163)\t201\n",
      "(FRO40251,SNA80324)\t1412\n",
      "(FRO40251,SNA90094)\t201\n",
      "(FRO40251,SNA93860)\t234\n",
      "(FRO40251,SNA96271)\t227\n",
      "(FRO40251,SNA99873)\t207\n",
      "(FRO43226,FRO48038)\t105\n",
      "(FRO43226,FRO66272)\t114\n",
      "(FRO43226,GRO73461)\t110\n",
      "(FRO47962,GRO73461)\t137\n",
      "(FRO53271,FRO80039)\t112\n",
      "(FRO53271,GRO21487)\t307\n",
      "(FRO53271,GRO30386)\t111\n",
      "(FRO53271,GRO32086)\t181\n",
      "(FRO53271,GRO38814)\t121\n",
      "(FRO53271,GRO59710)\t179\n",
      "(FRO53271,GRO73461)\t192\n",
      "(FRO53271,GRO85051)\t105\n",
      "(FRO53271,SNA45677)\t106\n",
      "(FRO53271,SNA80324)\t196\n",
      "(FRO53271,SNA99873)\t131\n",
      "(FRO61354,GRO73461)\t117\n",
      "(FRO61354,SNA80324)\t176\n",
      "(FRO64631,SNA99873)\t130\n",
      "(FRO66272,GRO73461)\t110\n",
      "(FRO73056,GRO44993)\t438\n",
      "(FRO73056,GRO73461)\t195\n",
      "(FRO74605,GRO73461)\t105\n",
      "(FRO75586,SNA80324)\t133\n",
      "(FRO78087,FRO80039)\t116\n",
      "(FRO78087,FRO85978)\t157\n",
      "(FRO78087,GRO30386)\t100\n",
      "(FRO78087,GRO39357)\t105\n",
      "(FRO78087,GRO59710)\t134\n",
      "(FRO78087,GRO71621)\t135\n",
      "(FRO78087,GRO73461)\t192\n",
      "(FRO78087,GRO94758)\t100\n",
      "(FRO78087,SNA45677)\t152\n",
      "(FRO78087,SNA55762)\t117\n",
      "(FRO78087,SNA80324)\t170\n",
      "(FRO78087,SNA95666)\t110\n",
      "(FRO78087,SNA99873)\t142\n",
      "(FRO79022,FRO99783)\t172\n",
      "(FRO79022,GRO46854)\t222\n",
      "(FRO79022,GRO56726)\t120\n",
      "(FRO80039,FRO85978)\t165\n",
      "(FRO80039,GRO15017)\t104\n",
      "(FRO80039,GRO21487)\t136\n",
      "(FRO80039,GRO30386)\t130\n",
      "(FRO80039,GRO38814)\t115\n",
      "(FRO80039,GRO46854)\t122\n",
      "(FRO80039,GRO50832)\t140\n",
      "(FRO80039,GRO56726)\t117\n",
      "(FRO80039,GRO59710)\t151\n",
      "(FRO80039,GRO64900)\t100\n",
      "(FRO80039,GRO71621)\t140\n",
      "(FRO80039,GRO73461)\t209\n",
      "(FRO80039,GRO94758)\t120\n",
      "(FRO80039,SNA45677)\t227\n",
      "(FRO80039,SNA55762)\t101\n",
      "(FRO80039,SNA80324)\t205\n",
      "(FRO80039,SNA93860)\t106\n",
      "(FRO80039,SNA96271)\t173\n",
      "(FRO80039,SNA96466)\t102\n",
      "(FRO80039,SNA99873)\t148\n",
      "(FRO85978,GRO15017)\t162\n",
      "(FRO85978,GRO21487)\t211\n",
      "(FRO85978,GRO44993)\t103\n",
      "(FRO85978,GRO46854)\t240\n",
      "(FRO85978,GRO56726)\t168\n",
      "(FRO85978,GRO61133)\t112\n",
      "(FRO85978,GRO64900)\t129\n",
      "(FRO85978,GRO71621)\t161\n",
      "(FRO85978,GRO73461)\t344\n",
      "(FRO85978,GRO94758)\t109\n",
      "(FRO85978,SNA45677)\t314\n",
      "(FRO85978,SNA55762)\t143\n",
      "(FRO85978,SNA55952)\t133\n",
      "(FRO85978,SNA90094)\t115\n",
      "(FRO85978,SNA93860)\t263\n",
      "(FRO85978,SNA95666)\t463\n",
      "(FRO85978,SNA99873)\t258\n",
      "(FRO92261,GRO71621)\t104\n",
      "(FRO92261,SNA80324)\t103\n",
      "(FRO92261,SNA99873)\t144\n",
      "(FRO92469,GRO73461)\t213\n",
      "(FRO92469,SNA80324)\t352\n",
      "(FRO98184,GRO30386)\t145\n",
      "(FRO98729,SNA55762)\t104\n",
      "(FRO99783,GRO46854)\t128\n",
      "(GRO15017,GRO21487)\t120\n",
      "(GRO15017,GRO46854)\t106\n",
      "(GRO15017,GRO59710)\t112\n",
      "(GRO15017,GRO61133)\t114\n",
      "(GRO15017,GRO73461)\t288\n",
      "(GRO15017,GRO81087)\t103\n",
      "(GRO15017,SNA18336)\t113\n",
      "(GRO15017,SNA45677)\t150\n",
      "(GRO15017,SNA55762)\t104\n",
      "(GRO15017,SNA80324)\t198\n",
      "(GRO15017,SNA90094)\t104\n",
      "(GRO15017,SNA99873)\t154\n",
      "(GRO21487,GRO30386)\t141\n",
      "(GRO21487,GRO38814)\t157\n",
      "(GRO21487,GRO44993)\t111\n",
      "(GRO21487,GRO46854)\t209\n",
      "(GRO21487,GRO56726)\t160\n",
      "(GRO21487,GRO59710)\t126\n",
      "(GRO21487,GRO69543)\t128\n",
      "(GRO21487,GRO71621)\t171\n",
      "(GRO21487,GRO73461)\t631\n",
      "(GRO21487,GRO81647)\t114\n",
      "(GRO21487,GRO85051)\t120\n",
      "(GRO21487,GRO94758)\t116\n",
      "(GRO21487,SNA45677)\t194\n",
      "(GRO21487,SNA55762)\t120\n",
      "(GRO21487,SNA72163)\t104\n",
      "(GRO21487,SNA80324)\t225\n",
      "(GRO21487,SNA90094)\t112\n",
      "(GRO21487,SNA93860)\t165\n",
      "(GRO21487,SNA99873)\t160\n",
      "(GRO24246,GRO56726)\t111\n",
      "(GRO24246,SNA30859)\t119\n",
      "(GRO30386,GRO44993)\t116\n",
      "(GRO30386,GRO46854)\t114\n",
      "(GRO30386,GRO56726)\t106\n",
      "(GRO30386,GRO59710)\t190\n",
      "(GRO30386,GRO61133)\t102\n",
      "(GRO30386,GRO69543)\t104\n",
      "(GRO30386,GRO71621)\t124\n",
      "(GRO30386,GRO73461)\t380\n",
      "(GRO30386,GRO81087)\t105\n",
      "(GRO30386,SNA12663)\t118\n",
      "(GRO30386,SNA45677)\t134\n",
      "(GRO30386,SNA72163)\t138\n",
      "(GRO30386,SNA80324)\t238\n",
      "(GRO30386,SNA88283)\t120\n",
      "(GRO30386,SNA90094)\t113\n",
      "(GRO30386,SNA96271)\t121\n",
      "(GRO30386,SNA99873)\t112\n",
      "(GRO32086,GRO73461)\t102\n",
      "(GRO32086,SNA99873)\t102\n",
      "(GRO35122,GRO56726)\t127\n",
      "(GRO35122,GRO73461)\t187\n",
      "(GRO35122,SNA80324)\t155\n",
      "(GRO36567,GRO38814)\t135\n",
      "(GRO36567,GRO73461)\t117\n",
      "(GRO36567,SNA45677)\t189\n",
      "(GRO36567,SNA47306)\t102\n",
      "(GRO38814,GRO46854)\t103\n",
      "(GRO38814,GRO59710)\t150\n",
      "(GRO38814,GRO61133)\t102\n",
      "(GRO38814,GRO64900)\t119\n",
      "(GRO38814,GRO71621)\t139\n",
      "(GRO38814,GRO73461)\t427\n",
      "(GRO38814,GRO85051)\t115\n",
      "(GRO38814,GRO94758)\t124\n",
      "(GRO38814,SNA45677)\t203\n",
      "(GRO38814,SNA55762)\t113\n",
      "(GRO38814,SNA72163)\t117\n",
      "(GRO38814,SNA80324)\t214\n",
      "(GRO38814,SNA93860)\t100\n",
      "(GRO38814,SNA99873)\t124\n",
      "(GRO38983,SNA45677)\t101\n",
      "(GRO38983,SNA96271)\t102\n",
      "(GRO44993,GRO73461)\t227\n",
      "(GRO44993,SNA45677)\t128\n",
      "(GRO44993,SNA80324)\t111\n",
      "(GRO44993,SNA85241)\t121\n",
      "(GRO46854,GRO56726)\t253\n",
      "(GRO46854,GRO61133)\t141\n",
      "(GRO46854,GRO71621)\t160\n",
      "(GRO46854,GRO73461)\t389\n",
      "(GRO46854,GRO81045)\t143\n",
      "(GRO46854,GRO94758)\t103\n",
      "(GRO46854,SNA45677)\t171\n",
      "(GRO46854,SNA55762)\t139\n",
      "(GRO46854,SNA66583)\t100\n",
      "(GRO46854,SNA80324)\t178\n",
      "(GRO46854,SNA90094)\t166\n",
      "(GRO46854,SNA93860)\t163\n",
      "(GRO46854,SNA96271)\t144\n",
      "(GRO46854,SNA99873)\t138\n",
      "(GRO50832,SNA80324)\t108\n",
      "(GRO56726,GRO61133)\t116\n",
      "(GRO56726,GRO73461)\t324\n",
      "(GRO56726,SNA45677)\t101\n",
      "(GRO56726,SNA80324)\t169\n",
      "(GRO56726,SNA90094)\t102\n",
      "(GRO56726,SNA93860)\t131\n",
      "(GRO59710,GRO61133)\t140\n",
      "(GRO59710,GRO73461)\t305\n",
      "(GRO59710,GRO81087)\t146\n",
      "(GRO59710,GRO94758)\t159\n",
      "(GRO59710,SNA38068)\t118\n",
      "(GRO59710,SNA40784)\t180\n",
      "(GRO59710,SNA45677)\t189\n",
      "(GRO59710,SNA55762)\t153\n",
      "(GRO59710,SNA59903)\t101\n",
      "(GRO59710,SNA62128)\t113\n",
      "(GRO59710,SNA90094)\t103\n",
      "(GRO59710,SNA93860)\t100\n",
      "(GRO59710,SNA99873)\t124\n",
      "(GRO61133,GRO73461)\t250\n",
      "(GRO61133,GRO81087)\t200\n",
      "(GRO61133,SNA80324)\t171\n",
      "(GRO61133,SNA99873)\t134\n",
      "(GRO64900,GRO73461)\t141\n",
      "(GRO64900,SNA45677)\t100\n",
      "(GRO64900,SNA80324)\t119\n",
      "(GRO68067,GRO73461)\t145\n",
      "(GRO69543,GRO73461)\t320\n",
      "(GRO69543,SNA80324)\t102\n",
      "(GRO71621,GRO73461)\t305\n",
      "(GRO71621,GRO81087)\t102\n",
      "(GRO71621,GRO94758)\t188\n",
      "(GRO71621,SNA40784)\t146\n",
      "(GRO71621,SNA45677)\t227\n",
      "(GRO71621,SNA55762)\t143\n",
      "(GRO71621,SNA80324)\t254\n",
      "(GRO71621,SNA90094)\t157\n",
      "(GRO71621,SNA93860)\t144\n",
      "(GRO71621,SNA96271)\t105\n",
      "(GRO71621,SNA99873)\t172\n",
      "(GRO73461,GRO81087)\t172\n",
      "(GRO73461,GRO81647)\t332\n",
      "(GRO73461,GRO85051)\t147\n",
      "(GRO73461,GRO88511)\t100\n",
      "(GRO73461,GRO94758)\t168\n",
      "(GRO73461,SNA12663)\t109\n",
      "(GRO73461,SNA18336)\t121\n",
      "(GRO73461,SNA38068)\t212\n",
      "(GRO73461,SNA42528)\t116\n",
      "(GRO73461,SNA44190)\t117\n",
      "(GRO73461,SNA45677)\t318\n",
      "(GRO73461,SNA47306)\t103\n",
      "(GRO73461,SNA55762)\t222\n",
      "(GRO73461,SNA55952)\t117\n",
      "(GRO73461,SNA59061)\t109\n",
      "(GRO73461,SNA59903)\t123\n",
      "(GRO73461,SNA62128)\t111\n",
      "(GRO73461,SNA69641)\t150\n",
      "(GRO73461,SNA72163)\t285\n",
      "(GRO73461,SNA80324)\t562\n",
      "(GRO73461,SNA88283)\t118\n",
      "(GRO73461,SNA90094)\t160\n",
      "(GRO73461,SNA93860)\t111\n",
      "(GRO73461,SNA95666)\t116\n",
      "(GRO73461,SNA96271)\t225\n",
      "(GRO73461,SNA99873)\t296\n",
      "(GRO81087,SNA18336)\t102\n",
      "(GRO81087,SNA45677)\t126\n",
      "(GRO81087,SNA80324)\t185\n",
      "(GRO85051,SNA45677)\t107\n",
      "(GRO85051,SNA80324)\t471\n",
      "(GRO88324,SNA80324)\t106\n",
      "(GRO94173,SNA45677)\t150\n",
      "(GRO94758,SNA45677)\t286\n",
      "(GRO94758,SNA55762)\t109\n",
      "(GRO94758,SNA80324)\t218\n",
      "(GRO94758,SNA99873)\t139\n",
      "(GRO99222,SNA80324)\t101\n",
      "(GRO99222,SNA90258)\t156\n",
      "(SNA20554,SNA45677)\t146\n",
      "(SNA24799,SNA55617)\t119\n",
      "(SNA30533,SNA96271)\t169\n",
      "(SNA31619,SNA96271)\t109\n",
      "(SNA38068,SNA80324)\t123\n",
      "(SNA40784,SNA99873)\t129\n",
      "(SNA44190,SNA80324)\t112\n",
      "(SNA45677,SNA47306)\t104\n",
      "(SNA45677,SNA55762)\t184\n",
      "(SNA45677,SNA80324)\t256\n",
      "(SNA45677,SNA90094)\t156\n",
      "(SNA45677,SNA93860)\t112\n",
      "(SNA45677,SNA96271)\t269\n",
      "(SNA45677,SNA99873)\t215\n",
      "(SNA53220,SNA93860)\t254\n",
      "(SNA55762,SNA80324)\t201\n",
      "(SNA55762,SNA99873)\t143\n",
      "(SNA59903,SNA72163)\t310\n",
      "(SNA72163,SNA80324)\t116\n",
      "(SNA72163,SNA93860)\t121\n",
      "(SNA74022,SNA96271)\t107\n",
      "(SNA80324,SNA90094)\t154\n",
      "(SNA80324,SNA93860)\t150\n",
      "(SNA80324,SNA96271)\t219\n",
      "(SNA80324,SNA99873)\t163\n",
      "(SNA90094,SNA96271)\t104\n",
      "(SNA93860,SNA99873)\t105\n",
      "16/05/28 20:49:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 20:49:30 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Print the output from the file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-4a/part-00000\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-4a/part-00000 /user/hadoop/product_pairs_output_full\n",
    "# Save the output of the counts for Step 2\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-4a/part-00000 /user/hadoop/product_pairs_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing product_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile product_reducer.py\n",
    "#!/usr/bin/env python\n",
    "## product_reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.4\n",
    "## This function expects keys to come in sorted.  The special value from the mapper: (*basketcount,*basketcount)\n",
    "## is used here to mark total basket count.\n",
    "## Because this value is so high, it will come in first from the mappers and used in the rest of \n",
    "## the function.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize the counters\n",
    "count_basket = 0\n",
    "i = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd and items as key and value\n",
    "    key = items[0]\n",
    "    value = int(items[1])\n",
    "    # If the value equals the special number (*basketcount,*basketcount), this indicates this is a total \n",
    "    # basket count value and it should be added to the count_basket total\n",
    "    if key == '(*basketcount,*basketcount)':\n",
    "        count_basket += float(value)\n",
    "    else:\n",
    "        # Only print the top 50 most frequent product pairs\n",
    "        if i < 50:\n",
    "            key1, key2 = key.strip('()').split(',')\n",
    "            print('%s, %s, %s, %f' % (key1, key2, value, value/count_basket))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 21:16:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 21:16:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-4b/_SUCCESS\n",
      "16/05/28 21:16:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-4b/part-00000\n",
      "16/05/28 21:16:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/28 21:17:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar275035286794707736/] [] /tmp/streamjob1499111225941256235.jar tmpDir=null\n",
      "16/05/28 21:17:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 21:17:01 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/28 21:17:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/28 21:17:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/28 21:17:02 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/28 21:17:02 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/28 21:17:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0239\n",
      "16/05/28 21:17:02 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0239\n",
      "16/05/28 21:17:02 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0239/\n",
      "16/05/28 21:17:02 INFO mapreduce.Job: Running job: job_1463787494457_0239\n",
      "16/05/28 21:17:06 INFO mapreduce.Job: Job job_1463787494457_0239 running in uber mode : false\n",
      "16/05/28 21:17:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/28 21:17:12 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/05/28 21:17:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/28 21:17:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/28 21:17:17 INFO mapreduce.Job: Job job_1463787494457_0239 completed successfully\n",
      "16/05/28 21:17:17 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=36068\n",
      "\t\tFILE: Number of bytes written=429830\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=32609\n",
      "\t\tHDFS: Number of bytes written=1757\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6594\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1908\n",
      "\t\tTotal time spent by all map tasks (ms)=6594\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1908\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6594\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1908\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6752256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1953792\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1335\n",
      "\t\tMap output records=1335\n",
      "\t\tMap output bytes=33392\n",
      "\t\tMap output materialized bytes=36074\n",
      "\t\tInput split bytes=196\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=36074\n",
      "\t\tReduce input records=1335\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=2670\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=248\n",
      "\t\tCPU time spent (ms)=1470\n",
      "\t\tPhysical memory (bytes) snapshot=657760256\n",
      "\t\tVirtual memory (bytes) snapshot=6298316800\n",
      "\t\tTotal committed heap usage (bytes)=505937920\n",
      "\tMapper Counters\n",
      "\t\tIdentity Calls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=32413\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1757\n",
      "16/05/28 21:17:17 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-4b\n"
     ]
    }
   ],
   "source": [
    "####################### HW3.4 ######################\n",
    "# Step 2: Run the sorting job\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-4b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-4b\n",
    "# Run a Hadoop streaming job.  The input file 'Consumer_Complaints.csv' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files Imapper.py,product_reducer.py -mapper Imapper.py -reducer product_reducer.py \\\n",
    "-input /user/hadoop/product_pairs_output -output /user/hadoop/outputHW3-4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/28 21:17:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779, ELE17451, 1592, 0.051188\t\n",
      "FRO40251, SNA80324, 1412, 0.045400\t\n",
      "DAI75645, FRO40251, 1254, 0.040320\t\n",
      "FRO40251, GRO85051, 1213, 0.039002\t\n",
      "DAI62779, GRO73461, 1139, 0.036623\t\n",
      "DAI75645, SNA80324, 1130, 0.036333\t\n",
      "DAI62779, FRO40251, 1070, 0.034404\t\n",
      "DAI62779, SNA80324, 923, 0.029678\t\n",
      "DAI62779, DAI85309, 918, 0.029517\t\n",
      "ELE32164, GRO59710, 911, 0.029292\t\n",
      "DAI62779, DAI75645, 882, 0.028359\t\n",
      "FRO40251, GRO73461, 882, 0.028359\t\n",
      "DAI62779, ELE92920, 877, 0.028198\t\n",
      "FRO40251, FRO92469, 835, 0.026848\t\n",
      "DAI62779, ELE32164, 832, 0.026752\t\n",
      "DAI75645, GRO73461, 712, 0.022893\t\n",
      "DAI43223, ELE32164, 711, 0.022861\t\n",
      "DAI62779, GRO30386, 709, 0.022797\t\n",
      "ELE17451, FRO40251, 697, 0.022411\t\n",
      "DAI85309, ELE99737, 659, 0.021189\t\n",
      "DAI62779, ELE26917, 650, 0.020900\t\n",
      "GRO21487, GRO73461, 631, 0.020289\t\n",
      "DAI62779, SNA45677, 604, 0.019421\t\n",
      "ELE17451, SNA80324, 597, 0.019196\t\n",
      "DAI62779, GRO71621, 595, 0.019131\t\n",
      "DAI62779, SNA55762, 593, 0.019067\t\n",
      "DAI62779, DAI83733, 586, 0.018842\t\n",
      "ELE17451, GRO73461, 580, 0.018649\t\n",
      "GRO73461, SNA80324, 562, 0.018070\t\n",
      "DAI62779, GRO59710, 561, 0.018038\t\n",
      "DAI62779, FRO80039, 550, 0.017684\t\n",
      "DAI75645, ELE17451, 547, 0.017588\t\n",
      "DAI62779, SNA93860, 537, 0.017266\t\n",
      "DAI55148, DAI62779, 526, 0.016913\t\n",
      "DAI43223, GRO59710, 512, 0.016462\t\n",
      "ELE17451, ELE32164, 511, 0.016430\t\n",
      "DAI62779, SNA18336, 506, 0.016270\t\n",
      "ELE32164, GRO73461, 486, 0.015627\t\n",
      "DAI62779, FRO78087, 482, 0.015498\t\n",
      "DAI85309, ELE17451, 482, 0.015498\t\n",
      "DAI62779, GRO94758, 479, 0.015401\t\n",
      "DAI62779, GRO21487, 471, 0.015144\t\n",
      "GRO85051, SNA80324, 471, 0.015144\t\n",
      "ELE17451, GRO30386, 468, 0.015048\t\n",
      "FRO85978, SNA95666, 463, 0.014887\t\n",
      "DAI62779, FRO19221, 462, 0.014855\t\n",
      "DAI62779, GRO46854, 461, 0.014823\t\n",
      "DAI43223, DAI62779, 459, 0.014758\t\n",
      "ELE92920, SNA18336, 455, 0.014630\t\n",
      "DAI88079, FRO40251, 446, 0.014340\t\n"
     ]
    }
   ],
   "source": [
    "################## OUTPUT FOR HW3.4 ######################\n",
    "# Print the output from the file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-4b/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.5.** Stripes.  Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Output is the same as with Pairs:  see section below marked: 'OUTPUT FOR HW3.5' for full output   \n",
    "\n",
    "| Product 1 | Product 2 | Pair Frequency | Pair Relative frequency |  \n",
    "| - | - | - | - |  \n",
    "| DAI62779 | ELE17451 | 1592 | 0.051188 |  \t\n",
    "| FRO40251 | SNA80324 | 1412 | 0.045400 |  \n",
    "| DAI75645 | FRO40251 | 1254 | 0.040320 |  \n",
    "| FRO40251 | GRO85051 | 1213 | 0.039002 |  \n",
    "| DAI62779 | GRO73461 | 1139 | 0.036623 |  \n",
    "| DAI75645 | SNA80324 | 1130 | 0.036333 |  \n",
    "| DAI62779 | FRO40251 | 1070 | 0.034404 |  \n",
    "| DAI62779 | SNA80324 | 923 | 0.029678 |  \n",
    "| DAI62779 | DAI85309 | 918 | 0.029517 |  \n",
    "| ELE32164 | GRO59710 | 911 | 0.029292 |  \n",
    "\n",
    "**HW3.5A**\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Computational setup: singler server, dual core, 4G RAM, Centos 7.0  \n",
    "\n",
    "Times for Step 1 - Wordcount MR job:\n",
    "\n",
    "|  | Pairs | Stripes |\n",
    "| ------ | ----------- | ------------ |\n",
    "| Total time spent by all map tasks (ms)   | 21,150 | 10,114 |\n",
    "| Total time spent by all reduce tasks (ms) | 4,049 | 177,562 |\n",
    "| Total time spent by all all tasks (ms) | 25,199 | 187,676 |\n",
    "| Number of Mappers | 2 | 2 |\n",
    "| Number of Reducers | 1 | 1 |\n",
    "\n",
    "Times for Step 2 - Sorting MR job:\n",
    "\n",
    "|  | Pairs | Stripes |\n",
    "| ------ | ----------- | ------------ |\n",
    "| Total time spent by all map tasks (ms)   | 6,594 | 6,498 |\n",
    "| Total time spent by all reduce tasks (ms) | 1,908 | 1,918 |\n",
    "| Total time spent by all all tasks (ms) | 8,502 | 8,416 |\n",
    "| Number of Mappers | 2 | 2 |\n",
    "| Number of Reducers | 1 | 1 |\n",
    "\n",
    "For the Step 1 - Wordcount MR job.  The mapper takes half the time for the stripes algorithm as for the pairs algorithm.  The theory for this is that the stripes approach is outputing less text than the pairs approach, so it will take less time to write.  For the reducer the pairs reducer takes about 1/40th of the time of the stripes reducer.  There could be many reasons for this.  The pairs algorithm is simply receiving pairs and counts from the mapper and summing the counts and outputting the sums.  The stripe algorithm is outputting the exact same information, but it has to do many more tasks than the pairs algorithm.  Those tasks are as follows:\n",
    "* run this command in Python: 'value = Counter(eval(items[1]))' to convert the array of values for the key back in to a dictionary-type data type called a Counter.\n",
    "* run code to sum 2 Counter data types together\n",
    "* for each key, iterate through a Counter data type to print out each (product,product) pair.  \n",
    "Because it has to run these more time intensive tasks in Python, the stripes algorithm is taking much more time.  \n",
    "\n",
    "For the Step 2 - Sorting MR job, the times between the pairs implementation and the stripes implementation are basically the same as the sorting job is doing almost the exact same thing in each case.  Each case starts with a file with the counts of the word pairs and then sorts them and creates output.\n",
    "\n",
    "**HW3.5B**  Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs.  \n",
    "\n",
    "Step 1 - Wordcount MR job:  \n",
    "\n",
    "|  | Pairs | Stripes |\n",
    "| ------ | ----------- | ------------ |\n",
    "| Mapper Calls | 2 | 2 |\n",
    "| Combiner Calls | 2 | 0 |\n",
    "| Reducer Calls | 1 | 1 |\n",
    "\n",
    "For the Step 1 - Wordcount MR job, combiners were used for the pairs algorithm, but they were not used for the stripes algorithm because the reducer in the stripes algorithm had a different output signature than the output signature of the mapper.  In order to use a combiner, those signatures must match.  \n",
    "\n",
    "In general, the stripes algorithm has more opportunities to use a combiner because it has fewer unique keys.  The number of unique keys in the stripes algorithm is the number of unique items in the dataset whereas the potential number of unique keys in the pairs algorithm is (the number of unique items x the number of unique items).  This means that there will be more opportunities for a combiner to combine common keys in the stripes algorithm than in the pairs algorithm.\n",
    "\n",
    "For the Step 2 - Sorting MR job, the algorithm and data for this job for both the pairs and stripes cases are almost exactly the same, so there will not be any notable differences with the use of a combiner for this step. Each case starts with a file with the counts of the word pairs and then sorts them and creates output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_mapper.py\n",
    "#!/usr/bin/env python\n",
    "## stripes_mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW3.5A\n",
    "## Read in each record (basket) from the product file.  Sort each basket alphabetically\n",
    "## by product ID.  For each product in the basket make an array of all of the products that follow it in the\n",
    "## sorted list and the count of 1.  Then print the product and its associated array.\n",
    "## Print the basket count with a special marker: *basketcount\n",
    "\n",
    "import sys\n",
    "\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize a basket count\n",
    "count_basket = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Break up each line by space delimiter and sort the products alphabetically by product ID\n",
    "    products = sorted(line.split())\n",
    "    # For each product in the record except for the last one\n",
    "    for i in range(len(products)-1):\n",
    "        # Store the product which will be the key for the reducer output\n",
    "        pi = products[i]\n",
    "        # Initialize a dictionary for storing the pairs\n",
    "        H = {}\n",
    "        # For each product, pj, that follows product, pi, store pj and a count of 1\n",
    "        # in the dictionary H\n",
    "        for pj in products[i+1:]:\n",
    "            H[pj] = 1\n",
    "        # For each product in the basket except for the last one, print the product and\n",
    "        # its dictionary H to send to the reducer.\n",
    "        print('%s\\t%s' % (pi, H))\n",
    "    # Keep track of the basket count and print it with a special marker: *basketcount\n",
    "    # so that it is passed to the reducer\n",
    "    count_basket += 1    \n",
    "    \n",
    "# Pass the basket count to the reducer\n",
    "print('*basketcount\\t%s' % ({'*basketcount':count_basket}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_reducer.py\n",
    "#!/usr/bin/env python\n",
    "## stripes_reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW3.5A\n",
    "## This function expects keys to come in sorted.  It sums the values of all keys that are the same,\n",
    "## then it outputs each product pair combination with its total sum of all of its values.\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "cur_key = None\n",
    "cur_count = Counter()\n",
    "for line in sys.stdin:\n",
    "    # Split the line on tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the 1st and 2nd items as key and value\n",
    "    key = items[0]\n",
    "    # Convert the string in to a Python Counter object\n",
    "    value = Counter(eval(items[1]))\n",
    "    # If this key is the same as the previous key add the values for common\n",
    "    # keys in the Counter objects together\n",
    "    if key == cur_key:\n",
    "        cur_count += value\n",
    "    # Otherwise, reset the current key and start a new count total\n",
    "    else:\n",
    "        if cur_key:\n",
    "            # if the count is greater than 100, output all product pairs from the\n",
    "            # key and all of the elements in its associated Counter object\n",
    "            for k in cur_count:\n",
    "                if cur_count[k] >= 100:\n",
    "                    print('(%s,%s)\\t%d' % (cur_key, k, cur_count[k]))\n",
    "        cur_key = key\n",
    "        cur_count = value\n",
    "\n",
    "# Print the final element\n",
    "for k in cur_count:\n",
    "    if cur_count[k] >= 100:\n",
    "        print('(%s,%s)\\t%d' % (cur_key, k, cur_count[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 16:30:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 16:30:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-5a/_SUCCESS\n",
      "16/05/29 16:30:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW3-5a/part-00000\n",
      "16/05/29 16:30:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/29 16:30:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5438104211775394502/] [] /tmp/streamjob1612339977836404862.jar tmpDir=null\n",
      "16/05/29 16:30:58 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 16:30:58 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 16:30:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 16:30:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 16:30:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0247\n",
      "16/05/29 16:30:59 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0247\n",
      "16/05/29 16:30:59 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0247/\n",
      "16/05/29 16:30:59 INFO mapreduce.Job: Running job: job_1463787494457_0247\n",
      "16/05/29 16:31:04 INFO mapreduce.Job: Job job_1463787494457_0247 running in uber mode : false\n",
      "16/05/29 16:31:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 16:31:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 16:31:21 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "16/05/29 16:31:24 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/05/29 16:31:27 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/05/29 16:31:33 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/05/29 16:31:36 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/05/29 16:31:39 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/05/29 16:31:57 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/05/29 16:32:00 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/05/29 16:32:09 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/05/29 16:32:12 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/05/29 16:32:18 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/05/29 16:32:24 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/05/29 16:32:27 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/05/29 16:32:36 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/05/29 16:32:43 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/05/29 16:32:49 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/05/29 16:32:55 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/05/29 16:32:58 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/05/29 16:33:04 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/05/29 16:33:10 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/05/29 16:33:13 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/05/29 16:33:19 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/05/29 16:33:22 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/05/29 16:33:28 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/05/29 16:33:35 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/05/29 16:33:38 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/05/29 16:33:44 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/05/29 16:33:50 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/05/29 16:33:53 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/05/29 16:33:59 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/05/29 16:34:05 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/05/29 16:34:08 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/05/29 16:34:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 16:34:12 INFO mapreduce.Job: Job job_1463787494457_0247 completed successfully\n",
      "16/05/29 16:34:12 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42480542\n",
      "\t\tFILE: Number of bytes written=85317156\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462069\n",
      "\t\tHDFS: Number of bytes written=32057\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10114\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=177562\n",
      "\t\tTotal time spent by all map tasks (ms)=10114\n",
      "\t\tTotal time spent by all reduce tasks (ms)=177562\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10114\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=177562\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10356736\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=181823488\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=349725\n",
      "\t\tMap output bytes=41642115\n",
      "\t\tMap output materialized bytes=42480548\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12012\n",
      "\t\tReduce shuffle bytes=42480548\n",
      "\t\tReduce input records=349725\n",
      "\t\tReduce output records=1335\n",
      "\t\tSpilled Records=699450\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=243\n",
      "\t\tCPU time spent (ms)=179400\n",
      "\t\tPhysical memory (bytes) snapshot=693485568\n",
      "\t\tVirtual memory (bytes) snapshot=6296387584\n",
      "\t\tTotal committed heap usage (bytes)=560988160\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32057\n",
      "16/05/29 16:34:12 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-5a\n"
     ]
    }
   ],
   "source": [
    "####### HW3.5\n",
    "# Step 1: Run a product count of the products in the ProductPurchaseData.txt file\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-5a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-5a\n",
    "# Run a Hadoop streaming job.  The input file 'ProductPurchaseData.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-files stripes_mapper.py,stripes_reducer.py -mapper stripes_mapper.py -reducer stripes_reducer.py \\\n",
    "-input /user/hadoop/ProductPurchaseData.txt -output /user/hadoop/outputHW3-5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 19:19:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cp: `/user/hadoop/product_stripes_output_full': File exists\n"
     ]
    }
   ],
   "source": [
    "# Print the output from the file\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-5a/part-00000\n",
    "# Save the output of the counts for Step 2\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/product_stripes_output_full\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/product_stripes_output\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-5a/part-00000 /user/hadoop/product_stripes_output_full\n",
    "/usr/local/hadoop/bin/hdfs dfs -cp /user/hadoop/outputHW3-5a/part-00000 /user/hadoop/product_stripes_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 16:35:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/outputHW3-5b/*': No such file or directory\n",
      "16/05/29 16:35:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rmdir: `/user/hadoop/outputHW3-5b': No such file or directory\n",
      "16/05/29 16:35:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3270057256142477719/] [] /tmp/streamjob1409962884169871950.jar tmpDir=null\n",
      "16/05/29 16:35:06 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 16:35:06 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/05/29 16:35:07 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/29 16:35:07 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/29 16:35:07 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/05/29 16:35:07 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/05/29 16:35:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0248\n",
      "16/05/29 16:35:07 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0248\n",
      "16/05/29 16:35:07 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0248/\n",
      "16/05/29 16:35:07 INFO mapreduce.Job: Running job: job_1463787494457_0248\n",
      "16/05/29 16:35:13 INFO mapreduce.Job: Job job_1463787494457_0248 running in uber mode : false\n",
      "16/05/29 16:35:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/29 16:35:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/29 16:35:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/29 16:35:23 INFO mapreduce.Job: Job job_1463787494457_0248 completed successfully\n",
      "16/05/29 16:35:23 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=36068\n",
      "\t\tFILE: Number of bytes written=429960\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=32613\n",
      "\t\tHDFS: Number of bytes written=1757\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6498\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1918\n",
      "\t\tTotal time spent by all map tasks (ms)=6498\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1918\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6498\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1918\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6653952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1964032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1335\n",
      "\t\tMap output records=1335\n",
      "\t\tMap output bytes=33392\n",
      "\t\tMap output materialized bytes=36074\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=36074\n",
      "\t\tReduce input records=1335\n",
      "\t\tReduce output records=50\n",
      "\t\tSpilled Records=2670\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=199\n",
      "\t\tCPU time spent (ms)=1490\n",
      "\t\tPhysical memory (bytes) snapshot=654450688\n",
      "\t\tVirtual memory (bytes) snapshot=6294089728\n",
      "\t\tTotal committed heap usage (bytes)=503316480\n",
      "\tMapper Counters\n",
      "\t\tIdentity Calls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=32413\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1757\n",
      "16/05/29 16:35:23 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW3-5b\n"
     ]
    }
   ],
   "source": [
    "####################### HW3.5 ######################\n",
    "# Step 2:  Sort the counts from Step 1\n",
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW3-5b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW3-5b\n",
    "# Run a Hadoop streaming job.  The output from the previous MR job is sent as input to this MR job\n",
    "!/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-files Imapper.py,product_reducer.py -mapper Imapper.py -reducer product_reducer.py \\\n",
    "-input /user/hadoop/product_stripes_output -output /user/hadoop/outputHW3-5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/29 16:35:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779, ELE17451, 1592, 0.051188\t\n",
      "FRO40251, SNA80324, 1412, 0.045400\t\n",
      "DAI75645, FRO40251, 1254, 0.040320\t\n",
      "FRO40251, GRO85051, 1213, 0.039002\t\n",
      "DAI62779, GRO73461, 1139, 0.036623\t\n",
      "DAI75645, SNA80324, 1130, 0.036333\t\n",
      "DAI62779, FRO40251, 1070, 0.034404\t\n",
      "DAI62779, SNA80324, 923, 0.029678\t\n",
      "DAI62779, DAI85309, 918, 0.029517\t\n",
      "ELE32164, GRO59710, 911, 0.029292\t\n",
      "DAI62779, DAI75645, 882, 0.028359\t\n",
      "FRO40251, GRO73461, 882, 0.028359\t\n",
      "DAI62779, ELE92920, 877, 0.028198\t\n",
      "FRO40251, FRO92469, 835, 0.026848\t\n",
      "DAI62779, ELE32164, 832, 0.026752\t\n",
      "DAI75645, GRO73461, 712, 0.022893\t\n",
      "DAI43223, ELE32164, 711, 0.022861\t\n",
      "DAI62779, GRO30386, 709, 0.022797\t\n",
      "ELE17451, FRO40251, 697, 0.022411\t\n",
      "DAI85309, ELE99737, 659, 0.021189\t\n",
      "DAI62779, ELE26917, 650, 0.020900\t\n",
      "GRO21487, GRO73461, 631, 0.020289\t\n",
      "DAI62779, SNA45677, 604, 0.019421\t\n",
      "ELE17451, SNA80324, 597, 0.019196\t\n",
      "DAI62779, GRO71621, 595, 0.019131\t\n",
      "DAI62779, SNA55762, 593, 0.019067\t\n",
      "DAI62779, DAI83733, 586, 0.018842\t\n",
      "ELE17451, GRO73461, 580, 0.018649\t\n",
      "GRO73461, SNA80324, 562, 0.018070\t\n",
      "DAI62779, GRO59710, 561, 0.018038\t\n",
      "DAI62779, FRO80039, 550, 0.017684\t\n",
      "DAI75645, ELE17451, 547, 0.017588\t\n",
      "DAI62779, SNA93860, 537, 0.017266\t\n",
      "DAI55148, DAI62779, 526, 0.016913\t\n",
      "DAI43223, GRO59710, 512, 0.016462\t\n",
      "ELE17451, ELE32164, 511, 0.016430\t\n",
      "DAI62779, SNA18336, 506, 0.016270\t\n",
      "ELE32164, GRO73461, 486, 0.015627\t\n",
      "DAI62779, FRO78087, 482, 0.015498\t\n",
      "DAI85309, ELE17451, 482, 0.015498\t\n",
      "DAI62779, GRO94758, 479, 0.015401\t\n",
      "DAI62779, GRO21487, 471, 0.015144\t\n",
      "GRO85051, SNA80324, 471, 0.015144\t\n",
      "ELE17451, GRO30386, 468, 0.015048\t\n",
      "FRO85978, SNA95666, 463, 0.014887\t\n",
      "DAI62779, FRO19221, 462, 0.014855\t\n",
      "DAI62779, GRO46854, 461, 0.014823\t\n",
      "DAI43223, DAI62779, 459, 0.014758\t\n",
      "ELE92920, SNA18336, 455, 0.014630\t\n",
      "DAI88079, FRO40251, 446, 0.014340\t\n"
     ]
    }
   ],
   "source": [
    "################## OUTPUT FOR HW3.5 ######################\n",
    "# Print the output from the file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW3-5b/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Imapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Imapper.py\n",
    "#!/usr/bin/env python\n",
    "## Identity mapper.  Takes in input and outputs it directly.\n",
    "import sys\n",
    "# Create a counter called Mapper Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Identity Calls,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    print(\"%s\" % (line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Ireducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Ireducer.py\n",
    "#!/usr/bin/env python\n",
    "## Identity reducer.  Takes in input and outputs it directly.\n",
    "import sys\n",
    "# Create a counter called Reducer Counters that will count the calls to function\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Identity Calls,1\\n\")\n",
    "for line in sys.stdin:    \n",
    "    print(\"%s\" % (line.strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
