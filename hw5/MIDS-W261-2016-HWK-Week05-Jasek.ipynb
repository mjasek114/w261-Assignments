{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  5\n",
    "* **Date:**  6/17/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "\n",
    "## HW 5.1\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "**ANSWER:** There are 2 tables in this problem.  The first one is labeled 'PageID-URL' and it contains Page ID's and the URL's that the id's are associated with.  It contains about 590 records.  The second one is labeled 'PageID-VisitorID' and it contains Page ID's associated with Visitor ID's.  It contains about 130,000 records.  The smaller table (PageID-URL) was chosen to be on the left because when doing the left join there needs to be some way to mark which elements in the PageID-URL are not contained in the PageID-VisitorID table.  Doing this marking will take either additional time or additional space that is proportional to the size of the left table.  In order to minimize time and space, the smaller table was chosen to be on the left.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "| Join Type | # of Rows |\n",
    "| - | - |\n",
    "| Left | 98,663 |\n",
    "| Right | 98,654 |\n",
    "| Inner | 98,654 |  \n",
    "\n",
    "Since the number of rows for the inner and right joins are the same, this means that there are not any entries in the PageID-VisitorID table that did not have PageID's in the PageID-URL table.\n",
    "\n",
    "Since the number of rows for the left join is greater than the inner join, this means that there were 9 (98,663 - 98,654) rows in the PageID-URL table that did not have PageID's in the PageID-VisitorID table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for hashside join** from Data-Intensive Text Processing with MapReduce by Jimmy Lin and Chris Dyer, section 3.5.3, page 67:  \n",
    "1. Load the smaller dataset into memory in every mapper, populating an associative array to facilitate random access to tuples based on the join key. The mapper initialization API hook (see Section 3.1.1) can be used for this purpose.\n",
    "2. Mappers are then applied to the other (larger) dataset, and for each input key-value pair, the mapper probes the in-memory dataset to see if there is a tuple with the same join key.\n",
    "3. If there is, the join is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (1) Left joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinLeft.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinLeft.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs left join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the left table, with the matching rows (matching Page ID's)\n",
    "# in the right table. The output is NULL or blank on the right side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinLeft(MRJob):\n",
    "    # Initialize a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    # Initialize a dictionary to keep track of the vroots (page_id's) that did not exist\n",
    "    # in the right table.\n",
    "    vroots_not_used = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot.  For each vroot\n",
    "        # in the file, store an empty entry for the vroot in the self.vroots_not_used dictionary\n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "                    self.vroots_not_used[page_id] = ''\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.  Delete the page_id from the self.vroots_not_used dictionary\n",
    "        # to indicate that this page_id has been output already.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                if page_id in self.vroots_not_used:\n",
    "                    del self.vroots_not_used[page_id]\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "\n",
    "    def mapper_memjoin_final(self):\n",
    "        # To complete the left join, for any vroots still left in the self.vroots_not_used\n",
    "        # dictionary, print them out with a None for the right side.\n",
    "        for page_id in self.vroots_not_used:\n",
    "            yield 'URL: ' + self.vroots[page_id], None\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.maps': '1'\n",
    "        }\n",
    "        return[\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin,\n",
    "                   mapper_final=self.mapper_memjoin_final)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinLeft.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98663 output/leftjoin/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "# Test this job with a smaller dataset\n",
    "#!python MemJoinLeft.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run the job on the full dataset\n",
    "!python MemJoinLeft.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/leftjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Left Join ######\n",
    "!wc -l output/leftjoin/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98663 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinLeft\n",
    "reload(MemJoinLeft)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            if value == None:\n",
    "                print key\n",
    "            else:\n",
    "                print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (2) Right joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinRight.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinRight.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# This class performs right join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the right table, with the matching rows (matching Page ID's)\n",
    "# in the left table. The output is NULL or blank on the left side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinRight(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file examine each line that starts with a 'V'.\n",
    "        # Yield each of these lines.  If there is no url for one of the lines in the \n",
    "        # file, then output it with a url of 'None'.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            url = 'None'\n",
    "            if page_id in self.vroots:\n",
    "                url = self.vroots[page_id]\n",
    "            yield 'URL: ' + url, page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinRight.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49062 output/rightjoin/part-00000\n",
      "49592 output/rightjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinRight.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinRight.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/rightjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Right Join ######\n",
    "!wc -l output/rightjoin/part-00000\n",
    "!wc -l output/rightjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinRight\n",
    "reload(MemJoinRight)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinInner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinInner.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs an inner join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will only output rows where the Page ID exists in both tables.  The output is \n",
    "# of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinInner(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinInner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823\n",
      "Removing temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823...\n",
      "49062 output/innerjoin/part-00000\n",
      "49592 output/innerjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinInner.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinInner.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/innerjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Inner Join ######\n",
    "!wc -l output/innerjoin/part-00000\n",
    "!wc -l output/innerjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinInner\n",
    "reload(MemJoinInner)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n",
    "\n",
    "## HW 5.3.1 OPTIONAL Question:\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALongest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALongest5gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDALongest5gram(MRJob):\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield record[0], len(ngram)\n",
    "    \n",
    "    def reducer_sum_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield ngram, sum(counts)\n",
    "    \n",
    "    def reducer_sort_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield ngram, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the characters) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_chars,   # STEP 1:  count the characters\n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    reducer=self.reducer_sort_chars)  # STEP 2:  sort the characters\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALongest5gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 16:38:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS 29\n",
      "A Circumstantial Narrative of the 29\n"
     ]
    }
   ],
   "source": [
    "import EDALongest5gram\n",
    "reload(EDALongest5gram)\n",
    "\n",
    "mr_job = EDALongest5gram.MREDALongest5gram(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    cur_value = \"\"\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count == 0 or value == cur_value:\n",
    "            print key, value\n",
    "            cur_value = value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Top 10 most frequent words (please use the count information), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAMostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAMostFrequentWords.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDAMostFrequentWords(MRJob):\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, int(record[1])\n",
    "    \n",
    "    def reducer_sum_chars(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def reducer_sort_chars(self, word, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the words) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_chars,   # STEP 1:  count the characters\n",
    "                   combiner=self.reducer_sum_chars,\n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    reducer=self.reducer_sort_chars)  # STEP 2:  sort the characters\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAMostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 18:42:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2217\n",
      "in 1201\n",
      "child's 1099\n",
      "christmas 1099\n",
      "wales 1099\n",
      "of 1011\n",
      "case 604\n",
      "study 604\n",
      "female 447\n",
      "collection 239\n"
     ]
    }
   ],
   "source": [
    "import EDAMostFrequentWords\n",
    "reload(EDAMostFrequentWords)\n",
    "\n",
    "mr_job = EDAMostFrequentWords.MREDAMostFrequentWords(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count < 10:\n",
    "            print key, value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAWordDensity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAWordDensity.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MREDAWordDensity(MRJob):\n",
    "    def mapper_count(self, _, line):\n",
    "        # read the next line from the file and output each word of the 5-gram and\n",
    "        # its count and page_count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, (int(record[1]), int(record[2]))\n",
    "    \n",
    "    def reducer_sum(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the \n",
    "        # word_counts and page_counts\n",
    "        word_total = 0\n",
    "        page_total = 0\n",
    "        for word_count, pages_count in counts:\n",
    "            word_total += word_count\n",
    "            page_total += pages_count\n",
    "        yield word, (word_total, page_total)\n",
    "    \n",
    "    def mapper_density(self, word, counts):\n",
    "        # output the word plus the word density (word_total / page_total)\n",
    "        word_total = float(counts[0])\n",
    "        page_total = float(counts[1])\n",
    "        yield word, word_total / page_total\n",
    "\n",
    "    def reducer_sort(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the words) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count,   # STEP 1:  count the words and pages\n",
    "                   combiner=self.reducer_sum,\n",
    "                   reducer=self.reducer_sum),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,      # STEP 2:  compute and sort the densities\n",
    "                   mapper=self.mapper_density,\n",
    "                   reducer=self.reducer_sort)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAWordDensity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDAWordDensity.hadoop.20160611.001445.732679\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/EDAWordDensity.hadoop.20160611.001445.732679/output...\n",
      "\"a\"\t1.0282931354359925\n",
      "\"bill\"\t1.0\n",
      "\"biography\"\t1.0222222222222221\n",
      "\"by\"\t1.0333333333333334\n",
      "\"case\"\t1.0\n",
      "\"child's\"\t1.0358152686145146\n",
      "\"christmas\"\t1.0358152686145146\n",
      "\"circumstantial\"\t1.0\n",
      "\"city\"\t1.0333333333333334\n",
      "\"collection\"\t1.0863636363636364\n",
      "\"establishing\"\t1.0\n",
      "\"fairy\"\t1.0512820512820513\n",
      "\"female\"\t1.0\n",
      "\"for\"\t1.0\n",
      "\"forms\"\t1.1262135922330097\n",
      "\"general\"\t1.0222222222222221\n",
      "\"george\"\t1.0222222222222221\n",
      "\"government\"\t1.0\n",
      "\"in\"\t1.0326741186586414\n",
      "\"limited\"\t1.0\n",
      "\"narrative\"\t1.0\n",
      "\"of\"\t1.0348004094165815\n",
      "\"religious\"\t1.0\n",
      "\"sea\"\t1.0333333333333334\n",
      "\"study\"\t1.0\n",
      "\"tales\"\t1.0512820512820513\n",
      "\"the\"\t1.0163934426229508\n",
      "\"wales\"\t1.0358152686145146\n",
      "Removing temp directory /tmp/EDAWordDensity.hadoop.20160611.001445.732679...\n"
     ]
    }
   ],
   "source": [
    "!python EDAWordDensity.py 5gram_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 18:40:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forms 1.12621359223\n",
      "collection 1.08636363636\n",
      "fairy 1.05128205128\n",
      "tales 1.05128205128\n",
      "child's 1.03581526861\n",
      "christmas 1.03581526861\n",
      "wales 1.03581526861\n",
      "of 1.03480040942\n",
      "by 1.03333333333\n",
      "city 1.03333333333\n",
      "sea 1.03333333333\n",
      "in 1.03267411866\n",
      "a 1.02829313544\n",
      "biography 1.02222222222\n",
      "general 1.02222222222\n",
      "george 1.02222222222\n",
      "the 1.01639344262\n",
      "bill 1.0\n",
      "case 1.0\n",
      "circumstantial 1.0\n",
      "establishing 1.0\n",
      "female 1.0\n",
      "for 1.0\n",
      "government 1.0\n",
      "limited 1.0\n",
      "narrative 1.0\n",
      "religious 1.0\n",
      "study 1.0\n"
     ]
    }
   ],
   "source": [
    "import EDAWordDensity\n",
    "reload(EDAWordDensity)\n",
    "\n",
    "#mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt'])\n",
    "mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Output the words and their densities in descending order by density\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALengthCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALengthCount.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the unique lengths of the 5-grams and the frequency of those lengths\n",
    "class MREDALengthCount(MRJob):\n",
    "    def mapper_count_lengths(self, _, line):\n",
    "        # read the next line from the file and output the length of the 5-gram and the count\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield len(ngram), int(record[1])\n",
    "    \n",
    "    def reducer_sum_lengths(self, length, counts):\n",
    "        # output each length (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield length, sum(counts)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_lengths,   # STEP 1:  count the lengths\n",
    "                   combiner=self.reducer_sum_lengths,\n",
    "                   reducer=self.reducer_sum_lengths)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALengthCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bb0f71790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bb4933c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import EDALengthCount\n",
    "reload(EDALengthCount)\n",
    "\n",
    "mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt'])\n",
    "#mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # Initialize a dictionary to store the lengths and frequencies\n",
    "    len_freq = {}\n",
    "    # stream_output: get access of the output \n",
    "    # the key is the length of a 5-gram and the value is number of times that\n",
    "    # length occurs.\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        #print key, value\n",
    "        len_freq[int(key)] = int(value)\n",
    "\n",
    "# Plot a histogram of the lengths and their frequencies\n",
    "df = pandas.DataFrame.from_dict(len_freq, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 01:13:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/11 01:13:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-65-filtered.txt\n"
     ]
    }
   ],
   "source": [
    "# Load the 5-gram data in to HDFS\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5data\n",
    "#!hdfs dfs -rm /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-65-filtered.txt\n",
    "for i in range(65,190):\n",
    "    filename = 'data/googlebooks-eng-all-5gram-20090715-' + str(i) + '-filtered.txt'\n",
    "    !hdfs dfs -copyFromLocal $filename /user/hadoop/HW5data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 01:11:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 66 items\n",
      "-rw-r--r--   3 hadoop supergroup   11444614 2016-06-10 20:54 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-11 01:06 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-1-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11447003 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-10-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11495017 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-11-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11443168 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-12-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11485635 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-13-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11442376 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-14-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11452631 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-15-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11525458 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-16-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11509921 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-17-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11492843 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-18-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11446280 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-19-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11469040 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-2-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11483940 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-20-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11419380 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-21-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11504055 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-22-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11444833 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-23-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11470920 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-24-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11447510 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-25-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11451162 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-26-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11432861 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-27-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11473738 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-28-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11466285 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-29-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11473472 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-3-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11451427 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-30-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11445890 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-31-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11451034 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-32-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11448837 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-33-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11477127 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-34-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11482525 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-35-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11485001 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-36-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11462556 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-37-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11471356 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-38-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11463027 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-39-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11474462 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-4-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11479067 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-40-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11492731 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-41-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11444143 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-42-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11457891 2016-06-11 01:09 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-43-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11477565 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-44-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11491534 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-45-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-46-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11437493 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-47-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11486342 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-48-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11506157 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-49-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11486767 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-5-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11448291 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-50-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11466332 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-51-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11454748 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-52-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11473706 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-53-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11449750 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-54-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11440722 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-55-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11443428 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-56-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11523191 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-57-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11464003 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-58-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11450319 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-59-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11465378 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-6-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11477145 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-60-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11484900 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-61-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11461230 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-62-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11479759 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-63-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11445477 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-64-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11519941 2016-06-11 01:10 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-65-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11459099 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-7-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11455035 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-8-filtered.txt\n",
      "-rw-r--r--   3 hadoop supergroup   11450336 2016-06-11 01:07 /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-9-filtered.txt\n"
     ]
    }
   ],
   "source": [
    "# List the files in the HW5data directory to ensure that they are all in there\n",
    "!hdfs dfs -ls /user/hadoop/HW5data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 5, 'a': 4, 'c': 3, 'b': 2, 'd': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bb4131710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD8CAYAAACrbmW5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpyyguPTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bb4155410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "a = ['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'c', 'd', 'e', 'e', 'e', 'e', 'e']\n",
    "letter_counts = Counter(a)\n",
    "print(letter_counts)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
