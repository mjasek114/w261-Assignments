{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  5\n",
    "* **Date:**  6/17/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "A data warehouse is a very large repository for data.  It can include many types of data like sales transactions or product inventories.  It can store relational (structured) data but more and more they are used to store unstructured data like tweets or semi-structured data like log files.  A data warehouse creates a base for data analysis activities like business intelligence and data mining.\n",
    "\n",
    "The Star schema is a type of database schema.  It is made up of one or more fact tables referencing one or more dimension tables.  A fact table holds measurable quantitative information and a dimension table holds descriptive attributes related to the fact data.  It is used in relational databases to organize information.\n",
    "\n",
    "\n",
    "## HW 5.1\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "The process of normalizing a database is the process of breaking down the data into smaller tables and linking them by keys.  The way in which you break down the data is such that no data is duplicated in the tables except for any information needed for linking the tables.  When data is not duplicated it saves space and allows for easier updates as the data only needs to be updated in one place.  3NF is a type of database normalization that reduces the duplication of data and ensures referential integrity.\n",
    "\n",
    "Machine learning does not usually use data in 3NF form.\n",
    "\n",
    "- In what form does ML consume data?  \n",
    "\n",
    "Machine learning consumes data in denormalized form.  This is because with machine learning, the algorithms usually need all of the data fields together as a set of features.  If many of the features that are desired are stored in different tables of the database, these tables would need to be joined together before the machine learning algorithm could have all of the features in one place.\n",
    "\n",
    "- Why would one use log files that are denormalized?  \n",
    "\n",
    "One would use log files that are denormalized in order to feed the features directly into a machine learning model without having to go through a process of joining additional features in to the log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "**ANSWER:** There are 2 tables in this problem.  The first one is labeled 'PageID-URL' and it contains Page ID's and the URL's that the id's are associated with.  It contains about 590 records.  The second one is labeled 'PageID-VisitorID' and it contains Page ID's associated with Visitor ID's.  It contains about 130,000 records.  The smaller table (PageID-URL) was chosen to be on the left because when doing the left join there needs to be some way to mark which elements in the PageID-URL are not contained in the PageID-VisitorID table.  Doing this marking will take additional time and space that is proportional to the size of the left table.  In order to minimize time and space, the smaller table was chosen to be on the left.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "| Join Type | # of Rows |\n",
    "| - | - |\n",
    "| Left | 98,663 |\n",
    "| Right | 98,654 |\n",
    "| Inner | 98,654 |  \n",
    "\n",
    "Since the number of rows for the inner and right joins are the same, this means that there are not any entries in the PageID-VisitorID table that did not have PageID's in the PageID-URL table.\n",
    "\n",
    "Since the number of rows for the left join is greater than the inner join, this means that there were 9 (98,663 - 98,654) rows in the PageID-URL table that did not have PageID's in the PageID-VisitorID table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for hashside join** from Data-Intensive Text Processing with MapReduce by Jimmy Lin and Chris Dyer, section 3.5.3, page 67:  \n",
    "1. Load the smaller dataset into memory in every mapper, populating an associative array to facilitate random access to tuples based on the join key. The mapper initialization API hook (see Section 3.1.1) can be used for this purpose.\n",
    "2. Mappers are then applied to the other (larger) dataset, and for each input key-value pair, the mapper probes the in-memory dataset to see if there is a tuple with the same join key.\n",
    "3. If there is, the join is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (1) Left joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinLeft.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinLeft.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs left join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the left table, with the matching rows (matching Page ID's)\n",
    "# in the right table. The output is NULL or blank on the right side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinLeft(MRJob):\n",
    "    # Initialize a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    # Initialize a dictionary to keep track of the vroots (page_id's) that did not exist\n",
    "    # in the right table.\n",
    "    vroots_not_used = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot.  For each vroot\n",
    "        # in the file, store an empty entry for the vroot in the self.vroots_not_used dictionary\n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "                    self.vroots_not_used[page_id] = ''\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.  Delete the page_id from the self.vroots_not_used dictionary\n",
    "        # to indicate that this page_id has been output already.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                if page_id in self.vroots_not_used:\n",
    "                    del self.vroots_not_used[page_id]\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "\n",
    "    def mapper_memjoin_final(self):\n",
    "        # To complete the left join, for any vroots still left in the self.vroots_not_used\n",
    "        # dictionary, print them out with a None for the right side.\n",
    "        for page_id in self.vroots_not_used:\n",
    "            yield 'URL: ' + self.vroots[page_id], None\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.maps': '1'\n",
    "        }\n",
    "        return[\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin,\n",
    "                   mapper_final=self.mapper_memjoin_final)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinLeft.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98663 output/leftjoin/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "# Test this job with a smaller dataset\n",
    "#!python MemJoinLeft.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run the job on the full dataset\n",
    "!python MemJoinLeft.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/leftjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Left Join ######\n",
    "!wc -l output/leftjoin/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98663 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinLeft\n",
    "reload(MemJoinLeft)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            if value == None:\n",
    "                print key\n",
    "            else:\n",
    "                print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (2) Right joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinRight.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinRight.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# This class performs right join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the right table, with the matching rows (matching Page ID's)\n",
    "# in the left table. The output is NULL or blank on the left side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinRight(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file examine each line that starts with a 'V'.\n",
    "        # Yield each of these lines.  If there is no url for one of the lines in the \n",
    "        # file, then output it with a url of 'None'.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            url = 'None'\n",
    "            if page_id in self.vroots:\n",
    "                url = self.vroots[page_id]\n",
    "            yield 'URL: ' + url, page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinRight.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49062 output/rightjoin/part-00000\n",
      "49592 output/rightjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinRight.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinRight.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/rightjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Right Join ######\n",
    "!wc -l output/rightjoin/part-00000\n",
    "!wc -l output/rightjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinRight\n",
    "reload(MemJoinRight)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinInner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinInner.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs an inner join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will only output rows where the Page ID exists in both tables.  The output is \n",
    "# of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinInner(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinInner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823\n",
      "Removing temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823...\n",
      "49062 output/innerjoin/part-00000\n",
      "49592 output/innerjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinInner.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinInner.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/innerjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Inner Join ######\n",
    "!wc -l output/innerjoin/part-00000\n",
    "!wc -l output/innerjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinInner\n",
    "reload(MemJoinInner)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- **Longest 5-gram (number of characters)**  \n",
    "**ANSWER:**  There were two 5-grams with the longest length of 155 characters.  Spaces and apostrophes were not counted.\n",
    "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t155\n",
    "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"\t155\n",
    "\n",
    "- **Top 10 most frequent words (please use the count information), i.e., unigrams**  \n",
    "**ANSWER:**  \n",
    "\"the\"\t5,490,815,394  \n",
    "\"of\"\t3,698,583,299  \n",
    "\"to\"\t2,227,866,570  \n",
    "\"in\"\t1,421,312,776  \n",
    "\"a\"\t1,361,123,022  \n",
    "\"and\"\t1,149,577,477  \n",
    "\"that\"\t802,921,147  \n",
    "\"is\"\t758,328,796  \n",
    "\"be\"\t688,707,130  \n",
    "\"as\"\t492,170,314  \n",
    "\n",
    "- **20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency**  \n",
    "**ANSWER:**  \n",
    "**20 MOST DENSE WORDS**  \n",
    "\"xxxx\"\t11.557291666666666  \n",
    "\"blah\"\t8.074159907300116  \n",
    "\"nnn\"\t7.533333333333333  \n",
    "\"na\"\t6.201749131424464  \n",
    "\"oooooooooooooooo\"\t4.921875  \n",
    "\"nd\"\t4.85430572723527  \n",
    "\"llll\"\t4.511627906976744  \n",
    "\"oooooo\"\t4.169650013358269  \n",
    "\"ooooo\"\t3.858637193467213  \n",
    "\"lillelu\"\t3.7624521072796937  \n",
    "\"madarassy\"\t3.576923076923077  \n",
    "\"pfeffermann\"\t3.576923076923077  \n",
    "\"meteoritical\"\t3.56  \n",
    "\"xxxxxxxx\"\t3.5  \n",
    "\"beep\"\t3.229038854805726  \n",
    "\"latha\"\t3.188679245283019  \n",
    "\"iyengar\"\t2.9191176470588234  \n",
    "\"counterfeiteth\"\t2.825  \n",
    "\"nonmorular\"\t2.81981981981982  \n",
    "\"nonsquamous\"\t2.81981981981982  \n",
    "**20 LEAST DENSE WORDS**  \n",
    "\"zwingst\"\t1.0  \n",
    "\"zwirnen\"\t1.0  \n",
    "\"zwischenstaatlicher\"\t1.0  \n",
    "\"zwitterionic\"\t1.0  \n",
    "\"zwt\"\t1.0  \n",
    "\"zwyn\"\t1.0  \n",
    "\"zx\"\t1.0  \n",
    "\"zxcvframeqasfuc\"\t1.0  \n",
    "\"zydeco\"\t1.0  \n",
    "\"zydom\"\t1.0  \n",
    "\"zygmunt\"\t1.0  \n",
    "\"zygomaticofacial\"\t1.0  \n",
    "\"zygomaticotemporal\"\t1.0  \n",
    "\"zygosity\"\t1.0  \n",
    "\"zylindrischen\"\t1.0  \n",
    "\"zymelman\"\t1.0  \n",
    "\"zymogens\"\t1.0  \n",
    "\"zymophore\"\t1.0  \n",
    "\"zymosan\"\t1.0  \n",
    "\"zymosis\"\t1.0  \n",
    "\n",
    "- **Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.**  \n",
    "**ANSWER:**  See histogram plotted below in section labeled: 'OUTPUT FOR:  Distribution of 5-gram sizes (character length)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALongest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALongest5gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDALongest5gram(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield record[0], str(len(ngram))\n",
    "    \n",
    "    def reducer_sum_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts.\n",
    "        c_sum = 0\n",
    "        for c in counts:\n",
    "            c_sum += int(c)\n",
    "        yield ngram, str(c_sum)\n",
    "\n",
    "    def mapper_sort_chars(self, word, value):\n",
    "        # add a label to each input for sorting based on the value\n",
    "        if int(value) < 75:\n",
    "            label = 'a'\n",
    "        else:\n",
    "            label = 'b'\n",
    "        yield label, str(value)+'\\t'+str(word)\n",
    "    \n",
    "    def reducer_sort_chars(self, label, value_pair):\n",
    "        # Output what is received.  The values are sorted at this point.\n",
    "        for vp in value_pair:\n",
    "            v, w = vp.split('\\t')\n",
    "            yield w, v\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {        \n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the characters).\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '100',\n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,            # STEP 1:  count the characters\n",
    "                   mapper=self.mapper_count_chars,   \n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,            # STEP 2:  sort the characters\n",
    "                   mapper=self.mapper_sort_chars,\n",
    "                   reducer=self.reducer_sort_chars)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALongest5gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the program on the small dataset\n",
    "!python EDALongest5gram.py -r hadoop 5gram_small.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 12:57:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/11 12:57:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### Upload test files to HDFS for testing\n",
    "#!hdfs dfs -copyFromLocal '5gram_small.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_half.txt' /user/hadoop\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5data-test\n",
    "#!hdfs dfs -rm /user/hadoop/HW5data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt' /user/hadoop/HW5data-test\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_tail.txt' /user/hadoop/HW5data-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 17:40:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/outputHW5/EDALongest5gram': No such file or directory\n",
      "nohup: ignoring input and appending output to ‘nohup.out’\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDALongest5gram'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDALongest5gram.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/17 23:42:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\t155\n",
      "AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\t155\n",
      "RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR JDDUMHUYPARICOLEVTYPS ILONBUNURT\t124\n",
      "RTYEARTHQUAKECVBNMKDSAW VBNEVWVOLCANICERUPTIONS FLOODSCVBEAVALANCHESUYT VBNHURRICANESTORNADOESX TIDALWAVECVBNCYCLONECVE\t115\n",
      "SHIPWRECKIERTGBVCXWQXCE CCCVBNWSESWDFGFIRESNPLM WQRAILROADWRECKSUTRFHJK EXPLOSIONSTRPIQURECEDFG RTYEARTHQUAKECVBNMKDSAW\t115\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "##### PREPARE FILES FOR HW5.3 LONGEST 5-GRAM #####\n",
    "#!hdfs dfs -cat outputHW5/EDALongest5gram/part-00000 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDALongest5gram/part-00001 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDALongest5gram/part-00000 > EDALongest5gram.txt\n",
    "#!hdfs dfs -cat outputHW5/EDALongest5gram/part-00001 >> EDALongest5gram.txt\n",
    "#!hdfs dfs -copyFromLocal EDALongest5gram.txt HW5Results\n",
    "\n",
    "##### OUTPUT FOR HW5.3 LONGEST 5-GRAM #####\n",
    "!hdfs dfs -cat /user/hadoop/HW5Results/EDALongest5gram.txt | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/17 23:45:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/17 23:45:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Circumstantial Narrative of the 29\n",
      "\n",
      "A BILL FOR ESTABLISHING RELIGIOUS 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import EDALongest5gram\n",
    "reload(EDALongest5gram)\n",
    "\n",
    "mr_job = EDALongest5gram.MREDALongest5gram(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    cur_value = \"\"\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count == 0 or value == cur_value:\n",
    "            print key, value\n",
    "            cur_value = value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Top 10 most frequent words (please use the count information), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAMostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAMostFrequentWords.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the the most frequent words occruing in the 5-grams \n",
    "# in descending order.\n",
    "class MREDAMostFrequentWords(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output each word with its count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, record[1]\n",
    "    \n",
    "    def reducer_sum_chars(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the counts.\n",
    "        c_sum = 0\n",
    "        for c in counts:\n",
    "            c_sum += int(c)\n",
    "        yield word, str(c_sum)\n",
    "    \n",
    "    def mapper_sort_chars(self, word, value):\n",
    "        # add a label to each input for sorting based on the value\n",
    "        #if int(value) < 1000:\n",
    "        if int(value) < 100000:\n",
    "            label = 'a'\n",
    "        else:\n",
    "            label = 'b'\n",
    "        yield label, str(value)+'\\t'+str(word)\n",
    "    \n",
    "    def reducer_sort_chars(self, label, value_pair):\n",
    "        # Output what is received.  The values are sorted at this point.\n",
    "        for vp in value_pair:\n",
    "            v, w = vp.split('\\t')\n",
    "            yield w, v\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {        \n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the characters).\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '100',\n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,            # STEP 1:  count the characters\n",
    "                   mapper=self.mapper_count_chars,   \n",
    "                   combiner=self.reducer_sum_chars,\n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,            # STEP 2:  sort the characters\n",
    "                   mapper=self.mapper_sort_chars,\n",
    "                   reducer=self.reducer_sort_chars)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAMostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 18:42:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2217\n",
      "in 1201\n",
      "child's 1099\n",
      "christmas 1099\n",
      "wales 1099\n",
      "of 1011\n",
      "case 604\n",
      "study 604\n",
      "female 447\n",
      "collection 239\n"
     ]
    }
   ],
   "source": [
    "import EDAMostFrequentWords\n",
    "reload(EDAMostFrequentWords)\n",
    "\n",
    "mr_job = EDAMostFrequentWords.MREDAMostFrequentWords(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count < 10:\n",
    "            print key, value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/17 23:51:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/17 23:51:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW5/EDAMostFrequent\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDAMostFrequentWords.hadoop.20160618.045111.068646\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/EDAMostFrequentWords.hadoop.20160618.045111.068646/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar4824390298770485761/] [] /tmp/streamjob8604883443540777318.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1466193215040_0022\n",
      "  Submitted application application_1466193215040_0022\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0022/\n",
      "  Running job: job_1466193215040_0022\n",
      "  Job job_1466193215040_0022 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0022 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/EDAMostFrequentWords.hadoop.20160618.045111.068646/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=564\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=301\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=379\n",
      "\t\tFILE: Number of bytes written=364474\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=884\n",
      "\t\tHDFS: Number of bytes written=301\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5286912\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3629056\n",
      "\t\tTotal time spent by all map tasks (ms)=5163\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5163\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3544\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5163\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3544\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1930\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=216\n",
      "\t\tInput split bytes=320\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=502\n",
      "\t\tMap output materialized bytes=385\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=686571520\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=385\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=60\n",
      "\t\tTotal committed heap usage (bytes)=520093696\n",
      "\t\tVirtual memory (bytes) snapshot=6349504512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar2947415443586971643/] [] /tmp/streamjob7656082363506997860.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1466193215040_0023\n",
      "  Submitted application application_1466193215040_0023\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0023/\n",
      "  Running job: job_1466193215040_0023\n",
      "  Job job_1466193215040_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0023 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/outputHW5/EDAMostFrequent\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=452\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=301\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=425\n",
      "\t\tFILE: Number of bytes written=486652\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=784\n",
      "\t\tHDFS: Number of bytes written=301\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10955776\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6884352\n",
      "\t\tTotal time spent by all map tasks (ms)=10699\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10699\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6723\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6723\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10699\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6723\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2730\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=384\n",
      "\t\tInput split bytes=332\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=357\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=817803264\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=630718464\n",
      "\t\tVirtual memory (bytes) snapshot=8392962048\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/outputHW5/EDAMostFrequent...\n",
      "a\t2217\n",
      "in\t1201\n",
      "christmas\t1099\n",
      "wales\t1099\n",
      "child's\t1099\n",
      "of\t1011\n",
      "STDERR: 16/06/17 23:52:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "case\t604\n",
      "study\t604\n",
      "female\t447\n",
      "collection\t239\n",
      "the\t124\n",
      "fairy\t123\n",
      "tales\t123\n",
      "forms\t116\n",
      "government\t102\n",
      "biography\t92\n",
      "george\t92\n",
      "general\t92\n",
      "city\t62\n",
      "sea\t62\n",
      "circumstantial\t62\n",
      "narrative\t62\n",
      "by\t62\n",
      "religious\t59\n",
      "establishing\t59\n",
      "bill\t59\n",
      "for\t59\n",
      "limited\t55\n",
      "STDERR: 16/06/17 23:52:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/EDAMostFrequentWords.hadoop.20160618.045111.068646...\n",
      "Removing temp directory /tmp/EDAMostFrequentWords.hadoop.20160618.045111.068646...\n"
     ]
    }
   ],
   "source": [
    "## Local file locations\n",
    "ifile = '5gram_small.txt'\n",
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "ofile = 'outputHW5/EDAMostFrequent'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python EDAMostFrequentWords.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python EDAMostFrequentWords.py -r hadoop $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/18 10:10:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the\t5490815394\n",
      "of\t3698583299\n",
      "to\t2227866570\n",
      "in\t1421312776\n",
      "a\t1361123022\n",
      "and\t1149577477\n",
      "that\t802921147\n",
      "is\t758328796\n",
      "be\t688707130\n",
      "as\t492170314\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "##### PREPARE FILES FOR HW5.3 MOST FREQUENT WORD #####\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00000 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00001 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00000 | wc -l\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00001 | wc -l\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00000 > EDAMostFrequentWords.txt\n",
    "#!hdfs dfs -cat outputHW5/EDAMostFrequentWords/part-00001 >> EDAMostFrequentWords.txt\n",
    "#!hdfs dfs -copyFromLocal EDAMostFrequentWords.txt HW5Results\n",
    "\n",
    "##### OUTPUT FOR HW5.3 MOST FREQUENT WORD #####\n",
    "!hdfs dfs -cat /user/hadoop/HW5Results/EDAMostFrequentWords.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAWordDensity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAWordDensity.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MREDAWordDensity(MRJob):\n",
    "    MRJob.SORT_VALUES = True\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    def mapper_count(self, _, line):\n",
    "        # read the next line from the file and output each word of the 5-gram and\n",
    "        # its count and page_count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, record[1] + '.' + record[2]\n",
    "    \n",
    "    def reducer_sum(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the \n",
    "        # word_counts and page_counts\n",
    "        word_total = 0\n",
    "        page_total = 0\n",
    "        for c in counts:\n",
    "            word_count, pages_count = c.split('.')\n",
    "            word_total += int(word_count)\n",
    "            page_total += int(pages_count)\n",
    "        yield word, str(word_total) + '.' + str(page_total)\n",
    "    \n",
    "    def mapper_density(self, word, counts):\n",
    "        # output the word plus the word density (word_total / page_total)\n",
    "        word_total, page_total = counts.split('.')\n",
    "        density = float(word_total) / float(page_total)\n",
    "        if density < 2.0:\n",
    "            label = 'a'\n",
    "        else:\n",
    "            label = 'b'\n",
    "        yield label, str(density)+'\\t'+str(word)\n",
    "\n",
    "    def reducer_sort(self, label, value_pair):\n",
    "        # Output what is received.  The values are sorted at this point.\n",
    "        for vp in value_pair:\n",
    "            v, w = vp.split('\\t')\n",
    "            yield w, v\n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {        \n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the words).\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.reduces': '2',\n",
    "            'mapreduce.job.maps': '2'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,      # STEP 1:  count the words and pages\n",
    "                   mapper=self.mapper_count,   \n",
    "                   combiner=self.reducer_sum,\n",
    "                   reducer=self.reducer_sum),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,      # STEP 2:  compute and sort the densities\n",
    "                   mapper=self.mapper_density,\n",
    "                   reducer=self.reducer_sort)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAWordDensity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDAWordDensity.hadoop.20160618.160444.500915\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/EDAWordDensity.hadoop.20160618.160444.500915/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar7790032683772958750/] [] /tmp/streamjob3835402448769547107.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1466193215040_0056\n",
      "  Submitted application application_1466193215040_0056\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0056/\n",
      "  Running job: job_1466193215040_0056\n",
      "  Job job_1466193215040_0056 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0056 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/EDAWordDensity.hadoop.20160618.160444.500915/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=564\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=405\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=497\n",
      "\t\tFILE: Number of bytes written=485576\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=872\n",
      "\t\tHDFS: Number of bytes written=405\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5563392\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5954560\n",
      "\t\tTotal time spent by all map tasks (ms)=5433\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5433\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5815\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5815\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5433\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5815\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2910\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=262\n",
      "\t\tInput split bytes=308\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=682\n",
      "\t\tMap output materialized bytes=509\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=854634496\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=509\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=60\n",
      "\t\tTotal committed heap usage (bytes)=649068544\n",
      "\t\tVirtual memory (bytes) snapshot=8479723520\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar5595128963211111896/] [] /tmp/streamjob2318109390653242281.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1466193215040_0057\n",
      "  Submitted application application_1466193215040_0057\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0057/\n",
      "  Running job: job_1466193215040_0057\n",
      "  Job job_1466193215040_0057 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0057 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/EDAWordDensity.hadoop.20160618.160444.500915/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=463\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=478\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=602\n",
      "\t\tFILE: Number of bytes written=606805\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=943\n",
      "\t\tHDFS: Number of bytes written=478\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7641088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6588416\n",
      "\t\tTotal time spent by all map tasks (ms)=7462\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7462\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6434\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6434\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7462\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6434\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=357\n",
      "\t\tInput split bytes=480\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=534\n",
      "\t\tMap output materialized bytes=626\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tPhysical memory (bytes) snapshot=1106059264\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=626\n",
      "\t\tShuffled Maps =6\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=827326464\n",
      "\t\tVirtual memory (bytes) snapshot=10570338304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/EDAWordDensity.hadoop.20160618.160444.500915/output...\n",
      "STDERR: 16/06/18 11:05:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "forms\t1.12621359223\n",
      "collection\t1.08636363636\n",
      "tales\t1.05128205128\n",
      "fairy\t1.05128205128\n",
      "wales\t1.03581526861\n",
      "christmas\t1.03581526861\n",
      "child's\t1.03581526861\n",
      "of\t1.03480040942\n",
      "by\t1.03333333333\n",
      "city\t1.03333333333\n",
      "sea\t1.03333333333\n",
      "in\t1.03267411866\n",
      "a\t1.02829313544\n",
      "george\t1.02222222222\n",
      "biography\t1.02222222222\n",
      "general\t1.02222222222\n",
      "the\t1.01639344262\n",
      "study\t1.0\n",
      "narrative\t1.0\n",
      "limited\t1.0\n",
      "female\t1.0\n",
      "case\t1.0\n",
      "for\t1.0\n",
      "establishing\t1.0\n",
      "circumstantial\t1.0\n",
      "government\t1.0\n",
      "bill\t1.0\n",
      "religious\t1.0\n",
      "STDERR: 16/06/18 11:05:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/EDAWordDensity.hadoop.20160618.160444.500915...\n",
      "Removing temp directory /tmp/EDAWordDensity.hadoop.20160618.160444.500915...\n"
     ]
    }
   ],
   "source": [
    "!python EDAWordDensity.py -r hadoop 5gram_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/11 20:21:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import EDAWordDensity\n",
    "reload(EDAWordDensity)\n",
    "\n",
    "#mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt'])\n",
    "mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Output the words and their densities in descending order by density\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        #print key, value\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDAWordDensity'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDAWordDensity.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### PREPARE FILES FOR HW5.3 MOST/LEAST WORD DENSITY #####\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00000 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00001 | head -5\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00000 | wc -l\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00001 | wc -l\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00000 > EDAWordDensity.txt\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00001 >> EDAWordDensity.txt\n",
    "#!hdfs dfs -copyFromLocal EDAWordDensity.txt HW5Results\n",
    "\n",
    "##### OUTPUT FOR HW5.3 MOST/LEAST WORD DENSITY #####\n",
    "#!hdfs dfs -cat /user/hadoop/HW5Results/EDAWordDensity.txt | head -20\n",
    "#!hdfs dfs -cat /user/hadoop/HW5Results/EDAWordDensity.txt | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALengthCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALengthCount.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the unique lengths of the 5-grams and the frequency of those lengths\n",
    "class MREDALengthCount(MRJob):\n",
    "    def mapper_count_lengths(self, _, line):\n",
    "        # read the next line from the file and output the length of the 5-gram and the count\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield len(ngram), int(record[1])\n",
    "    \n",
    "    def reducer_sum_lengths(self, length, counts):\n",
    "        # output each length (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield length, sum(counts)\n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.reduces': '5'             # Specify 5 reducers\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper_count_lengths,\n",
    "                   combiner=self.reducer_sum_lengths,\n",
    "                   reducer=self.reducer_sum_lengths)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALengthCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import EDALengthCount\n",
    "reload(EDALengthCount)\n",
    "\n",
    "mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt'])\n",
    "#mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # Initialize a dictionary to store the lengths and frequencies\n",
    "    len_freq = {}\n",
    "    # stream_output: get access of the output \n",
    "    # the key is the length of a 5-gram and the value is number of times that\n",
    "    # length occurs.\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        #print key, value\n",
    "        len_freq[int(key)] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "\n",
    "# Plot a histogram of the lengths and their frequencies\n",
    "#df = pandas.DataFrame.from_dict(len_freq, orient='index')\n",
    "#df = df.sort_index()\n",
    "#df.plot(kind='bar', figsize=(5,5), legend=False, title='Distribution of 5-gram sizes (character length)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/17 18:32:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/17 18:32:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW5/EDALengthCount\n",
      "nohup: ignoring input and appending output to ‘nohup.out’\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDALengthCount'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDALengthCount.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### PREPARE DATA FOR HW5.3 5-GRAM LENGTH DISTRIBUTIONS #####\n",
    "#!rm EDALengthCount_results.text\n",
    "#!hdfs dfs -cat outputHW5/EDALengthCount/part-00000 > EDALengthCount_results.text\n",
    "#!hdfs dfs -cat outputHW5/EDALengthCount/part-00001 >> EDALengthCount_results.text\n",
    "#!hdfs dfs -cat outputHW5/EDALengthCount/part-00002 >> EDALengthCount_results.text\n",
    "#!hdfs dfs -cat outputHW5/EDALengthCount/part-00003 >> EDALengthCount_results.text\n",
    "#!hdfs dfs -cat outputHW5/EDALengthCount/part-00004 >> EDALengthCount_results.text\n",
    "#!wc -l EDALengthCount_results.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f02acbff350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAAHyCAYAAACd/wkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAgEEZfgAAAAAGZfgBAAAAGNQLx96HBxP2P1kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02acd6dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.3 5-GRAM LENGTH DISTRIBUTIONS #####\n",
    "%matplotlib inline\n",
    "import pandas\n",
    "\n",
    "# Initialize a dictionary to store the lengths and frequencies\n",
    "len_freq = {}\n",
    "filename = 'EDALengthCount_results.text'\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        key,value =  line.strip().split('\\t',2)\n",
    "        len_freq[int(key)] = int(value)\n",
    "\n",
    "#http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html\n",
    "\n",
    "# Plot a histogram of the lengths and their frequencies\n",
    "df = pandas.DataFrame.from_dict(len_freq, orient='index')\n",
    "df = df.sort_index()\n",
    "df.plot(kind='bar', figsize=(20,8), legend=False,\n",
    "        title='Distribution of 5-gram sizes (character length)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n",
    "**ANSWER:**  \n",
    "\n",
    "**S3 Locations:**  \n",
    "**stripes:**  **bucket:**  berkeley.mids.w261.hw5, **folder:**  HW5Results, **file:**  stripes_2.txt  \n",
    "**link:** https://s3-us-west-2.amazonaws.com/berkeley.mids.w261.hw5/HW5Results/stripes_2.txt  \n",
    "\n",
    "**similarities:**  **bucket:**  berkeley.mids.w261.hw5, **folder:**  HW5Results, **file:**  similarity_sorted.txt  \n",
    "**link:** https://s3-us-west-2.amazonaws.com/berkeley.mids.w261.hw5/HW5Results/similarity_sorted.txt\n",
    "\n",
    "**Cluster configuration**  \n",
    "\n",
    "|  | Description |\n",
    "| - | - |\n",
    "| Cloud environment | IBM SoftLayer |\n",
    "| Master server | 2 CPU, 8GB RAM, 25GB disk, 100GB disk |\n",
    "| 3 slaves | 2 CPU, 8GB RAM, 25GB disk, 100GB disk |\n",
    "| 2 slaves | 8 CPU, 16GB RAM, 100GB disk, 100GB disk |\n",
    "| **TOTAL** | **24 CPU, 64GB RAM, 300GB disk, 600GB disk** |\n",
    "\n",
    "**Timings**  \n",
    "\n",
    "| Task | Time |\n",
    "| - | - |\n",
    "| Build Inverted Index | 1.5mins |\n",
    "| Calculate Similarities | 1hr, 24mins |\n",
    "\n",
    "**Number of Pairs Processed**  \n",
    "**Actual Number of pairs processed**  \n",
    "The number of lines in the output from the similarity job was 35,039,156 so 35,039,156 pairs were processed.  \n",
    "**Actual Number of pairs processed = 35,039,156 (about 35 million)**    \n",
    "\n",
    "**Theoretical Number of pairs processed** \n",
    "Theoretically, the number of pairs that could potentially processed in this problem is as follows:  \n",
    "Number of pairs processed = (# of rows in the inverted index) * (# of pairs processed per row)  \n",
    "Number of pairs processed = (1,000) * (10,000 choose 2)  \n",
    "Number of pairs processed = (1,000) * (10,000! / (2! * 9,998!))  \n",
    "Number of pairs processed = (1,000) * (10,000 * 9,9999 / 2 )  \n",
    "Number of pairs processed = (1,000) * (45,995,000)  \n",
    "\n",
    "Formula used in the above calculation.  Number combinations (order does not matter) of n things taken k at a time without repetition is n! / (k! * (n-k)!).  \n",
    "\n",
    "**Theoretical Number of pairs processed = 45,995,000,000 (about 46 billion)**   \n",
    "\n",
    "Because the inverted index is sparse, there are 3 orders of magnitude fewer pairs to process.\n",
    "\n",
    "**Formulas for Similarity**  \n",
    "The formulas for cosine and jaccard similarity used in the code below are as follows:  \n",
    "\n",
    "Cosine similarity between 2 vectors A and B:  (A dot B) / (||A|| * ||B||).\n",
    "\n",
    "Jaccard similarity between A and B:  |A and B| / (|A| + |B| - |A and B|) \n",
    "\n",
    "**STOP WORDS USED**\n",
    "'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "     'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "     'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'and', 'but', 'if', 'or', 'because', \n",
    "     'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "     'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "     'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "     'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', \n",
    "     'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "     'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \n",
    "     \"hasn't\", \"haven't\", \"isn't\", 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \n",
    "     \"weren't\", \"won't\", \"wouldn't\", \"don't\", \"can't\", 'would', 'could', 'might', 'must', 'need'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Extended Systems Test Data and Load it in to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ext_systems_test_Stripes.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_systems_test_Stripes.txt\n",
    "\"also.3\"\t{\"expected\": 20, \"years\": 30, \"size\": 5}\n",
    "\"back.2\"\t{\"expected\": 100, \"years\": 20}\n",
    "\"case.4\"\t{\"may\": 5, \"new\": 20, \"size\": 5, \"years\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat ext_systems_test_Stripes.txt\n",
    "#!hdfs dfs -copyFromLocal ext_systems_test_Stripes.txt /user/hadoop\n",
    "#!hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Stripes5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Stripes5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "    \n",
    "# This class outputs stripes for the top 10,000 most frequent words.  In each of the stripes\n",
    "# are the counts of how many times each feature (the 9,001 to 10,000 most frequent words occur)\n",
    "# occur.  Symmetry is preserved.\n",
    "class MRStripes5Gram(MRJob):\n",
    "    freq_words = set()\n",
    "    features = set()\n",
    "    \n",
    "    # Create sets for storing the frequent words and the features\n",
    "    def mapper_stripes_init(self):\n",
    "        self.word_stripe_counts = {}\n",
    "        with open('5-4words.txt', 'r') as f:\n",
    "            count = 0\n",
    "            for line in f.readlines():\n",
    "                if count < 10000:\n",
    "                    self.freq_words.add(line.strip())\n",
    "                else:\n",
    "                    self.features.add(line.strip())\n",
    "                count += 1\n",
    "    \n",
    "    def mapper_stripes(self, _, line):\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        count = int(record[1])\n",
    "        # loop through all of the words from the line and create\n",
    "        # a stripe that contains all of the co-occuring words (except itself)\n",
    "        # and their counts.\n",
    "        for w1 in words:\n",
    "            if w1 in self.freq_words:\n",
    "                H = {}\n",
    "                for w2 in words:\n",
    "                    if (w1 != w2) and (w2 in self.features):\n",
    "                        if w2 in H:\n",
    "                            H[w2] += count\n",
    "                        else:\n",
    "                            H[w2] = count\n",
    "                if H != {}:\n",
    "                    yield w1, H\n",
    "\n",
    "    # sum the values in the stripes for common words\n",
    "    def combiner_stripes(self, word, counts):\n",
    "        sum_H = Counter()\n",
    "        for H in counts:\n",
    "            sum_H += Counter(H)\n",
    "        yield word, sum_H\n",
    "\n",
    "    # sum the values in the stripes for common words\n",
    "    # also attach the length of the stripe to the key with a period\n",
    "    # this is the cardinality of that word to be used for \n",
    "    # jaccard similarity later.\n",
    "    def reducer_stripes(self, word, counts):\n",
    "        sum_H = Counter()\n",
    "        for H in counts:\n",
    "            sum_H += Counter(H)\n",
    "        yield word+'.'+str(len(sum_H)), sum_H\n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.reduces': '2'             # Specify 2 reducers\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper_init=self.mapper_stripes_init, mapper=self.mapper_stripes, \n",
    "                   combiner=self.combiner_stripes,\n",
    "                   reducer=self.reducer_stripes)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRStripes5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add required files to HDFS\n",
    "#!hdfs dfs -rm /user/hadoop/megan_test.txt\n",
    "#!hdfs dfs -copyFromLocal 'megan_test.txt' /user/hadoop\n",
    "#!hdfs dfs -rm '5-4words.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal '5-4words.txt' /user/hadoop\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5Results\n",
    "#!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Local TESTS\n",
    "#ifile = 'megan_test.txt'\n",
    "## HDFS TESTS\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data-test/'\n",
    "ifile = 'hdfs:///user/hadoop/HW5data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/megan_test.txt'\n",
    "ofile = 'outputHW5/Stripes5Gram'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python Stripes5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python Stripes5Gram.py -r hadoop $ifile --output-dir=$ofile --file='5-4words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### OUTPUT FOR HW5.4 STRIPES #####\n",
    "#!hdfs dfs -cat outputHW5/Stripes5Gram/part-00000 | head -1\n",
    "#!hdfs dfs -cat outputHW5/Stripes5Gram/part-00001 | head\n",
    "#!hdfs dfs -cat outputHW5/Stripes5Gram/part-00000 > stripes_2.txt\n",
    "#!hdfs dfs -cat outputHW5/Stripes5Gram/part-00001 >> stripes_2.txt\n",
    "#!hdfs dfs -copyFromLocal stripes_2.txt HW5Results\n",
    "#!hdfs dfs -rm megan_test_Stripes.txt\n",
    "#!hdfs dfs -cp outputHW5/Stripes5Gram/part-00000 megan_test_Stripes.txt\n",
    "#!hdfs dfs -cat megan_test_Stripes.txt\n",
    "#!hdfs dfs -copyToLocal outputHW5/Stripes5Gram/part-00000 megan_test_Stripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvIndex5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvIndex5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import ast\n",
    "from collections import Counter\n",
    "    \n",
    "# This class takes stripes as inputs, binarizes them (sets all of the counts of the words\n",
    "# in the stripe to 1), normalizes them (divides them by the square root of their length)\n",
    "# and inverts the stripe matrix.\n",
    "class MRInvIndex5Gram(MRJob):\n",
    "    word_stripe_counts = {}\n",
    "\n",
    "    # set all word counts to 1\n",
    "    def binarize_stripe(self, stripe):\n",
    "        for w in stripe:\n",
    "            stripe[w] = 1\n",
    "        return stripe\n",
    "    \n",
    "    # divide all word counts by the square root of their length\n",
    "    def normalize_stripe(self, stripe):\n",
    "        length = float(len(stripe))\n",
    "        for w in stripe:\n",
    "            stripe[w] /= pow(length, 0.5)\n",
    "        return stripe\n",
    "    \n",
    "    # invert the stripes matrix.\n",
    "    def mapper_inv_index(self, _, data):\n",
    "        word, stripe = data.split('\\t')\n",
    "        word = word.strip('\"')\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        bin_stripe = self.binarize_stripe(stripe)\n",
    "        norm_stripe = self.normalize_stripe(bin_stripe)\n",
    "        for w, value in norm_stripe.iteritems():\n",
    "            H = {}\n",
    "            H[word] = value\n",
    "            yield w, H\n",
    "\n",
    "    # sum the values in the stripes for common words\n",
    "    def reducer_inv_index(self, word, counts):\n",
    "        sum_H = Counter()\n",
    "        for H in counts:\n",
    "            sum_H += Counter(H)\n",
    "        yield word, sum_H\n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.reduces': '2'                   # Set number of reducers to 2\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper_inv_index,\n",
    "                   combiner=self.reducer_inv_index,\n",
    "                   reducer=self.reducer_inv_index)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRInvIndex5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\r\n",
      "Running step 1 of 1...\r\n",
      "Creating temp directory /tmp/InvIndex5Gram.hadoop.20160617.135658.524173\r\n",
      "Streaming final output from outputHW5/InvIndex5Gram...\r\n",
      "\"years\"\t{\"back.2\": 0.7071067811865475, \"case.4\": 0.5, \"also.3\": 0.5773502691896258}\r\n",
      "\"expected\"\t{\"back.2\": 0.7071067811865475, \"also.3\": 0.5773502691896258}\r\n",
      "\"may\"\t{\"case.4\": 0.5}\r\n",
      "\"new\"\t{\"case.4\": 0.5}\r\n",
      "\"size\"\t{\"case.4\": 0.5, \"also.3\": 0.5773502691896258}\r\n",
      "Removing temp directory /tmp/InvIndex5Gram.hadoop.20160617.135658.524173...\r\n"
     ]
    }
   ],
   "source": [
    "## Local file locations\n",
    "#ifile = 'systems_test_Stripes.txt'\n",
    "#ifile = 'megan_test_Stripes.txt'\n",
    "ifile = 'ext_systems_test_Stripes.txt'\n",
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/systems_test_Stripes.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/megan_test_Stripes.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/outputHW5/Stripes5Gram'\n",
    "ofile = 'outputHW5/InvIndex5Gram'\n",
    "#!hdfs dfs -rm /user/hadoop/outputHW5/Stripes5Gram/_SUCCESS\n",
    "#!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python InvIndex5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python InvIndex5Gram.py $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ext_systems_test_InvIndex.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ext_systems_test_InvIndex.txt\n",
    "\"years\"\t{\"back.2\": 0.7071067811865475, \"case.4\": 0.5, \"also.3\": 0.5773502691896258}\n",
    "\"expected\"\t{\"back.2\": 0.7071067811865475, \"also.3\": 0.5773502691896258}\n",
    "\"may\"\t{\"case.4\": 0.5}\n",
    "\"new\"\t{\"case.4\": 0.5}\n",
    "\"size\"\t{\"case.4\": 0.5, \"also.3\": 0.5773502691896258}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the test case in to HDFS\n",
    "#!cat ext_systems_test_InvIndex.txt\n",
    "#!hdfs dfs -copyFromLocal ext_systems_test_InvIndex.txt /user/hadoop\n",
    "#!hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### OUTPUT FOR HW5.4 INVERTED INDEX #####\n",
    "#!hdfs dfs -rm 'systems_test_Stripes.txt'\n",
    "#!hdfs dfs -rm 'megan_test_Stripes.txt'\n",
    "#!hdfs dfs -copyFromLocal 'systems_test_Stripes.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal 'megan_test_Stripes.txt' /user/hadoop\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | tail -1\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00001 | tail -1\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 > inverted_index_2.txt\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00001 >> inverted_index_2.txt\n",
    "#!hdfs dfs -copyFromLocal inverted_index_2.txt HW5Results\n",
    "#!hdfs dfs -rm outputHW5/InvIndex5Gram/_SUCCESS\n",
    "#!hdfs dfs -ls outputHW5/InvIndex5Gram\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | head -20\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | wc -l\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | tail -20\n",
    "#!hdfs dfs -cp outputHW5/InvIndex5Gram/part-00000 HW5Results/inverted_index_full.txt\n",
    "#!hdfs dfs -cp outputHW5/InvIndex5Gram/part-00000 megan_test_InvIndex.txt\n",
    "#!hdfs dfs -rm megan_test_InvIndex.txt\n",
    "#!hdfs dfs -cp outputHW5/InvIndex5Gram/part-00000 megan_test_InvIndex.txt\n",
    "#!hdfs dfs -cp megan_test_InvIndex.txt HW5Results/megan_test_InvIndex.txt\n",
    "#!hdfs dfs -copyToLocal systems_test_InvIndex.txt\n",
    "#!hdfs dfs -copyToLocal megan_test_InvIndex.txt\n",
    "#!hdfs dfs -cat systems_test_InvIndex.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "import ast\n",
    "    \n",
    "# This class calculates the cosine similarity and the jaccard similarity from\n",
    "# an inverted index\n",
    "class MRSimilarity5Gram(MRJob):\n",
    "    def mapper_similarity(self, _, data):\n",
    "        word, stripe = data.split('\\t')\n",
    "        word = word.strip('\"')\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        # If there is only 1 word in the stripe, this will be []\n",
    "        keys1 = stripe.keys()[:-1]\n",
    "        # Loop through all of the keys except the last one\n",
    "        for i, w1 in enumerate(keys1):\n",
    "            # Loop through all of the keys following w1 in the stripe\n",
    "            keys2 = stripe.keys()[i+1:]\n",
    "            for w2 in keys2:\n",
    "                # Calculate the dot product for cosine similarity\n",
    "                cosine = stripe[w1]*stripe[w2]\n",
    "                # Yield the dot product and a count of 1 for jaccard similarity\n",
    "                yield (w1,w2), (cosine,1)\n",
    "\n",
    "    # Sum the values for cosine and jaccard similarities for each key\n",
    "    def combiner_similarity(self, word, data):\n",
    "        cosine = 0\n",
    "        jaccard = 0\n",
    "        for d in data:\n",
    "            cosine += d[0]\n",
    "            jaccard += d[1]\n",
    "        yield word, (cosine,jaccard)\n",
    "    \n",
    "    # Sum the values for cosine and jaccard similarities for each key\n",
    "    # Calculate the jaccard similarity using the formula |w1 and w2| / |w1| + |w2| - |w1 and w2|\n",
    "    def reducer_similarity(self, word, data):\n",
    "        w1, c1 = word[0].split('.')\n",
    "        w2, c2 = word[1].split('.')\n",
    "        cosine = 0\n",
    "        jac_and = 0\n",
    "        for d in data:\n",
    "            cosine += d[0]\n",
    "            jac_and += d[1]\n",
    "        jaccard = jac_and / float(int(c1) + int(c2) - jac_and)\n",
    "        yield ('%s.%s.%s' % (w1,w2,'C')), cosine\n",
    "        yield ('%s.%s.%s' % (w1,w2,'J')), jaccard\n",
    "    \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {        \n",
    "            'mapreduce.job.maps': '20',\n",
    "            'mapreduce.job.reduces': '10'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,             # STEP 1:  calculate the similarity\n",
    "                   mapper=self.mapper_similarity,  \n",
    "                   combiner=self.combiner_similarity,\n",
    "                   reducer=self.reducer_similarity)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSimilarity5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Local file locations\n",
    "#ifile = 'megan_test_InvIndex.txt'\n",
    "#ifile = 'systems_test_InvIndex.txt'\n",
    "#ifile = 'ext_systems_test_InvIndex.txt'\n",
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "#ifile = 'hdfs:///user/hadoop/megan_test_InvIndex.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/systems_test_InvIndex.txt'\n",
    "ifile = 'hdfs:///user/hadoop/ext_systems_test_InvIndex.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/outputHW5/InvIndex5Gram'\n",
    "ofile = 'outputHW5/Similarity5Gram'\n",
    "#!hdfs dfs -rm /user/hadoop/outputHW5/InvIndex5Gram/_SUCCESS\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python Similarity5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python Similarity5Gram.py -r hadoop $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/17 16:46:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.4 SIMILARITY CALCULATION #####\n",
    "### Inspect and aggregate the results ###\n",
    "#!hdfs dfs -ls outputHW5/Similarity5Gram\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00009 | tail -5\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00000 > similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00001 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00002 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00003 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00004 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00005 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00006 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00007 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00008 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5Gram/part-00009 >> similarity_unsorted.txt\n",
    "#!hdfs dfs -copyFromLocal similarity_unsorted.txt HW5Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity5GramSort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity5GramSort.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "# This class sorts the similarities\n",
    "class MRSimilarity5GramSort(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    #yield coming from the previous job: ('%s.%s.%s' % (w1,w2,'J')), jaccard\n",
    "    # Read in the values from the previous job and add a label of 'a' to 'e'\n",
    "    # depending on the value stored in the input\n",
    "    def mapper_sort(self, _, data):\n",
    "        word, value = data.split('\\t')\n",
    "        word = word.strip('\"')\n",
    "        value = float(value)\n",
    "        if value < 0.2:\n",
    "            label = 'a'\n",
    "        elif value < 0.4:\n",
    "            label = 'b'\n",
    "        elif value < 0.6:\n",
    "            label = 'c'\n",
    "        elif value < 0.8:\n",
    "            label = 'd'\n",
    "        else:\n",
    "            label = 'e'\n",
    "        yield label, str(value)+'\\t'+str(word)\n",
    "    \n",
    "    # Output what is received.  The values are sorted at this point.\n",
    "    def reducer_sort(self, label, value_pair):\n",
    "        for vp in value_pair:\n",
    "            v, w = vp.split('\\t')\n",
    "            yield w, v\n",
    "    \n",
    "    # Tell the Hadoop shuffle to partition based on the label (a-e) and sort on the 2nd\n",
    "    # field in reverse order.\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.reduces': '5'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper=self.mapper_sort,\n",
    "                   reducer=self.reducer_sort)        \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSimilarity5GramSort.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Local file locations\n",
    "#ifile = 'ext_systems_test_Similarity.txt'\n",
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/megan_test_Similarity.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/ext_systems_test_Similarity.txt'\n",
    "ifile = 'hdfs:///user/hadoop/HW5Results/similarity_unsorted.txt'\n",
    "ofile = 'outputHW5/Similarity5GramSort'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python Similarity5GramSort.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python Similarity5GramSort.py -r hadoop $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### OUTPUT FOR HW5.4 SIMILARITY CALCULATION SORT #####\n",
    "#!hdfs dfs -ls outputHW5/Similarity5GramSort\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00000 | head -2\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00001 | head -2\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00002 | head -2\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00003 | head -2\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00004 | head -2\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00001 > similarity_sorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00000 >> similarity_sorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00004 >> similarity_sorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00003 >> similarity_sorted.txt\n",
    "#!hdfs dfs -cat outputHW5/Similarity5GramSort/part-00002 >> similarity_sorted.txt\n",
    "#!hdfs dfs -copyFromLocal similarity_sorted.txt HW5Results\n",
    "\n",
    "#!hdfs dfs -cat HW5Results/similarity_sorted.txt | head -2000 | grep '.C' | head -1000 > similarity_cos_top1000.txt \n",
    "#!hdfs dfs -cat HW5Results/similarity_sorted.txt | head -10 | grep '.C\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n",
    "\n",
    "**ANSWER:**  \n",
    "\n",
    "TP = true positive, FP = false positive, FN = false negative  \n",
    "Precision (P) = TP / (TP + FP), uses false positives  \n",
    "Recall (R) = TP / (TP + FN), uses false negatives  \n",
    "F1 = 2 x (precision x recall) / (precision + recall)  \n",
    "\n",
    "**Hits:** 15/1,000  \n",
    "**Precision:** 0.015000  \n",
    "**Recall:** 1.000000  \n",
    "**F1 Score:** 0.029557\n",
    "\n",
    "TP = the number of hits found by nltk in the top 1,000\n",
    "FP = the number of non-hits found by nltk in the top 1,000\n",
    "FN = 0, there are no false negatives found by nltk in the top 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make made\n",
      "means way\n",
      "well good\n",
      "take made\n",
      "men man\n",
      "work made\n",
      "place order\n",
      "world man\n",
      "make form\n",
      "found see\n",
      "men world\n",
      "state states\n",
      "still yet\n",
      "best well\n",
      "form made\n",
      "Hits: 15/1000, Precision: 0.015000, Recall: 1.000000, F1 Score: 0.029557\n"
     ]
    }
   ],
   "source": [
    "import nltk_synonyms as nltk\n",
    "\n",
    "# Calculate how many synonyms are found in the input file using the nltk library.\n",
    "filename = 'similarity_cos_top1000.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    "    for line in f.readlines():\n",
    "        w1,w2,_ = line.strip().split('\\t')[0].strip('\"').split('.')\n",
    "        # If a one of the pair of words is in the synonym list of the other, then\n",
    "        # mark it a 'hit' and increase the total_correct\n",
    "        if (w1 in nltk.synonyms(w2)) or (w2 in nltk.synonyms(w1)):\n",
    "            total_correct += 1\n",
    "            print w1, w2\n",
    "        total_count += 1\n",
    "    # Calculate precision, recall and F1 score according to the formulas above\n",
    "    TP = total_correct\n",
    "    FP = total_count - total_correct # total incorrect\n",
    "    FN = 0\n",
    "    precision = TP / float(TP + FP)\n",
    "    recall = TP / float(TP + FN)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print\n",
    "    print('Hits: %d/%d, Precision: %f, Recall: %f, F1 Score: %f' % (total_correct, total_count, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Google N-Gram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_5gram_data.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_5gram_data.sh\n",
    "#!/bin/bash\n",
    "\n",
    "for index in {0..189}\n",
    "do\n",
    "  wget http://filtered-5grams.s3.amazonaws.com/googlebooks-eng-all-5gram-20090715-${index}-filtered.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x get_5gram_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!./get_5gram_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the 5-gram data in to HDFS\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5data\n",
    "#!hdfs dfs -rm /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-65-filtered.txt\n",
    "for i in range(65,190):\n",
    "    filename = 'data/googlebooks-eng-all-5gram-20090715-' + str(i) + '-filtered.txt'\n",
    "    !hdfs dfs -copyFromLocal $filename /user/hadoop/HW5data\n",
    "# List the files in the HW5data directory to ensure that they are all in there\n",
    "!hdfs dfs -ls /user/hadoop/HW5data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file of Words and Values for 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "### STOP WORDS USED ###\n",
    "w = ['the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "     'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "     'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'and', 'but', 'if', 'or', 'because', \n",
    "     'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "     'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "     'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "     'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', \n",
    "     'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "     'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \n",
    "     \"hasn't\", \"haven't\", \"isn't\", 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \n",
    "     \"weren't\", \"won't\", \"wouldn't\", \"don't\", \"can't\", 'would', 'could', 'might', 'must', 'need']\n",
    "\n",
    "def remove_stop_words(f1, f2, w, n):\n",
    "    with open(f1, 'r') as ifile, open(f2, 'w+') as ofile:\n",
    "        i = 0\n",
    "        count = 0\n",
    "        word_count = 0\n",
    "        for line in ifile.readlines():\n",
    "            if word_count < n:\n",
    "                word = line.split()[0].strip('\"')\n",
    "                if word in w:\n",
    "                    #print i, word\n",
    "                    count += 1\n",
    "                else:\n",
    "                    ofile.write(word+'\\n')\n",
    "                    word_count += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        print(count)\n",
    "\n",
    "remove_stop_words('5-4words_full.txt', '5-4words.txt', w, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
