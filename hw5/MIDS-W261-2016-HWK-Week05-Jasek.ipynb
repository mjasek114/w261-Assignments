{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  5\n",
    "* **Date:**  6/17/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "\n",
    "## HW 5.1\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "**ANSWER:** There are 2 tables in this problem.  The first one is labeled 'PageID-URL' and it contains Page ID's and the URL's that the id's are associated with.  It contains about 590 records.  The second one is labeled 'PageID-VisitorID' and it contains Page ID's associated with Visitor ID's.  It contains about 130,000 records.  The smaller table (PageID-URL) was chosen to be on the left because when doing the left join there needs to be some way to mark which elements in the PageID-URL are not contained in the PageID-VisitorID table.  Doing this marking will take additional time and space that is proportional to the size of the left table.  In order to minimize time and space, the smaller table was chosen to be on the left.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "| Join Type | # of Rows |\n",
    "| - | - |\n",
    "| Left | 98,663 |\n",
    "| Right | 98,654 |\n",
    "| Inner | 98,654 |  \n",
    "\n",
    "Since the number of rows for the inner and right joins are the same, this means that there are not any entries in the PageID-VisitorID table that did not have PageID's in the PageID-URL table.\n",
    "\n",
    "Since the number of rows for the left join is greater than the inner join, this means that there were 9 (98,663 - 98,654) rows in the PageID-URL table that did not have PageID's in the PageID-VisitorID table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for hashside join** from Data-Intensive Text Processing with MapReduce by Jimmy Lin and Chris Dyer, section 3.5.3, page 67:  \n",
    "1. Load the smaller dataset into memory in every mapper, populating an associative array to facilitate random access to tuples based on the join key. The mapper initialization API hook (see Section 3.1.1) can be used for this purpose.\n",
    "2. Mappers are then applied to the other (larger) dataset, and for each input key-value pair, the mapper probes the in-memory dataset to see if there is a tuple with the same join key.\n",
    "3. If there is, the join is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (1) Left joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinLeft.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinLeft.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs left join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the left table, with the matching rows (matching Page ID's)\n",
    "# in the right table. The output is NULL or blank on the right side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinLeft(MRJob):\n",
    "    # Initialize a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    # Initialize a dictionary to keep track of the vroots (page_id's) that did not exist\n",
    "    # in the right table.\n",
    "    vroots_not_used = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot.  For each vroot\n",
    "        # in the file, store an empty entry for the vroot in the self.vroots_not_used dictionary\n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "                    self.vroots_not_used[page_id] = ''\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.  Delete the page_id from the self.vroots_not_used dictionary\n",
    "        # to indicate that this page_id has been output already.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                if page_id in self.vroots_not_used:\n",
    "                    del self.vroots_not_used[page_id]\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "\n",
    "    def mapper_memjoin_final(self):\n",
    "        # To complete the left join, for any vroots still left in the self.vroots_not_used\n",
    "        # dictionary, print them out with a None for the right side.\n",
    "        for page_id in self.vroots_not_used:\n",
    "            yield 'URL: ' + self.vroots[page_id], None\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP = {        \n",
    "            'mapreduce.job.maps': '1'\n",
    "        }\n",
    "        return[\n",
    "            MRStep(jobconf=JOBCONF_STEP,\n",
    "                   mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin,\n",
    "                   mapper_final=self.mapper_memjoin_final)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinLeft.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98663 output/leftjoin/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "# Test this job with a smaller dataset\n",
    "#!python MemJoinLeft.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run the job on the full dataset\n",
    "!python MemJoinLeft.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/leftjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Left Join ######\n",
    "!wc -l output/leftjoin/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98663 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinLeft\n",
    "reload(MemJoinLeft)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinLeft.MRMemJoinLeft(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            if value == None:\n",
    "                print key\n",
    "            else:\n",
    "                print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (2) Right joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinRight.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinRight.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# This class performs right join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will output all rows from the right table, with the matching rows (matching Page ID's)\n",
    "# in the left table. The output is NULL or blank on the left side when there is no match.\n",
    "# The output is of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinRight(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file examine each line that starts with a 'V'.\n",
    "        # Yield each of these lines.  If there is no url for one of the lines in the \n",
    "        # file, then output it with a url of 'None'.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            url = 'None'\n",
    "            if page_id in self.vroots:\n",
    "                url = self.vroots[page_id]\n",
    "            yield 'URL: ' + url, page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinRight.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49062 output/rightjoin/part-00000\n",
      "49592 output/rightjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinRight.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinRight.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/rightjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Right Join ######\n",
    "!wc -l output/rightjoin/part-00000\n",
    "!wc -l output/rightjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinRight\n",
    "reload(MemJoinRight)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinRight.MRMemJoinRight(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.2 (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MemJoinInner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MemJoinInner.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "# This class performs an inner join on the 2 datasets in the input file.\n",
    "# The left table is Page ID's and URL's.  The right table is Page_ID's and Visitor ID's.\n",
    "# It will only output rows where the Page ID exists in both tables.  The output is \n",
    "# of the form URL, Page ID, Visitor ID\n",
    "class MRMemJoinInner(MRJob):\n",
    "    # Create a dictionary to store the smaller dataset that will be held in memory.\n",
    "    vroots = {}\n",
    "    \n",
    "    def mapper_memjoin_init(self):\n",
    "        # Read the data from the filename and store it in the self.vroots dictionary.  This\n",
    "        # stores the base URL and the vroot labels for each vroot. \n",
    "        filename = 'anonymous-msweb_converted.data'\n",
    "        with open(filename, 'r') as f:\n",
    "            base_url = \"\"\n",
    "            for line in f.readlines():\n",
    "                record = line.strip().split(',')\n",
    "                if record[0] == 'I':\n",
    "                    base_url = record[2].strip('\"')\n",
    "                elif record[0] == 'A':\n",
    "                    page_id = record[1]\n",
    "                    vroot = record[4].strip('\"')\n",
    "                    self.vroots[page_id] = base_url + vroot\n",
    "\n",
    "    def mapper_memjoin(self, _, line):\n",
    "        # read the next line from the file and only if it is a visitor record, denoted by\n",
    "        # 'V', and only if the page_id is in the vroots dictionary, output the URL, Page ID\n",
    "        # and Visitor ID.\n",
    "        record = line.strip().split(',')\n",
    "        if record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('Page ID: %s, Visitor ID: %s' % (page_id, visitor_id))\n",
    "            if page_id in self.vroots:\n",
    "                yield 'URL: ' + self.vroots[page_id], page_visitor_pair\n",
    "    \n",
    "    # Create the steps for this job.  No reducer is required.\n",
    "    def steps(self):\n",
    "        return[\n",
    "            MRStep(mapper_init=self.mapper_memjoin_init, \n",
    "                   mapper=self.mapper_memjoin)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMemJoinInner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823\n",
      "Removing temp directory /tmp/MemJoinInner.hadoop.20160611.051202.927823...\n",
      "49062 output/innerjoin/part-00000\n",
      "49592 output/innerjoin/part-00001\n",
      "98654\n"
     ]
    }
   ],
   "source": [
    "# Test this job on a smaller dataset\n",
    "#!python MemJoinInner.py anonymous-msweb_converted_small.data --file=anonymous-msweb_converted_small.data\n",
    "# Run this job on the full dataset\n",
    "!python MemJoinInner.py anonymous-msweb_converted.data --file=anonymous-msweb_converted.data --output-dir=output/innerjoin --no-output\n",
    "########### HW4.2 OUTPUT:  Number of Rows from Inner Join ######\n",
    "!wc -l output/innerjoin/part-00000\n",
    "!wc -l output/innerjoin/part-00001\n",
    "print(49062+49592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records\n"
     ]
    }
   ],
   "source": [
    "import MemJoinInner\n",
    "reload(MemJoinInner)\n",
    "\n",
    "TEST_FILE = False\n",
    "#mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted_small.data','--file=anonymous-msweb_converted_small.data'])\n",
    "mr_job = MemJoinInner.MRMemJoinInner(args=['anonymous-msweb_converted.data','--file=anonymous-msweb_converted.data'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        if TEST_FILE:\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key + ', ' + value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)  \n",
    "**ANSWER:**  There were two 5-grams with the longest length of 155 characters.  Spaces and apostrophes were not counted.\n",
    "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t155\n",
    "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"\t155\n",
    "\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams  \n",
    "**ANSWER:**  \n",
    "\"the\"\t5,490,815,394  \n",
    "\"of\"\t3,698,583,299  \n",
    "\"to\"\t2,227,866,570  \n",
    "\"in\"\t1,421,312,776  \n",
    "\"a\"\t1,361,123,022  \n",
    "\"and\"\t1,149,577,477  \n",
    "\"that\"\t802,921,147  \n",
    "\"is\"\t758,328,796  \n",
    "\"be\"\t688,707,130  \n",
    "\"as\"\t492,170,314  \n",
    "\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency  \n",
    "**ANSWER:**  \n",
    "**20 MOST DENSE WORDS**  \n",
    "\"xxxx\"\t11.557291666666666  \n",
    "\"blah\"\t8.074159907300116  \n",
    "\"nnn\"\t7.533333333333333  \n",
    "\"na\"\t6.201749131424464  \n",
    "\"oooooooooooooooo\"\t4.921875  \n",
    "\"nd\"\t4.85430572723527  \n",
    "\"llll\"\t4.511627906976744  \n",
    "\"oooooo\"\t4.169650013358269  \n",
    "\"ooooo\"\t3.858637193467213  \n",
    "\"lillelu\"\t3.7624521072796937  \n",
    "\"madarassy\"\t3.576923076923077  \n",
    "\"pfeffermann\"\t3.576923076923077  \n",
    "\"meteoritical\"\t3.56  \n",
    "\"xxxxxxxx\"\t3.5  \n",
    "\"beep\"\t3.229038854805726  \n",
    "\"latha\"\t3.188679245283019  \n",
    "\"iyengar\"\t2.9191176470588234  \n",
    "\"counterfeiteth\"\t2.825  \n",
    "\"nonmorular\"\t2.81981981981982  \n",
    "\"nonsquamous\"\t2.81981981981982  \n",
    "**20 LEAST DENSE WORDS**  \n",
    "\"zwingst\"\t1.0  \n",
    "\"zwirnen\"\t1.0  \n",
    "\"zwischenstaatlicher\"\t1.0  \n",
    "\"zwitterionic\"\t1.0  \n",
    "\"zwt\"\t1.0  \n",
    "\"zwyn\"\t1.0  \n",
    "\"zx\"\t1.0  \n",
    "\"zxcvframeqasfuc\"\t1.0  \n",
    "\"zydeco\"\t1.0  \n",
    "\"zydom\"\t1.0  \n",
    "\"zygmunt\"\t1.0  \n",
    "\"zygomaticofacial\"\t1.0  \n",
    "\"zygomaticotemporal\"\t1.0  \n",
    "\"zygosity\"\t1.0  \n",
    "\"zylindrischen\"\t1.0  \n",
    "\"zymelman\"\t1.0  \n",
    "\"zymogens\"\t1.0  \n",
    "\"zymophore\"\t1.0  \n",
    "\"zymosan\"\t1.0  \n",
    "\"zymosis\"\t1.0  \n",
    "\n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.  \n",
    "**ANSWER:**  See histogram plotted below in section labeled: 'OUTPUT FOR:  Distribution of 5-gram sizes (character length)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALongest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALongest5gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDALongest5gram(MRJob):\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield record[0], len(ngram)\n",
    "    \n",
    "    def reducer_sum_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield ngram, sum(counts)\n",
    "    \n",
    "    def reducer_sort_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield ngram, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the characters) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_chars,   # STEP 1:  count the characters\n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    reducer=self.reducer_sort_chars)  # STEP 2:  sort the characters\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALongest5gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDALongest5gram.hadoop.20160612.002359.463775\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/EDALongest5gram.hadoop.20160612.002359.463775/output...\n",
      "\"A BILL FOR ESTABLISHING RELIGIOUS\"\t29\n",
      "\"A Biography of General George\"\t25\n",
      "\"A Case Study in Government\"\t22\n",
      "\"A Case Study of Female\"\t18\n",
      "\"A Case Study of Limited\"\t19\n",
      "\"A Child's Christmas in Wales\"\t23\n",
      "\"A Circumstantial Narrative of the\"\t29\n",
      "\"A City by the Sea\"\t13\n",
      "\"A Collection of Fairy Tales\"\t23\n",
      "\"A Collection of Forms of\"\t20\n",
      "Removing temp directory /tmp/EDALongest5gram.hadoop.20160612.002359.463775...\n"
     ]
    }
   ],
   "source": [
    "# Test the program on the small dataset\n",
    "!python EDALongest5gram.py 5gram_small.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 12:57:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/11 12:57:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### Upload test files to HDFS for testing\n",
    "#!hdfs dfs -copyFromLocal '5gram_small.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_half.txt' /user/hadoop\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5data-test\n",
    "#!hdfs dfs -rm /user/hadoop/HW5data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt' /user/hadoop/HW5data-test\n",
    "#!hdfs dfs -copyFromLocal 'data-test/googlebooks-eng-all-5gram-20090715-0-filtered_tail.txt' /user/hadoop/HW5data-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 17:40:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/hadoop/outputHW5/EDALongest5gram': No such file or directory\n",
      "nohup: ignoring input and appending output to ‘nohup.out’\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDALongest5gram'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDALongest5gram.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 19:06:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t155\n",
      "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"\t155\n",
      "\"RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR JDDUMHUYPARICOLEVTYPS ILONBUNURT\"\t124\n",
      "\"RTYEARTHQUAKECVBNMKDSAW VBNEVWVOLCANICERUPTIONS FLOODSCVBEAVALANCHESUYT VBNHURRICANESTORNADOESX TIDALWAVECVBNCYCLONECVE\"\t115\n",
      "\"SHIPWRECKIERTGBVCXWQXCE CCCVBNWSESWDFGFIRESNPLM WQRAILROADWRECKSUTRFHJK EXPLOSIONSTRPIQURECEDFG RTYEARTHQUAKECVBNMKDSAW\"\t115\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.3 LONGEST 5-GRAM #####\n",
    "!hdfs dfs -cat outputHW5/EDALongest5gram/part-00000 | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 16:38:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS 29\n",
      "A Circumstantial Narrative of the 29\n"
     ]
    }
   ],
   "source": [
    "import EDALongest5gram\n",
    "reload(EDALongest5gram)\n",
    "\n",
    "mr_job = EDALongest5gram.MREDALongest5gram(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    cur_value = \"\"\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count == 0 or value == cur_value:\n",
    "            print key, value\n",
    "            cur_value = value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Top 10 most frequent words (please use the count information), i.e., unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAMostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAMostFrequentWords.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDAMostFrequentWords(MRJob):\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, int(record[1])\n",
    "    \n",
    "    def reducer_sum_chars(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def reducer_sort_chars(self, word, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the words) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_chars,   # STEP 1:  count the characters\n",
    "                   combiner=self.reducer_sum_chars,\n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    reducer=self.reducer_sort_chars)  # STEP 2:  sort the characters\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAMostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/10 18:42:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 2217\n",
      "in 1201\n",
      "child's 1099\n",
      "christmas 1099\n",
      "wales 1099\n",
      "of 1011\n",
      "case 604\n",
      "study 604\n",
      "female 447\n",
      "collection 239\n"
     ]
    }
   ],
   "source": [
    "import EDAMostFrequentWords\n",
    "reload(EDAMostFrequentWords)\n",
    "\n",
    "mr_job = EDAMostFrequentWords.MREDAMostFrequentWords(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Only output the longest ngram.  If there are multiple ngrams with the longest \n",
    "    # number of characters, then output all of them, sorted alphabetically\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        if count < 10:\n",
    "            print key, value\n",
    "        else:\n",
    "            break\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDAMostFrequent'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDAMostFrequentWords.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 20:11:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"the\"\t5490815394\n",
      "\"of\"\t3698583299\n",
      "\"to\"\t2227866570\n",
      "\"in\"\t1421312776\n",
      "\"a\"\t1361123022\n",
      "\"and\"\t1149577477\n",
      "\"that\"\t802921147\n",
      "\"is\"\t758328796\n",
      "\"be\"\t688707130\n",
      "\"as\"\t492170314\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.3 MOST FREQUENT WORD #####\n",
    "!hdfs dfs -cat outputHW5/EDAMostFrequent/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDAWordDensity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDAWordDensity.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MREDAWordDensity(MRJob):\n",
    "    def mapper_count(self, _, line):\n",
    "        # read the next line from the file and output each word of the 5-gram and\n",
    "        # its count and page_count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        for word in words:\n",
    "            yield word, (int(record[1]), int(record[2]))\n",
    "    \n",
    "    def reducer_sum(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the \n",
    "        # word_counts and page_counts\n",
    "        word_total = 0\n",
    "        page_total = 0\n",
    "        for word_count, pages_count in counts:\n",
    "            word_total += word_count\n",
    "            page_total += pages_count\n",
    "        yield word, (word_total, page_total)\n",
    "    \n",
    "    def mapper_density(self, word, counts):\n",
    "        # output the word plus the word density (word_total / page_total)\n",
    "        word_total = float(counts[0])\n",
    "        page_total = float(counts[1])\n",
    "        yield word, word_total / page_total\n",
    "\n",
    "    def reducer_sort(self, word, counts):\n",
    "        # output each word (key) that is input to the reducer plus the sum of the counts\n",
    "        # for that page.  (NOTE:  There should only be one count value for each ngram.)\n",
    "        yield word, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the words) and then if there are ties,\n",
    "        # the data should be sorted by first output or the key.\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count,   # STEP 1:  count the words and pages\n",
    "                   combiner=self.reducer_sum,\n",
    "                   reducer=self.reducer_sum),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,      # STEP 2:  compute and sort the densities\n",
    "                   mapper=self.mapper_density,\n",
    "                   reducer=self.reducer_sort)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDAWordDensity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDAWordDensity.hadoop.20160612.012009.966284\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/EDAWordDensity.hadoop.20160612.012009.966284/output...\n",
      "\"a\"\t1.0282931354359925\n",
      "\"bill\"\t1.0\n",
      "\"biography\"\t1.0222222222222221\n",
      "\"by\"\t1.0333333333333334\n",
      "\"case\"\t1.0\n",
      "\"child's\"\t1.0358152686145146\n",
      "\"christmas\"\t1.0358152686145146\n",
      "\"circumstantial\"\t1.0\n",
      "\"city\"\t1.0333333333333334\n",
      "\"collection\"\t1.0863636363636364\n",
      "Removing temp directory /tmp/EDAWordDensity.hadoop.20160612.012009.966284...\n"
     ]
    }
   ],
   "source": [
    "!python EDAWordDensity.py 5gram_small.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/11 20:21:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import EDAWordDensity\n",
    "reload(EDAWordDensity)\n",
    "\n",
    "#mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt'])\n",
    "mr_job = EDAWordDensity.MREDAWordDensity(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    # Output the words and their densities in descending order by density\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        #print key, value\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDAWordDensity'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDAWordDensity.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### OUTPUT FOR HW5.3 MOST/LEAST WORD DENSITY #####\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00000 | head -20\n",
    "#!hdfs dfs -cat outputHW5/EDAWordDensity/part-00000 | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3 EDA:  Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALengthCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALengthCount.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the unique lengths of the 5-grams and the frequency of those lengths\n",
    "class MREDALengthCount(MRJob):\n",
    "    def mapper_count_lengths(self, _, line):\n",
    "        # read the next line from the file and output the length of the 5-gram and the count\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield len(ngram), int(record[1])\n",
    "    \n",
    "    def reducer_sum_lengths(self, length, counts):\n",
    "        # output each length (key) that is input to the reducer plus the sum of the counts.\n",
    "        yield length, sum(counts)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_lengths,   # STEP 1:  count the lengths\n",
    "                   combiner=self.reducer_sum_lengths,\n",
    "                   reducer=self.reducer_sum_lengths)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALengthCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f45145068d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4534272890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import EDALengthCount\n",
    "reload(EDALengthCount)\n",
    "\n",
    "mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt'])\n",
    "#mr_job = EDALengthCount.MREDALengthCount(args=['5gram_small.txt', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # Initialize a dictionary to store the lengths and frequencies\n",
    "    len_freq = {}\n",
    "    # stream_output: get access of the output \n",
    "    # the key is the length of a 5-gram and the value is number of times that\n",
    "    # length occurs.\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        #print key, value\n",
    "        len_freq[int(key)] = int(value)\n",
    "\n",
    "# Plot a histogram of the lengths and their frequencies\n",
    "df = pandas.DataFrame.from_dict(len_freq, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## HDFS file locations\n",
    "ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "ofile = 'outputHW5/EDALengthCount'\n",
    "!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "!nohup python EDALengthCount.py -r hadoop $ifile --output-dir=$ofile --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/11 20:32:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "80\n",
      "16/06/11 20:32:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "19\t922216080\n",
      "18\t904035885\n",
      "20\t894627915\n",
      "21\t827102119\n",
      "17\t826323470\n",
      "22\t739276973\n",
      "16\t693497694\n",
      "23\t636531045\n",
      "24\t529780375\n",
      "15\t519867524\n",
      "16/06/11 20:32:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "78\t95\n",
      "124\t92\n",
      "85\t92\n",
      "82\t91\n",
      "99\t91\n",
      "102\t90\n",
      "71\t87\n",
      "86\t84\n",
      "75\t83\n",
      "79\t51\n",
      "16/06/11 20:32:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/11 20:32:57 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.3 5-GRAM LENGTH DISTRIBUTIONS #####\n",
    "!hdfs dfs -cat outputHW5/EDALengthCount/part-00000 | wc -l\n",
    "!hdfs dfs -cat outputHW5/EDALengthCount/part-00000 | sort -k2nr | head -10\n",
    "!hdfs dfs -cat outputHW5/EDALengthCount/part-00000 | sort -k2nr | tail -10\n",
    "#!hdfs dfs -copyToLocal outputHW5/EDALengthCount/part-00000 EDALengthCount_results.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 14140, 6: 5740, 7: 31532, 8: 114018, 9: 888942, 10: 5952482, 11: 23274454, 12: 70405762, 13: 171917855, 14: 327544039, 15: 519867524, 16: 693497694, 17: 826323470, 18: 904035885, 19: 922216080, 20: 894627915, 21: 827102119, 22: 739276973, 23: 636531045, 24: 529780375, 25: 433432935, 26: 345313911, 27: 269150679, 28: 205155710, 29: 155329669, 30: 115113287, 31: 84060583, 32: 59886887, 33: 43837657, 34: 30942047, 155: 182, 36: 15491605, 37: 10876066, 38: 7639861, 39: 5113864, 40: 3551016, 41: 2563933, 42: 1590108, 43: 1168388, 44: 792178, 45: 547287, 46: 369236, 47: 233832, 48: 154624, 49: 91827, 50: 64095, 51: 58099, 52: 23292, 53: 22267, 54: 14716, 55: 8536, 56: 4489, 57: 4811, 58: 2544, 59: 2094, 60: 1449, 61: 1263, 62: 460, 63: 365, 64: 211, 65: 230, 66: 375, 67: 390, 68: 236, 69: 182, 71: 87, 72: 157, 75: 83, 78: 95, 79: 51, 80: 421, 82: 91, 35: 22081460, 85: 92, 86: 84, 87: 155, 99: 91, 102: 90, 115: 148, 124: 92}\n",
      "Distribution of 5-gram sizes (character length)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd4eebb7810>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEPCAYAAABShj9RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxFJhAK6iEgiFNBFRBLx/wHhK33nUFhS1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd4eebb7190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### OUTPUT FOR:  Distribution of 5-gram sizes (character length) #####\n",
    "%matplotlib inline\n",
    "import pandas\n",
    "\n",
    "# Initialize a dictionary to store the lengths and frequencies\n",
    "len_freq = {}\n",
    "filename = 'EDALengthCount_results.text'\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        key,value =  line.strip().split('\\t',2)\n",
    "        len_freq[int(key)] = int(value)\n",
    "\n",
    "print(len_freq)\n",
    "print('Distribution of 5-gram sizes (character length)')\n",
    "# Plot a histogram of the lengths and their frequencies\n",
    "df = pandas.DataFrame.from_dict(len_freq, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using cooccurence informationa based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 30, 'X': 20, 'Z': 5} {'Y': 20, 'X': 100} {'Z': 5, 'M': 5, 'N': 20}\n"
     ]
    }
   ],
   "source": [
    "# Load SYSTEMS TEST DATASET\n",
    "docA = {'X':20, 'Y':30, 'Z':5}\n",
    "docB = {'X':100, 'Y':20}\n",
    "docC = {'M':5, 'N':20, 'Z':5}\n",
    "print(docA, docB, docC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing systems_test_data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile systems_test_data.txt\n",
    "DocA X:20 Y:30 Z:5\n",
    "DocB X:100 Y:20\n",
    "DocC M:5 N:20 Z:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Stripes5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Stripes5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MRStripes5Gram(MRJob):\n",
    "    freq_words = set()\n",
    "    features = set()\n",
    "    \n",
    "    def mapper_stripes_init(self):\n",
    "        self.word_stripe_counts = {}\n",
    "        with open('5-4words.txt', 'r') as f:\n",
    "            count = 0\n",
    "            for line in f.readlines():\n",
    "                if count < 10000:\n",
    "                    self.freq_words.add(line.strip())\n",
    "                else:\n",
    "                    self.features.add(line.strip())\n",
    "                count += 1\n",
    "    \n",
    "    def mapper_stripes(self, _, line):\n",
    "        # read the next line from the file and output each word of the 5-gram and\n",
    "        # its count and page_count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        count = int(record[1])\n",
    "        for w1 in words:\n",
    "            if w1 in self.freq_words:\n",
    "                H = {}\n",
    "                for w2 in words:\n",
    "                    if (w1 != w2) and (w2 in self.features):\n",
    "                        if w2 in H:\n",
    "                            H[w2] += count\n",
    "                        else:\n",
    "                            H[w2] = count\n",
    "                if H != {}:\n",
    "                    yield w1, H\n",
    "\n",
    "    def reducer_stripes(self, word, counts):\n",
    "        sum_H = Counter()\n",
    "        for H in counts:\n",
    "            sum_H += Counter(H)\n",
    "        yield word, sum_H\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_stripes_init, mapper=self.mapper_stripes, \n",
    "                   combiner=self.reducer_stripes,\n",
    "                   reducer=self.reducer_stripes)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRStripes5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -rm 'megan_test.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal 'megan_test.txt' /user/hadoop\n",
    "#!hdfs dfs -copyFromLocal '5-4words.txt' /user/hadoop\n",
    "#!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/Stripes5Gram.hadoop.20160613.052132.582697\n",
      "Traceback (most recent call last):\n",
      "  File \"Stripes5Gram.py\", line 54, in <module>\n",
      "    MRStripes5Gram.run()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 430, in run\n",
      "    mr_job.execute()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 448, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 160, in execute\n",
      "    self.run_job()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 230, in run_job\n",
      "    runner.run()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/runner.py\", line 473, in run\n",
      "    self._run()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/sim.py\", line 172, in _run\n",
      "    self._invoke_step(step_num, 'mapper')\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/sim.py\", line 259, in _invoke_step\n",
      "    working_dir, env)\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/inline.py\", line 157, in _run_step\n",
      "    child_instance.execute()\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 439, in execute\n",
      "    self.run_mapper(self.options.step_num)\n",
      "  File \"/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 504, in run_mapper\n",
      "    for out_key, out_value in mapper(key, value) or ():\n",
      "  File \"Stripes5Gram.py\", line 27, in mapper_stripes\n",
      "    count = int(record[1])\n",
      "ValueError: invalid literal for int() with base 10: '{\"expected\": 20, \"years\": 30, \"size\": 5}'\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data-test/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "#ifile = 'megan_test.txt'\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data-test/googlebooks-eng-all-5gram-20090715-0-filtered_head.txt'\n",
    "ifile = 'systems_test_data.txt'\n",
    "ofile = 'outputHW5/Stripes5Gram'\n",
    "#!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python Stripes5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python Stripes5Gram.py $ifile --output-dir=$ofile --file='5-4words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/13 01:00:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.4 STRIPES #####\n",
    "#!hdfs dfs -rm outputHW5/Stripes5Gram/_SUCCESS\n",
    "#!hdfs dfs -ls outputHW5/Stripes5Gram\n",
    "!hdfs dfs -cat outputHW5/Stripes5Gram/part-00000 | grep -c '{'\n",
    "#!hdfs dfs -cat outputHW5/Stripes5Gram/part-00000 | tail -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvIndex5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvIndex5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MRInvIndex5Gram(MRJob):\n",
    "    word_stripe_counts = {}\n",
    "\n",
    "    def reducer_stripes(self, word, counts):\n",
    "        sum_H = Counter()\n",
    "        for H in counts:\n",
    "            sum_H += Counter(H)\n",
    "        #    yield word, H\n",
    "        yield word, sum_H\n",
    "    \n",
    "    def binarize_stripe(self, stripe):\n",
    "        for w in stripe:\n",
    "            stripe[w] = 1\n",
    "        return stripe\n",
    "    \n",
    "    def normalize_stripe(self, stripe):\n",
    "        length = float(len(stripe))\n",
    "        for w in stripe:\n",
    "            stripe[w] /= pow(length, 0.5)\n",
    "        return stripe\n",
    "    \n",
    "    def mapper_inv_index(self, _, data):\n",
    "        word, stripe = data.split('\\t')\n",
    "        word = word.strip('\"')\n",
    "        stripe = eval(stripe)\n",
    "        bin_stripe = self.binarize_stripe(stripe)\n",
    "        norm_stripe = self.normalize_stripe(bin_stripe)\n",
    "        for w, value in norm_stripe.iteritems():\n",
    "            H = {}\n",
    "            H[word] = value\n",
    "            yield w, H\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_inv_index,\n",
    "                   #combiner=self.reducer_stripes,\n",
    "                   reducer=self.reducer_stripes)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRInvIndex5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/InvIndex5Gram.hadoop.20160613.053440.348283\n",
      "Streaming final output from outputHW5/InvIndex5Gram...\n",
      "\"size\"\t{\"case\": 0.5773502691896258, \"also\": 0.5773502691896258}\n",
      "\"years\"\t{\"also\": 0.5773502691896258, \"back\": 0.7071067811865475}\n",
      "\"expected\"\t{\"also\": 0.5773502691896258, \"back\": 0.7071067811865475}\n",
      "\"may\"\t{\"case\": 0.5773502691896258}\n",
      "\"new\"\t{\"case\": 0.5773502691896258}\n",
      "Removing temp directory /tmp/InvIndex5Gram.hadoop.20160613.053440.348283...\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "ifile = 'systems_test_data.txt'\n",
    "ofile = 'outputHW5/InvIndex5Gram'\n",
    "#!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python InvIndex5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python InvIndex5Gram.py $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/13 00:33:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `outputHW5/InvIndex5Gram/_SUCCESS': No such file or directory\n",
      "16/06/13 00:33:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ls: `outputHW5/InvIndex5Gram': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.4 INVERTED INDEX #####\n",
    "!hdfs dfs -rm outputHW5/InvIndex5Gram/_SUCCESS\n",
    "!hdfs dfs -ls outputHW5/InvIndex5Gram\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | head -20\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | tail -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity5Gram.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MRSimilarity5Gram(MRJob):\n",
    "    word_stripe_counts = {}\n",
    "    \n",
    "    def mapper_similarity(self, _, data):\n",
    "        word, stripe = data.split('\\t')\n",
    "        word = word.strip('\"')\n",
    "        stripe = eval(stripe)\n",
    "        keys1 = stripe.keys()[:-1]\n",
    "        last_word = stripe.keys()[-1]\n",
    "        yield '*' + last_word, 1\n",
    "        for i, w1 in enumerate(keys1):\n",
    "            keys2 = stripe.keys()[i+1:]\n",
    "            for w2 in keys2:\n",
    "                cosine = stripe[w1]*stripe[w2]\n",
    "                yield (w1,w2), (cosine,1)\n",
    "            yield '*' + w1, 1\n",
    "\n",
    "    def reducer_similarity(self, word, data):\n",
    "        if word[0] == '*':\n",
    "            self.word_stripe_counts[word[1:]] = sum(data)\n",
    "        else:\n",
    "            w1 = word[0]\n",
    "            w2 = word[1]\n",
    "            cosine = 0\n",
    "            jac_union = 0\n",
    "            for d in data:\n",
    "                cos = d[0]\n",
    "                jac = d[1]\n",
    "                cosine += cos\n",
    "                jac_union += jac\n",
    "            jaccard = jac_union / float(self.word_stripe_counts[w1] + self.word_stripe_counts[w2] - jac_union)\n",
    "            yield (w1,w2), (cosine,jaccard)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_similarity,\n",
    "                   reducer=self.reducer_similarity)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSimilarity5Gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\r\n",
      "Running step 1 of 1...\r\n",
      "Creating temp directory /tmp/Similarity5Gram.hadoop.20160613.033329.757765\r\n",
      "Streaming final output from outputHW5/Similarity5Gram...\r\n",
      "[\"case\", \"also\"]\t[0.9999999999999998, 1.0]\r\n",
      "[\"case\", \"back\"]\t[0.408248290463863, 0.25]\r\n",
      "[\"case\", \"day\"]\t[0.408248290463863, 0.25]\r\n",
      "[\"also\", \"back\"]\t[0.408248290463863, 0.25]\r\n",
      "[\"also\", \"day\"]\t[0.408248290463863, 0.25]\r\n",
      "[\"back\", \"day\"]\t[0.6666666666666669, 0.5]\r\n",
      "Removing temp directory /tmp/Similarity5Gram.hadoop.20160613.033329.757765...\r\n"
     ]
    }
   ],
   "source": [
    "## HDFS file locations\n",
    "#ifile = 'hdfs:///user/hadoop/HW5data/'\n",
    "## Local file locations\n",
    "#ifile = '5gram_small.txt'\n",
    "#ifile = 'megan_test.txt'\n",
    "#ifile = 'fox_bunny.txt'\n",
    "#ifile = 'fox_bunny2.txt'\n",
    "ifile = 'outputHW5/InvIndex5Gram'\n",
    "ofile = 'outputHW5/Similarity5Gram'\n",
    "#!hdfs dfs -rm -r /user/hadoop/$ofile\n",
    "#!nohup python InvIndex5Gram.py -r hadoop $ifile --output-dir=$ofile --no-output\n",
    "!python Similarity5Gram.py $ifile --output-dir=$ofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### OUTPUT FOR HW5.4 INVERTED INDEX #####\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | head -20\n",
    "#!hdfs dfs -cat outputHW5/InvIndex5Gram/part-00000 | tail -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Stripes5Gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Stripes5GramORIGINAL.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from collections import Counter\n",
    "    \n",
    "# This class outputs the words in decreasing order of density (count/pages_count)\n",
    "class MRStripes5Gram(MRJob):\n",
    "    word_stripe_counts = {}\n",
    "    r_word_stripe_counts = {}\n",
    "    freq_words = set()\n",
    "    features = set()\n",
    "    \n",
    "    def mapper_stripes_init(self):\n",
    "        self.word_stripe_counts = {}\n",
    "        with open('5-4words.txt', 'r') as f:\n",
    "            count = 0\n",
    "            for line in f.readlines():\n",
    "                if count < 10000:\n",
    "                    self.freq_words.add(line.strip())\n",
    "                else:\n",
    "                    self.features.add(line.strip())\n",
    "                count += 1\n",
    "    \n",
    "    def mapper_stripes(self, _, line):\n",
    "        # read the next line from the file and output each word of the 5-gram and\n",
    "        # its count and page_count\n",
    "        record = line.strip().split('\\t')\n",
    "        words = record[0].lower().split()\n",
    "        count = int(record[1])\n",
    "        for i, w1 in enumerate(words):\n",
    "            if w1 in self.freq_words:\n",
    "                H = {}\n",
    "                for w2 in (words[:i] + words[i+1:]):\n",
    "                    if (w1 != w2) and (w2 in self.features):\n",
    "                        if w2 in H:\n",
    "                            H[w2] += count\n",
    "                        else:\n",
    "                            H[w2] = count\n",
    "                        if w2 in self.word_stripe_counts:\n",
    "                            self.word_stripe_counts[w2] += 1\n",
    "                        else:\n",
    "                            self.word_stripe_counts[w2] = 1\n",
    "                yield w1, H\n",
    "\n",
    "    def mapper_stripes_final(self):\n",
    "        for w, count in self.word_stripe_counts.iteritems():\n",
    "            yield '*' + w, count \n",
    "    \n",
    "    def reducer_stripes(self, word, counts):\n",
    "        if word[0] == '*':\n",
    "            yield word, sum(counts)\n",
    "        else:\n",
    "            sum_H = Counter()\n",
    "            for H in counts:\n",
    "                sum_H += Counter(H)\n",
    "            yield word, sum_H\n",
    "    \n",
    "    def binarize_stripe(self, stripe):\n",
    "        for w in stripe:\n",
    "            stripe[w] = 1\n",
    "        return stripe\n",
    "    \n",
    "    def normalize_stripe(self, stripe):\n",
    "        length = float(len(stripe))\n",
    "        for w in stripe:\n",
    "            stripe[w] /= pow(length, 0.5)\n",
    "        return stripe\n",
    "    \n",
    "    def mapper_inv_index(self, word, data):\n",
    "        if word[0] == '*':\n",
    "            yield word, data\n",
    "        else:\n",
    "            bin_stripe = self.binarize_stripe(data)\n",
    "            norm_stripe = self.normalize_stripe(bin_stripe)\n",
    "            for w, value in norm_stripe.iteritems():\n",
    "                 yield w, {word: value}\n",
    "\n",
    "    def mapper_similarity(self, word, data):\n",
    "        if word[0] == '*':\n",
    "            yield word, data\n",
    "        else:\n",
    "            length = len(data)\n",
    "            keys1 = data.keys()[:-1]\n",
    "            for i, w1 in enumerate(keys1):\n",
    "                keys2 = data.keys()[i+1:]\n",
    "                for w2 in keys2:\n",
    "                    cosine = data[w1]*data[w2]\n",
    "                    yield (w1,w2), (cosine,1)\n",
    "\n",
    "    def reducer_similarity(self, word, data):\n",
    "        if word[0] == '*':\n",
    "            self.r_word_stripe_counts[word[1:]] = sum(data)\n",
    "        else:\n",
    "            w1 = word[0]\n",
    "            w2 = word[1]\n",
    "            cosine = 0\n",
    "            jac_union = 0\n",
    "            for d in data:\n",
    "                cos = d[0]\n",
    "                jac = d[1]\n",
    "                cosine += cos\n",
    "                jac_union += jac\n",
    "            jaccard = jac_union / float(self.r_word_stripe_counts[w1] + self.r_word_stripe_counts[w2] - jac_union)\n",
    "            yield (w1,w2), (cosine,jaccard)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_stripes_init, mapper=self.mapper_stripes,  # STEP 1:  count the words and pages\n",
    "                   mapper_final=self.mapper_stripes_final,\n",
    "                   combiner=self.reducer_stripes,\n",
    "                   reducer=self.reducer_stripes),\n",
    "            MRStep(mapper=self.mapper_inv_index,      # STEP 2:  compute and sort the densities\n",
    "                   combiner=self.reducer_stripes,\n",
    "                   reducer=self.reducer_stripes),\n",
    "            MRStep(mapper=self.mapper_similarity,\n",
    "                  reducer=self.reducer_similarity)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRStripes5Gram.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Google N-Gram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_5gram_data.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_5gram_data.sh\n",
    "#!/bin/bash\n",
    "\n",
    "for index in {0..189}\n",
    "do\n",
    "  wget http://filtered-5grams.s3.amazonaws.com/googlebooks-eng-all-5gram-20090715-${index}-filtered.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x get_5gram_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!./get_5gram_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the 5-gram data in to HDFS\n",
    "#!hdfs dfs -mkdir /user/hadoop/HW5data\n",
    "#!hdfs dfs -rm /user/hadoop/HW5data/googlebooks-eng-all-5gram-20090715-65-filtered.txt\n",
    "for i in range(65,190):\n",
    "    filename = 'data/googlebooks-eng-all-5gram-20090715-' + str(i) + '-filtered.txt'\n",
    "    !hdfs dfs -copyFromLocal $filename /user/hadoop/HW5data\n",
    "# List the files in the HW5data directory to ensure that they are all in there\n",
    "!hdfs dfs -ls /user/hadoop/HW5data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file of Words and Values for 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/12 19:07:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cat: Unable to write to output stream.\n",
      "16/06/12 19:07:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "##### OUTPUT FOR HW5.3 MOST FREQUENT WORD #####\n",
    "!hdfs dfs -cat outputHW5/EDAMostFrequent/part-00000 | head -10155 > 5-4words_full.txt\n",
    "!hdfs dfs -cat outputHW5/EDAMostFrequent/part-00000 | head -1155 > 5-4values_full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "w = ['the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "     'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "     'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'and', 'but', 'if', 'or', 'because', \n",
    "     'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "     'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "     'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "     'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', \n",
    "     'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "     'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \n",
    "     \"hasn't\", \"haven't\", \"isn't\", 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \n",
    "     \"weren't\", \"won't\", \"wouldn't\", \"don't\", \"can't\", 'would', 'could', 'might', 'must', 'need']\n",
    "\n",
    "def remove_stop_words(f1, f2, w, n):\n",
    "    with open(f1, 'r') as ifile, open(f2, 'w+') as ofile:\n",
    "        i = 0\n",
    "        count = 0\n",
    "        word_count = 0\n",
    "        for line in ifile.readlines():\n",
    "            if word_count < n:\n",
    "                word = line.split()[0].strip('\"')\n",
    "                if word in w:\n",
    "                    #print i, word\n",
    "                    count += 1\n",
    "                else:\n",
    "                    ofile.write(word+'\\n')\n",
    "                    word_count += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        print(count)\n",
    "\n",
    "remove_stop_words('5-4words_full.txt', '5-4words.txt', w, 10000)\n",
    "remove_stop_words('5-4values_full.txt', '5-4values.txt', w, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 5, 'a': 4, 'c': 3, 'b': 2, 'd': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bb4131710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD8CAYAAACrbmW5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpyyguPTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bb4155410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### TEST CELL #####\n",
    "import pandas\n",
    "from collections import Counter\n",
    "a = ['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'c', 'd', 'e', 'e', 'e', 'e', 'e']\n",
    "letter_counts = Counter(a)\n",
    "print(letter_counts)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A {'A': [50], 'X': [50], 'L': [50], 'P': [50]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7334e3768d50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mH\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m#if w2 in H:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#    H[w2] += count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "#### TEST CELL ####\n",
    "count = 50\n",
    "words = ['A', 'X', 'A', 'P', 'L']\n",
    "for i, w1 in enumerate(words):\n",
    "    H = {} \n",
    "    for w2 in (words[:i] + words[i+1:]):\n",
    "        H[w2] = [int(H[w2])+count if w2 in H else count]\n",
    "        #if w2 in H:\n",
    "        #    H[w2] += count\n",
    "        #else:\n",
    "        #    H[w2] = count\n",
    "    print w1, H\n",
    "print\n",
    "print(words.remove('A'))\n",
    "wset = set(words)\n",
    "for i, w1 in enumerate(wset):\n",
    "    H = {} \n",
    "    for w2 in words:\n",
    "        if w2 in H:\n",
    "            H[w2] += count\n",
    "        else:\n",
    "            H[w2] = count\n",
    "    print w1, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\", \"don't\", \"can't\"]\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "filename = 'stop_words.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        m = line.strip('\\n[]').split(',')\n",
    "        n = [x.strip(' ').strip('u').strip(\"'\") for x in m]\n",
    "\n",
    "w = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "     'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "     'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "     'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', \n",
    "     'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "     'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "     'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "     'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', \n",
    "     'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "     'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \n",
    "     \"hasn't\", \"haven't\", \"isn't\", 'ma', \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \n",
    "     \"weren't\", \"won't\", \"wouldn't\", \"don't\", \"can't\"]\n",
    "print w\n",
    "print len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "10000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "freq_words = set()\n",
    "features = set()\n",
    "with open('5-4words.txt', 'r') as f:\n",
    "    count = 0\n",
    "    for line in f.readlines():\n",
    "        if count < 10000:\n",
    "            freq_words.add(line.strip())\n",
    "        else:\n",
    "            features.add(line.strip())\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "print len(freq_words)\n",
    "print len(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
