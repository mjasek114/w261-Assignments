{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Sort MRJob Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Megan Jasek, 6/18/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of using MRJob to do a secondary sort.  I was unable to get sorting on an additional field to work (like to break ties).\n",
    "\n",
    "The key things about this solution are as follows:\n",
    "1.  MRJob.SORT_VALUES = True.  \n",
    "Note this from the docs when you set this value to True, MRJob automatically sets these 4 values:  \n",
    "stream.num.map.output.key.fields=2  \n",
    "mapred.text.key.partitioner.options=k1,1  \n",
    "blank out: mapred.output.key.comparator.class (to prevent interference from mrjob.conf.)  \n",
    "blank out: mapred.text.key.comparator.options (to prevent interference from mrjob.conf.)  \n",
    "\n",
    "2.  INTERNAL_PROTOCOL = RawProtocol\n",
    "This is what I have been using.  I actually didn't try it without doing this, so you might want to try that first because, when you set this value, you have to modify how you pass things around.  I made sure all of my output are strings and it works.\n",
    "\n",
    "3.  What is yielded from the mapper\n",
    "Yield something like this from your mapper.  The value must be before the word.  And I think you need to use a tab as well.  \n",
    "yield label, str(value)+'\\t'+str(word)  \n",
    "\n",
    "4.  Set these values in a jobconf  \n",
    "'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',  \n",
    "'mapred.text.key.comparator.options': '-k2,2nr',  \n",
    "\n",
    "This code works, but there are many other ways to do this.  It seems like MRJob has a bunch of bugs in it.  Or I am missing some special parameter somewhere.  Here are a few examples of what's weird:\n",
    "\n",
    "1.  When sorting is set to true, these parameters seem to be ignored:  \n",
    "            'stream.num.map.output.key.field': 3,  \n",
    "            'stream.map.output.field.separator':'\\t',  \n",
    "            And the '-k3,3' part here:  'mapreduce.partition.keycomparator.options': '-k2,2nr -k3,3',  \n",
    "\n",
    "2.  When sorting is set to false, but INTERNAL_PROTOCOL = RawProtocol, these parameters don't work:  \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',  \n",
    "            'stream.num.map.output.key.field': 3,  \n",
    "            'stream.map.output.field.separator':'\\t',  \n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k3,3',  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5gram_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 5gram_test.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EDALongest5gram_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EDALongest5gram_test.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "    \n",
    "# This class outputs the 5-grams from the input file in descending order based\n",
    "# on the number of characters in the 5-gram.\n",
    "class MREDALongest5gram(MRJob):\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "\n",
    "    def mapper_count_chars(self, _, line):\n",
    "        # read the next line from the file and output the first record (the 5-gram) with the count\n",
    "        # of its characters.\n",
    "        record = line.strip().split('\\t')\n",
    "        # Remove spaces and apostrophes\n",
    "        ngram = re.sub(\"[' ]\", '', record[0])\n",
    "        yield record[0], str(len(ngram))\n",
    "    \n",
    "    def reducer_sum_chars(self, ngram, counts):\n",
    "        # output each ngram (key) that is input to the reducer plus the sum of the counts.\n",
    "        c_sum = 0\n",
    "        for c in counts:\n",
    "            c_sum += int(c)\n",
    "        yield ngram, str(c_sum)\n",
    "\n",
    "    def mapper_sort_chars(self, word, value):\n",
    "        # add a label to each input for sorting based on the value\n",
    "        if int(value) < 20:\n",
    "            label = 'a'\n",
    "        else:\n",
    "            label = 'b'\n",
    "        yield label, str(value)+'\\t'+str(word)\n",
    "    \n",
    "    def reducer_sort_chars(self, label, value_pair):\n",
    "        # Output what is received.  The values are sorted at this point.\n",
    "        for vp in value_pair:\n",
    "            v, w = vp.split('\\t')\n",
    "            yield w, v\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {        \n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        # define the steps this MR job.  The JOBCONF_STEP2 tells Hadoop how to handle the data during\n",
    "        # the Hadoop shuffle for the 2nd job.  In this case the data should be sorted in reverse order\n",
    "        # by the 2nd output (in this case the values or counts of the characters).\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapreduce.job.maps': '4',\n",
    "            'mapreduce.job.reduces': '2'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOBCONF_STEP1,            # STEP 1:  count the characters\n",
    "                   mapper=self.mapper_count_chars,   \n",
    "                   reducer=self.reducer_sum_chars),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,            # STEP 2:  sort the characters\n",
    "                   mapper=self.mapper_sort_chars,\n",
    "                   reducer=self.reducer_sort_chars)  \n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MREDALongest5gram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/EDALongest5gram_test.hadoop.20160618.161244.782700\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/EDALongest5gram_test.hadoop.20160618.161244.782700/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar3578916398811490092/] [] /tmp/streamjob3184951053275566339.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1466193215040_0060\n",
      "  Submitted application application_1466193215040_0060\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0060/\n",
      "  Running job: job_1466193215040_0060\n",
      "  Job job_1466193215040_0060 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0060 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/EDALongest5gram_test.hadoop.20160618.161244.782700/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=302\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=344\n",
      "\t\tFILE: Number of bytes written=484118\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=881\n",
      "\t\tHDFS: Number of bytes written=302\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11420672\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6027264\n",
      "\t\tTotal time spent by all map tasks (ms)=11153\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11153\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5886\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5886\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11153\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5886\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=403\n",
      "\t\tInput split bytes=318\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=312\n",
      "\t\tMap output materialized bytes=356\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPhysical memory (bytes) snapshot=833040384\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=356\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=659554304\n",
      "\t\tVirtual memory (bytes) snapshot=8425439232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar3467435926986680210/] [] /tmp/streamjob5331429277017970833.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:5\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "  Submitting tokens for job: job_1466193215040_0061\n",
      "  Submitted application application_1466193215040_0061\n",
      "  The url to track the job: http://master:8088/proxy/application_1466193215040_0061/\n",
      "  Running job: job_1466193215040_0061\n",
      "  Job job_1466193215040_0061 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466193215040_0061 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/EDALongest5gram_test.hadoop.20160618.161244.782700/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=495\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=342\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=354\n",
      "\t\tFILE: Number of bytes written=849257\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1325\n",
      "\t\tHDFS: Number of bytes written=342\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=5\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=31219712\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=11545600\n",
      "\t\tTotal time spent by all map tasks (ms)=30488\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30488\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11275\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11275\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30488\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11275\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4390\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=840\n",
      "\t\tInput split bytes=830\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=322\n",
      "\t\tMap output materialized bytes=402\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=10\n",
      "\t\tPhysical memory (bytes) snapshot=1624698880\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=402\n",
      "\t\tShuffled Maps =10\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=1263534080\n",
      "\t\tVirtual memory (bytes) snapshot=14769197056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/EDALongest5gram_test.hadoop.20160618.161244.782700/output...\n",
      "\"A BILL FOR ESTABLISHING RELIGIOUS\"\t\"29\"\n",
      "\"A Circumstantial Narrative of the\"\t\"29\"\n",
      "\"A Biography of General George\"\t\"25\"\n",
      "\"A Child's Christmas in Wales\"\t\"23\"\n",
      "\"A Collection of Fairy Tales\"\t\"23\"\n",
      "\"A Case Study in Government\"\t\"22\"\n",
      "\"A Collection of Forms of\"\t\"20\"\n",
      "STDERR: 16/06/18 11:13:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\"A Case Study of Limited\"\t\"19\"\n",
      "\"A Case Study of Female\"\t\"18\"\n",
      "\"A City by the Sea\"\t\"13\"\n",
      "STDERR: 16/06/18 11:13:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/EDALongest5gram_test.hadoop.20160618.161244.782700...\n",
      "Removing temp directory /tmp/EDALongest5gram_test.hadoop.20160618.161244.782700...\n"
     ]
    }
   ],
   "source": [
    "# Test the program on the small dataset\n",
    "!python EDALongest5gram_test.py -r hadoop 5gram_test.txt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
