{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 DATASCI W261: Machine Learning at Scale  \n",
    " \n",
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  9\n",
    "* **Homework Number:**  7\n",
    "* **Date:**  7/10/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">General Description</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "In this assignment you will explore networks and develop MRJob code for \n",
    "finding shortest path graph distances. To build up to large data \n",
    "you will develop your code on some very simple, toy networks.\n",
    "After this you will take your developed code forward and modify it and \n",
    "apply it to two larger datasets (performing EDA along the way).\n",
    "\n",
    "<h3>Undirected toy network dataset</h3>\n",
    "\n",
    "\n",
    "In an undirected network all links are symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' both of the links:\n",
    "\n",
    "A -> B and B -> A\n",
    "\n",
    "will exist. \n",
    "\n",
    "The toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/undirected_toy.txt\n",
    "On under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "The Data folder is in: https://db.tt/Kxu48mL1)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted).\n",
    "\n",
    "\n",
    "<h3>Directed toy network dataset</h3>\n",
    "\n",
    "In a directed network all links are not necessarily symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' it is possible for only one of:\n",
    "\n",
    "A -> B or B -> A\n",
    "\n",
    "to exist. \n",
    "\n",
    "These toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/directed_toy.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name\n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted).\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting directed_toy.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile directed_toy.txt\n",
    "1\t{'2': 1, '6': 1}\n",
    "2\t{'1': 1, '3': 1, '4': 1}\n",
    "3\t{'2': 1, '4': 1}\n",
    "4\t{'2': 1, '5': 1}\n",
    "5\t{'1': 1, '2': 1, '4': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting undirected_toy.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile undirected_toy.txt\n",
    "1\t{'2': 1,'5': 1}\n",
    "2\t{'1': 1,'3': 1,'4': 1,'5': 1}\n",
    "3\t{'2': 1, '4': 1}\n",
    "4\t{'2': 1,'3': 1,'5': 1}\n",
    "5\t{'1': 1, '2': 1, '4': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.0: Shortest path graph distances (toy networks)</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "In this part of your assignment you will develop the base of your code for the week.\n",
    "\n",
    "Write MRJob classes to find shortest path graph distances, as described in the lectures. In addition to finding the distances, your code should also output a distance-minimizing path between the source and target.\n",
    "Work locally for this part of the assignment, and use both of the undirected and directed toy networks.\n",
    "\n",
    "To proof you code's function, run the following jobs\n",
    "\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "Solution: 1,5,4. NOTE: There is another shortest path also (HINT: 1->5->4)! Either will suffice (you will find this also in the remaining problems. E.g., 7.2 and 7.4.\n",
    " \n",
    "\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "Solution: 1,2,4,5\n",
    "\n",
    "and report your output---make sure it is correct!\n",
    "\n",
    "<h3>Main dataset 1: NLTK synonyms</h3>\n",
    "\n",
    "In the next part of this assignment you will explore a network derived from the NLTK synonym database used for evaluation in HW 5. At a high level, this network is undirected, defined so that there exists link between two nodes/words if the pair or words are a synonym. These data may be found at the location:\n",
    "\n",
    "<a href=\"s3://ucb-mids-mls-networks/synNet/synNet.txt\">s3://ucb-mids-mls-networks/synNet/synNet.txt</a>\n",
    "<a href=\"s3://ucb-mids-mls-networks/synNet/indices.txt\">s3://ucb-mids-mls-networks/synNet/indices.txt</a>\n",
    "On under the Data Subfolder for HW7 on Dropbox with the same file names\n",
    "\n",
    "where synNet.txt contains a sparse representation of the network:\n",
    "\n",
    "(index) \\t (dictionary of links)\n",
    "\n",
    "in indexed form, and indices.txt contains a lookup list\n",
    "\n",
    "(word) \\t (index)\n",
    "\n",
    "of indices and words. This network is small enough for you to explore and run\n",
    "scripts locally, but will also be good for a systems test (for later) on AWS.\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted).\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SSSP.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SSSP.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "class MRSSSP(MRJob):\n",
    "    # create instance variables to store the centroid_points for the mapper and the\n",
    "    # value of k for the reducer.\n",
    "    centroid_points=[]\n",
    "    k=4\n",
    "    def configure_options(self):\n",
    "        # Configure a new command line option to enable the job to accept the value\n",
    "        # of k required by the Kmeans algorithm.\n",
    "        super(MRSSSP, self).configure_options()\n",
    "        self.add_passthrough_option('--k', type='int', default=4)\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        #print \"Current path:\", os.path.dirname(os.path.realpath(__file__))        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        # This is the line that breaks things with multiple mappers, so it is commented out.\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point plus a count of 1. \n",
    "    def mapper(self, _, line):\n",
    "        node_num, adj_dict, min_dist, short_path, state = line.strip().split('\\t')\n",
    "        if state == 'Q':\n",
    "            yield node_num, tuple((adj_dict, min_dist, short_path+node_num, 'V'))\n",
    "            d = eval(adj_dict)\n",
    "            for node, dist in d.iteritems():\n",
    "                yield node, tuple(('{}', float(min_dist)+float(dist), short_path+node_num, 'Q'))\n",
    "        else:\n",
    "            yield node_num, tuple((adj_dict, min_dist, short_path, state))\n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        num = 0\n",
    "        sumD = [0.0]*1000\n",
    "        for D,n in inputdata:\n",
    "            num += n\n",
    "            sumD = [x + y for x, y in zip(sumD,D)]\n",
    "        yield idx,(sumD,num)\n",
    "    \n",
    "    # Set the value of k for the reducer from the command line argument passed in\n",
    "    def reducer_init(self): \n",
    "        self.k = self.options.k\n",
    "        \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, node, data):\n",
    "        f_adj_dict = Counter()\n",
    "        f_min_dist = 1000000.0\n",
    "        f_short_path = ''\n",
    "        f_state = ''\n",
    "        print node\n",
    "        for adj_dict, min_dist, short_path, state in data:\n",
    "            print adj_dict, min_dist, short_path, state\n",
    "            if state == 'V':\n",
    "                f_adj_dict = adj_dict\n",
    "                f_min_dist = min_dist\n",
    "                f_short_path = short_path\n",
    "                f_state = 'V'\n",
    "                break\n",
    "            f_adj_dict += Counter(eval(adj_dict))\n",
    "            if min_dist < f_min_dist:\n",
    "                f_min_dist = min_dist\n",
    "                f_short_path = short_path\n",
    "            f_state += state\n",
    "        if 'V' in f_state:\n",
    "            f_state = 'V'        \n",
    "        elif 'Q' in f_state:\n",
    "            f_state = 'Q'\n",
    "        else:\n",
    "            f_state = 'U'\n",
    "        yield node, tuple((f_adj_dict, f_min_dist, f_short_path, f_state))\n",
    "\n",
    "    def steps(self):\n",
    "        # Create the steps for the MRJob.  There is only one MRJob here containing\n",
    "        # a mapper, a combiner and a reducer.\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   #combiner = self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    MRSSSP.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are helper functions for the shortest path code\n",
    "\n",
    "def init_work_table(ifilename, ofilename):\n",
    "    with open(ifilename, 'r') as ifile, open(ofilename, 'w') as ofile:\n",
    "        line = ifile.readline()\n",
    "        ofile.write('%s\\t%f\\t%s\\t%s\\n' % (line.strip(),0.0,\"\",'Q'))\n",
    "        for line in ifile.readlines():\n",
    "            # ??fix this to be infinity or something\n",
    "            ofile.write('%s\\t%f\\t%s\\t%s\\n' % (line.strip(),1000000000.0,\"\",'U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "1\n",
      "{'2': 1,'5': 1} 0.000000 1 V\n",
      "2\n",
      "{'1': 1,'3': 1,'4': 1,'5': 1} 1000000000.000000  U\n",
      "{} 1.0 1 Q\n",
      "3\n",
      "{'2': 1, '4': 1} 1000000000.000000  U\n",
      "4\n",
      "{'2': 1,'3': 1,'5': 1} 1000000000.000000  U\n",
      "5\n",
      "{'1': 1, '2': 1, '4': 1} 1000000000.000000  U\n",
      "{} 1.0 1 Q\n",
      "4 [{u'3': 1, u'2': 1, u'5': 1}, 1000000.0, u'', u'U']\n",
      "5 [{u'1': 1, u'2': 1, u'4': 1}, 1.0, u'1', u'Q']\n",
      "1 [u\"{'2': 1,'5': 1}\", u'0.000000', u'1', u'V']\n",
      "2 [{u'1': 1, u'3': 1, u'5': 1, u'4': 1}, 1.0, u'1', u'Q']\n",
      "3 [{u'2': 1, u'4': 1}, 1000000.0, u'', u'U']\n",
      "Iteration 1\n",
      "1\n",
      "{'2': 1,'5': 1} 0.000000 1 V\n",
      "2\n",
      "{u'1': 1, u'3': 1, u'5': 1, u'4': 1} 1.0 12 V\n",
      "3\n",
      "{u'2': 1, u'4': 1} 1000000.0  U\n",
      "{} 2.0 12 Q\n",
      "4\n",
      "{u'3': 1, u'2': 1, u'5': 1} 1000000.0  U\n",
      "{} 2.0 12 Q\n",
      "{} 2.0 15 Q\n",
      "5\n",
      "{u'1': 1, u'2': 1, u'4': 1} 1.0 15 V\n",
      "4 [{u'3': 1, u'2': 1, u'5': 1}, 2.0, u'12', u'Q']\n",
      "5 [u\"{u'1': 1, u'2': 1, u'4': 1}\", u'1.0', u'15', u'V']\n",
      "1 [u\"{'2': 1,'5': 1}\", u'0.000000', u'1', u'V']\n",
      "2 [u\"{u'1': 1, u'3': 1, u'5': 1, u'4': 1}\", u'1.0', u'12', u'V']\n",
      "3 [{u'2': 1, u'4': 1}, 2.0, u'12', u'Q']\n",
      "Iteration 2\n",
      "1\n",
      "{'2': 1,'5': 1} 0.000000 1 V\n",
      "2\n",
      "{u'1': 1, u'3': 1, u'5': 1, u'4': 1} 1.0 12 V\n",
      "3\n",
      "{u'2': 1, u'4': 1} 2.0 123 V\n",
      "4\n",
      "{u'3': 1, u'2': 1, u'5': 1} 2.0 124 V\n",
      "5\n",
      "{u'1': 1, u'2': 1, u'4': 1} 1.0 15 V\n",
      "4 [u\"{u'3': 1, u'2': 1, u'5': 1}\", u'2.0', u'124', u'V']\n",
      "5 [u\"{u'1': 1, u'2': 1, u'4': 1}\", u'1.0', u'15', u'V']\n",
      "1 [u\"{'2': 1,'5': 1}\", u'0.000000', u'1', u'V']\n",
      "2 [u\"{u'1': 1, u'3': 1, u'5': 1, u'4': 1}\", u'1.0', u'12', u'V']\n",
      "3 [u\"{u'2': 1, u'4': 1}\", u'2.0', u'123', u'V']\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy import random\n",
    "from SSSP import MRSSSP\n",
    "\n",
    "work_filename = 'work_table.txt'\n",
    "init_work_table('undirected_toy.txt', work_filename)\n",
    "\n",
    "# Run the KMeans MRJob\n",
    "mr_job = MRSSSP(args=['work_table.txt'])\n",
    "    \n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "not_all_visited = True\n",
    "while(not_all_visited):\n",
    "    work_table = {}\n",
    "    not_all_visited = False\n",
    "    print('Iteration %d' % (i))\n",
    "    # save previous centoids to check convergency\n",
    "    #centroid_points_old = centroid_points[:]\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            work_table[key] = value\n",
    "            if value[3] in 'QU':\n",
    "                not_all_visited = True \n",
    "        \n",
    "        # Update work_table for the next iteration\n",
    "        with open(work_filename, 'w') as f:\n",
    "            for key, value in work_table.iteritems():\n",
    "                f.write(key)\n",
    "                f.writelines('\\t'+str(v) for v in value)\n",
    "                f.write('\\n')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{'2': 1,'5': 1}\t0.000000\t1\tV\r\n",
      "3\t{u'2': 1, u'4': 1}\t2.0\t123\tV\r\n",
      "2\t{u'1': 1, u'3': 1, u'5': 1, u'4': 1}\t1.0\t12\tV\r\n",
      "5\t{u'1': 1, u'2': 1, u'4': 1}\t1.0\t15\tV\r\n",
      "4\t{u'3': 1, u'2': 1, u'5': 1}\t2.0\t124\tV\r\n"
     ]
    }
   ],
   "source": [
    "!cat work_table.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.1: Exploratory data analysis (NLTK synonyms)</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Using MRJob, explore the synonyms network data.\n",
    "Consider plotting the degree distribution (does it follow a power law?),\n",
    "and determine some of the key features, like:\n",
    "\n",
    "number of nodes, \n",
    "number links,\n",
    "or the average degree (i.e., the average number of links per node),\n",
    "etc...\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS).\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.2: Shortest path graph distances (NLTK synonyms)</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Write (reuse your code from 7.0) an MRJob class to find shortest path graph distances, \n",
    "and apply it to the NLTK synonyms network dataset. \n",
    "\n",
    "Proof your code's function by running the job:\n",
    "\n",
    "- shortest path starting at \"walk\" (index=7827) and ending at \"make\" (index=536),\n",
    "\n",
    "and showing you code's output. Once again, your output should include the path and the distance.\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS).\n",
    "\n",
    "=====================================\n",
    "<strong>NOTE: Dataset 2 English Wikipedia hyperlink network.data </strong>\n",
    "The dataset is available via Dropbox at:\n",
    "\n",
    "https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "on S3 at \n",
    "<a href=\"s3://ucb-mids-mls-networks/wikipedia/\">s3://ucb-mids-mls-networks/wikipedia/</a>\n",
    "<a href=\"s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\">s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt</a> # Graph\n",
    "<a href=\"s3://ucb-mids-mls-networks/wikipedia/indices.txt\">s3://ucb-mids-mls-networks/wikipedia/indices.txt</a> # Page titles and page Ids\n",
    "\n",
    "For the remainder of this assignment you will explore the English Wikipedia hyperlink network.\n",
    "\n",
    "The dataset is built from the Sept. 2015 XML snapshot of English Wikipedia.\n",
    "For this directed network, a link between articles: \n",
    "\n",
    "A -> B\n",
    "\n",
    "is defined by the existence of a hyperlink in A pointing to B.\n",
    "This network also exists in the indexed format:\n",
    "\n",
    "Data: <a href=\"s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\">s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt</a>\n",
    "Data: <a href=\"s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-in.txt\">s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-in.txt</a>\n",
    "Data: <a href=\"s3://ucb-mids-mls-networks/wikipedia/indices.txt\">s3://ucb-mids-mls-networks/wikipedia/indices.txt</a>\n",
    "\n",
    "but has an index with more detailed data:\n",
    "\n",
    "(article name) \\t (index) \\t (in degree) \\t (out degree)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values .\n",
    "Here, a weight indicates the number of time a page links to another.\n",
    "However, for the sake of this assignment, treat this an unweighted network,\n",
    "and set all weights to 1 upon data input.\n",
    "\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.3: Exploratory data analysis (Wikipedia)</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1---does is scale well? \n",
    "\n",
    "Be cautioned that Wikipedia is a directed network, where links are not symmetric. \n",
    "So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. \n",
    "\n",
    "This means that you may have to ADJUST your code (depending on its design). \n",
    "\n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.4: Shortest path graph distances (Wikipedia)</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud.\n",
    "Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network.\n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "- shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359),\n",
    "\n",
    "and show your code's output. Show the shortest path in terms of just page IDS but also in terms of the name of page (show of your MapReduce join skills!!)\n",
    "\n",
    "Once your code is running, find some other shortest paths and report your results.\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.5: Conceptual exercise: Largest single-source network distances</h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Suppose you wanted to find the largest network distance from a single source,\n",
    "i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "\n",
    "How would you implement this task? \n",
    "How is this different from finding the shortest path graph distances?\n",
    "\n",
    "Is this task more difficult to implement than the shortest path distance?\n",
    "\n",
    "As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc...\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.5.1: </h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Can we utilize combiners in the HW 7 to perform the shortest path implementation?\n",
    "Does order inversion help with the HW 7 shortest path implementation?\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.5.2: OPTIONAL </h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Implement combiners in the context of HW 7.5 and contrast the performance of this implementation versus the implementation with no combiners. \n",
    "\n",
    "Please report the cluster configuration and runtimes in tabular format for both experiments and comment on your findings.\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#021353;\">HW 7.6: Computational exercise: Largest single-source network distances: OPTIONAL </h1>\n",
    "<div style=\"margin:10px;border-left:5px solid #eee;\">\n",
    "<pre style=\"font-family:sans-serif;background-color:transparent\">\n",
    "Using MRJob, write a code to find the largest graph distance and distance-maximizing nodes from a single-source.\n",
    "Test your code first on the toy networks and synonyms network to proof its function.\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "==================END HW 7=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
