{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  4\n",
    "* **Date:**  6/7/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is MrJob? How is it different to Hadoop MapReduce? What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is serialization in the context of MrJob or Hadoop? When it used in these frameworks? What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:  \n",
    "C,\"10001\",10001 #Visitor id 10001  \n",
    "V,1000,1 #Visit by Visitor 10001 to page id 1000  \n",
    "V,1001,1 #Visit by Visitor 10001 to page id 1001  \n",
    "V,1002,1 #Visit by Visitor 10001 to page id 1002  \n",
    "C,\"10002\",10002 #Visitor id 10001  \n",
    "V  \n",
    "Note: #denotes comments  \n",
    "to the format:  \n",
    "V,1000,1,C, 10001  \n",
    "V,1001,1,C, 10001  \n",
    "V,1002,1,C, 10001  \n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This code reads the input file (infile), converts the data in it and writes the conversion\n",
    "    to the output file (outfile).  The data is converted from writing customer information on\n",
    "    a single line to writing customer information on the line corresponding its associated visit\n",
    "    as per the instructions for HW4.2 above.\n",
    "'''\n",
    "infile = \"anonymous-msweb.data\"\n",
    "outfile = \"anonymous-msweb_converted.data\"\n",
    "with open(infile, 'r') as rf, open(outfile, 'w') as wf:\n",
    "    for line in rf.readlines():\n",
    "        # Split the lines on commas\n",
    "        items = line.split(',')\n",
    "        # If the line is a customer line, then save the customer ID for later use\n",
    "        # Write the line to the output file\n",
    "        if items[0] == 'C':\n",
    "            cust_str = items[2]\n",
    "            wf.write(line)\n",
    "        # If the line is a visit line, then concatenate, the original line with the\n",
    "        # current customer information (the current value of cust_str) and write\n",
    "        # it to the output file\n",
    "        elif items[0] == 'V':\n",
    "            wf.write('%s,C,%s' % (line.strip(), cust_str))\n",
    "        # All other lines write directly to the output file as is\n",
    "        else:\n",
    "            wf.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentVisits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentVisits.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "#from mrjob.step import MRJobStep\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "#            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "\n",
    "class MRMostFrequentVisits(MRJob):\n",
    "\n",
    "    def mapper_count_visits(self, _, line):\n",
    "        record = csv_readline(line)\n",
    "        if record[0] == 'V':\n",
    "            yield record[1], 1\n",
    "    \n",
    "    def reducer_sum_visits(self, page_id, counts):\n",
    "        yield page_id, sum(counts)\n",
    "    \n",
    "    def reducer_sort_visits(self, page_id, counts):\n",
    "        yield page_id, sum(counts)\n",
    "        \n",
    "    def steps(self):\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_visits,   # STEP 1:  count the visits\n",
    "                   reducer=self.reducer_sum_visits),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    reducer=self.reducer_sort_visits)  # STEP 2:  sort the visits\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRMostFrequentVisits.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostFrequentVisits.hadoop.20160606.070103.450541\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisits.hadoop.20160606.070103.450541/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar140944324081476772/] [] /tmp/streamjob2687987219726866806.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1463787494457_0332\n",
      "  Submitted application application_1463787494457_0332\n",
      "  The url to track the job: http://master:8088/proxy/application_1463787494457_0332/\n",
      "  Running job: job_1463787494457_0332\n",
      "  Job job_1463787494457_0332 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1463787494457_0332 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisits.hadoop.20160606.070103.450541/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2216181\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1085200\n",
      "\t\tFILE: Number of bytes written=2531479\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2216527\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8911872\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3162112\n",
      "\t\tTotal time spent by all map tasks (ms)=8703\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8703\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3088\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3088\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8703\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3088\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2810\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=184\n",
      "\t\tInput split bytes=346\n",
      "\t\tMap input records=131666\n",
      "\t\tMap output bytes=887886\n",
      "\t\tMap output materialized bytes=1085206\n",
      "\t\tMap output records=98654\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=665219072\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=98654\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=1085206\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=197308\n",
      "\t\tTotal committed heap usage (bytes)=505413632\n",
      "\t\tVirtual memory (bytes) snapshot=6288109568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar4721341468041621831/] [] /tmp/streamjob5309373016102259863.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1463787494457_0333\n",
      "  Submitted application application_1463787494457_0333\n",
      "  The url to track the job: http://master:8088/proxy/application_1463787494457_0333/\n",
      "  Running job: job_1463787494457_0333\n",
      "  Job job_1463787494457_0333 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1463787494457_0333 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisits.hadoop.20160606.070103.450541/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4355\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3764\n",
      "\t\tFILE: Number of bytes written=370242\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4683\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6915072\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2032640\n",
      "\t\tTotal time spent by all map tasks (ms)=6753\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6753\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1985\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1985\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6753\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1985\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1220\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=173\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=285\n",
      "\t\tMap output bytes=3188\n",
      "\t\tMap output materialized bytes=3770\n",
      "\t\tMap output records=285\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=651837440\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=285\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=3770\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=570\n",
      "\t\tTotal committed heap usage (bytes)=506462208\n",
      "\t\tVirtual memory (bytes) snapshot=6292733952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisits.hadoop.20160606.070103.450541/output...\n",
      "\"1008\"\t10836\n",
      "\"1034\"\t9383\n",
      "\"1004\"\t8463\n",
      "\"1018\"\t5330\n",
      "\"1017\"\t5108\n",
      "\"1009\"\t4628\n",
      "\"1001\"\t4451\n",
      "\"1026\"\t3220\n",
      "\"1003\"\t2968\n",
      "\"1025\"\t2123\n",
      "\"1035\"\t1791\n",
      "\"1040\"\t1506\n",
      "\"1041\"\t1500\n",
      "\"1032\"\t1446\n",
      "\"1037\"\t1160\n",
      "\"1030\"\t1115\n",
      "\"1038\"\t1110\n",
      "\"1020\"\t1087\n",
      "\"1000\"\t912\n",
      "\"1007\"\t865\n",
      "\"1052\"\t842\n",
      "\"1036\"\t759\n",
      "\"1002\"\t749\n",
      "\"1014\"\t728\n",
      "\"1295\"\t716\n",
      "\"1010\"\t698\n",
      "\"1058\"\t672\n",
      "\"1053\"\t670\n",
      "\"1046\"\t636\n",
      "\"1070\"\t602\n",
      "\"1074\"\t584\n",
      "\"1031\"\t574\n",
      "\"1067\"\t548\n",
      "\"1024\"\t521\n",
      "\"1027\"\t507\n",
      "\"1045\"\t474\n",
      "\"1078\"\t462\n",
      "\"1076\"\t444\n",
      "\"1075\"\t396\n",
      "\"1130\"\t395\n",
      "\"1060\"\t391\n",
      "\"1021\"\t380\n",
      "\"1123\"\t372\n",
      "\"1119\"\t365\n",
      "\"1039\"\t345\n",
      "\"1049\"\t343\n",
      "\"1054\"\t338\n",
      "\"1022\"\t325\n",
      "\"1064\"\t324\n",
      "\"1065\"\t323\n",
      "\"1100\"\t291\n",
      "\"1016\"\t287\n",
      "\"1042\"\t281\n",
      "\"1056\"\t276\n",
      "\"1061\"\t269\n",
      "\"1055\"\t264\n",
      "\"1059\"\t258\n",
      "\"1082\"\t241\n",
      "\"1088\"\t237\n",
      "\"1069\"\t227\n",
      "\"1043\"\t224\n",
      "\"1124\"\t222\n",
      "\"1081\"\t215\n",
      "\"1096\"\t214\n",
      "\"1048\"\t210\n",
      "\"1073\"\t204\n",
      "\"1125\"\t199\n",
      "\"1068\"\t198\n",
      "\"1057\"\t195\n",
      "\"1023\"\t191\n",
      "\"1087\"\t189\n",
      "\"1071\"\t187\n",
      "\"1084\"\t186\n",
      "\"1105\"\t183\n",
      "\"1113\"\t181\n",
      "\"1136\"\t181\n",
      "\"1011\"\t179\n",
      "\"1118\"\t172\n",
      "\"1044\"\t168\n",
      "\"1183\"\t167\n",
      "\"1134\"\t162\n",
      "\"1089\"\t157\n",
      "\"1077\"\t155\n",
      "\"1131\"\t148\n",
      "\"1062\"\t141\n",
      "\"1079\"\t136\n",
      "\"1006\"\t135\n",
      "\"1029\"\t132\n",
      "\"1127\"\t132\n",
      "\"1072\"\t128\n",
      "\"1112\"\t128\n",
      "\"1157\"\t124\n",
      "\"1137\"\t123\n",
      "\"1080\"\t121\n",
      "\"1099\"\t120\n",
      "\"1102\"\t118\n",
      "\"1140\"\t118\n",
      "\"1135\"\t115\n",
      "\"1063\"\t113\n",
      "\"1019\"\t111\n",
      "\"1090\"\t107\n",
      "\"1050\"\t106\n",
      "\"1083\"\t105\n",
      "\"1095\"\t102\n",
      "\"1098\"\t98\n",
      "\"1092\"\t97\n",
      "\"1148\"\t96\n",
      "\"1188\"\t94\n",
      "\"1028\"\t93\n",
      "\"1150\"\t93\n",
      "\"1168\"\t93\n",
      "\"1158\"\t90\n",
      "\"1051\"\t86\n",
      "\"1085\"\t86\n",
      "\"1147\"\t86\n",
      "\"1066\"\t82\n",
      "\"1015\"\t79\n",
      "\"1146\"\t79\n",
      "\"1156\"\t75\n",
      "\"1167\"\t72\n",
      "\"1091\"\t69\n",
      "\"1133\"\t69\n",
      "\"1154\"\t67\n",
      "\"1093\"\t65\n",
      "\"1121\"\t63\n",
      "\"1176\"\t63\n",
      "\"1013\"\t61\n",
      "\"1143\"\t60\n",
      "\"1109\"\t59\n",
      "\"1184\"\t57\n",
      "\"1097\"\t56\n",
      "\"1203\"\t55\n",
      "\"1152\"\t52\n",
      "\"1155\"\t52\n",
      "\"1189\"\t51\n",
      "\"1164\"\t49\n",
      "\"1114\"\t48\n",
      "\"1162\"\t48\n",
      "\"1190\"\t48\n",
      "\"1108\"\t47\n",
      "\"1171\"\t47\n",
      "\"1110\"\t46\n",
      "\"1186\"\t46\n",
      "\"1172\"\t45\n",
      "\"1215\"\t45\n",
      "\"1012\"\t44\n",
      "\"1169\"\t44\n",
      "\"1177\"\t43\n",
      "\"1005\"\t42\n",
      "\"1159\"\t41\n",
      "\"1165\"\t38\n",
      "\"1187\"\t38\n",
      "\"1201\"\t38\n",
      "\"1103\"\t36\n",
      "\"1111\"\t36\n",
      "\"1141\"\t36\n",
      "\"1144\"\t36\n",
      "\"1160\"\t36\n",
      "\"1104\"\t35\n",
      "\"1208\"\t34\n",
      "\"1138\"\t33\n",
      "\"1166\"\t33\n",
      "\"1197\"\t32\n",
      "\"1227\"\t32\n",
      "\"1116\"\t31\n",
      "\"1204\"\t30\n",
      "\"1216\"\t30\n",
      "\"1206\"\t29\n",
      "\"1223\"\t29\n",
      "\"1126\"\t27\n",
      "\"1033\"\t26\n",
      "\"1194\"\t26\n",
      "\"1163\"\t25\n",
      "\"1193\"\t25\n",
      "\"1212\"\t25\n",
      "\"1185\"\t24\n",
      "\"1205\"\t24\n",
      "\"1132\"\t23\n",
      "\"1220\"\t23\n",
      "\"1086\"\t22\n",
      "\"1151\"\t21\n",
      "\"1230\"\t21\n",
      "\"1145\"\t20\n",
      "\"1142\"\t19\n",
      "\"1231\"\t19\n",
      "\"1139\"\t18\n",
      "\"1198\"\t18\n",
      "\"1200\"\t18\n",
      "\"1218\"\t18\n",
      "\"1106\"\t16\n",
      "\"1161\"\t16\n",
      "\"1170\"\t16\n",
      "\"1211\"\t16\n",
      "\"1224\"\t16\n",
      "\"1115\"\t15\n",
      "\"1195\"\t15\n",
      "\"1094\"\t14\n",
      "\"1101\"\t14\n",
      "\"1226\"\t14\n",
      "\"1122\"\t13\n",
      "\"1217\"\t13\n",
      "\"1228\"\t13\n",
      "\"1250\"\t13\n",
      "\"1149\"\t12\n",
      "\"1181\"\t12\n",
      "\"1207\"\t12\n",
      "\"1107\"\t11\n",
      "\"1179\"\t11\n",
      "\"1222\"\t11\n",
      "\"1240\"\t11\n",
      "\"1251\"\t11\n",
      "\"1174\"\t10\n",
      "\"1221\"\t10\n",
      "\"1236\"\t10\n",
      "\"1246\"\t10\n",
      "\"1117\"\t9\n",
      "\"1180\"\t9\n",
      "\"1209\"\t9\n",
      "\"1235\"\t9\n",
      "\"1241\"\t9\n",
      "\"1153\"\t8\n",
      "\"1234\"\t8\n",
      "\"1129\"\t7\n",
      "\"1182\"\t7\n",
      "\"1192\"\t7\n",
      "\"1225\"\t7\n",
      "\"1175\"\t6\n",
      "\"1210\"\t5\n",
      "\"1253\"\t5\n",
      "\"1257\"\t5\n",
      "\"1267\"\t5\n",
      "\"1191\"\t4\n",
      "\"1202\"\t4\n",
      "\"1219\"\t4\n",
      "\"1229\"\t4\n",
      "\"1232\"\t4\n",
      "\"1238\"\t4\n",
      "\"1242\"\t4\n",
      "\"1243\"\t4\n",
      "\"1244\"\t4\n",
      "\"1262\"\t4\n",
      "\"1264\"\t4\n",
      "\"1173\"\t3\n",
      "\"1213\"\t3\n",
      "\"1214\"\t3\n",
      "\"1237\"\t3\n",
      "\"1239\"\t3\n",
      "\"1247\"\t3\n",
      "\"1252\"\t3\n",
      "\"1255\"\t3\n",
      "\"1256\"\t3\n",
      "\"1258\"\t3\n",
      "\"1265\"\t3\n",
      "\"1276\"\t3\n",
      "\"1178\"\t2\n",
      "\"1245\"\t2\n",
      "\"1249\"\t2\n",
      "\"1261\"\t2\n",
      "\"1263\"\t2\n",
      "\"1266\"\t2\n",
      "\"1269\"\t2\n",
      "\"1278\"\t2\n",
      "\"1280\"\t2\n",
      "\"1282\"\t2\n",
      "\"1120\"\t1\n",
      "\"1128\"\t1\n",
      "\"1196\"\t1\n",
      "\"1199\"\t1\n",
      "\"1233\"\t1\n",
      "\"1248\"\t1\n",
      "\"1254\"\t1\n",
      "\"1259\"\t1\n",
      "\"1260\"\t1\n",
      "\"1268\"\t1\n",
      "\"1270\"\t1\n",
      "\"1271\"\t1\n",
      "\"1272\"\t1\n",
      "\"1273\"\t1\n",
      "\"1274\"\t1\n",
      "\"1275\"\t1\n",
      "\"1277\"\t1\n",
      "\"1279\"\t1\n",
      "\"1281\"\t1\n",
      "\"1283\"\t1\n",
      "\"1284\"\t1\n",
      "STDERR: 16/06/06 02:01:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisits.hadoop.20160606.070103.450541...\n",
      "Removing temp directory /tmp/MostFrequentVisits.hadoop.20160606.070103.450541...\n"
     ]
    }
   ],
   "source": [
    "# There is a known bug, that step-level jobconf does not work in local and inline modes\n",
    "#!python MostFrequentVisits.py anonymous-msweb_converted.data\n",
    "# The job must be run with args '-r hadoop' to enable step-level jobconf\n",
    "!python MostFrequentVisits.py -r hadoop anonymous-msweb_converted.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/05 14:32:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1008', 10836)\n",
      "(u'1034', 9383)\n",
      "(u'1004', 8463)\n",
      "(u'1018', 5330)\n",
      "(u'1017', 5108)\n",
      "(u'1009', 4628)\n",
      "(u'1001', 4451)\n",
      "(u'1026', 3220)\n",
      "(u'1003', 2968)\n",
      "(u'1025', 2123)\n",
      "(u'1035', 1791)\n",
      "(u'1040', 1506)\n",
      "(u'1041', 1500)\n",
      "(u'1032', 1446)\n",
      "(u'1037', 1160)\n",
      "(u'1030', 1115)\n",
      "(u'1038', 1110)\n",
      "(u'1020', 1087)\n",
      "(u'1000', 912)\n",
      "(u'1007', 865)\n",
      "(u'1052', 842)\n",
      "(u'1036', 759)\n",
      "(u'1002', 749)\n",
      "(u'1014', 728)\n",
      "(u'1295', 716)\n",
      "(u'1010', 698)\n",
      "(u'1058', 672)\n",
      "(u'1053', 670)\n",
      "(u'1046', 636)\n",
      "(u'1070', 602)\n",
      "(u'1074', 584)\n",
      "(u'1031', 574)\n",
      "(u'1067', 548)\n",
      "(u'1024', 521)\n",
      "(u'1027', 507)\n",
      "(u'1045', 474)\n",
      "(u'1078', 462)\n",
      "(u'1076', 444)\n",
      "(u'1075', 396)\n",
      "(u'1130', 395)\n",
      "(u'1060', 391)\n",
      "(u'1021', 380)\n",
      "(u'1123', 372)\n",
      "(u'1119', 365)\n",
      "(u'1039', 345)\n",
      "(u'1049', 343)\n",
      "(u'1054', 338)\n",
      "(u'1022', 325)\n",
      "(u'1064', 324)\n",
      "(u'1065', 323)\n"
     ]
    }
   ],
   "source": [
    "from MostFrequentVisits import MRMostFrequentVisits\n",
    "# There is a known bug, that step-level jobconf does not work in local and inline modes\n",
    "#mr_job = MRMostFrequentVisits (args=['anonymous-msweb_converted.data'])\n",
    "# The job must be run with args '-r', 'hadoop' to enable step-level jobconf\n",
    "mr_job = MRMostFrequentVisits (args=['anonymous-msweb_converted.data', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    # stream_output and print each line of the output\n",
    "    for counter, line in enumerate(runner.stream_output()):\n",
    "        if counter < 50:\n",
    "            print mr_job.parse_output_line(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most frequent visitor of each page using MrJob and the output of 4.2 (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentVisitors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentVisitors.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class MRMostFrequentVisitors(MRJob):\n",
    "    reducer_current_pageid = \"\"\n",
    "    vroots = {}\n",
    "    #SORT_VALUES = True\n",
    "    \n",
    "    def mapper_count_visits(self, _, line):\n",
    "        record = csv_readline(line)\n",
    "        if record[0] == 'I':\n",
    "            self.vroots['0'] = record[2]\n",
    "        elif record[0] == 'A':\n",
    "            page_id = record[1]\n",
    "            vroot = record[4]\n",
    "            self.vroots[page_id] = vroot\n",
    "        elif record[0] == 'V':\n",
    "            page_id = record[1]\n",
    "            visitor_id = record[4]\n",
    "            page_visitor_pair = ('(%s.%s)' % (page_id, visitor_id))\n",
    "            yield page_visitor_pair, 1\n",
    "        \n",
    "    def reducer_sum_visits(self, page_visitor_pairs, counts):\n",
    "        yield page_visitor_pairs, sum(counts)\n",
    "    \n",
    "    def reducer_sort_visits(self, page_visitor_pairs, counts):\n",
    "        page_id, visitor_id = page_visitor_pairs.strip('()').split('.', 2)\n",
    "        #print(page_id)\n",
    "        #print(visitor_id)\n",
    "        if page_id != self.reducer_current_pageid:\n",
    "            self.reducer_current_pageid = page_id\n",
    "            #total_visits = sum(counts)\n",
    "            #output_str_1 = ('URL: %s%s, Page ID: %s, Visitor ID: %s' % \n",
    "            #              (self.vroots['0'], self.vroots[page_id], page_id, visitor_id))\n",
    "            #output_str_2 = ('# Page Visits: %d' % (total_visits))\n",
    "            #yield output_str_1, output_str_2\n",
    "            yield page_visitor_pairs, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP2 = {        \n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_visits,   # STEP 1:  count the visits\n",
    "                   reducer=self.reducer_sum_visits),\n",
    "            MRStep(jobconf=JOBCONF_STEP2,\n",
    "                   reducer=self.reducer_sort_visits)  # STEP 2:  sort the visits\n",
    "        ]\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    MRMostFrequentVisitors.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostFrequentVisitors.hadoop.20160606.073016.889574\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisitors.hadoop.20160606.073016.889574/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar4260441000237539642/] [] /tmp/streamjob3705776716557138787.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1463787494457_0342\n",
      "  Submitted application application_1463787494457_0342\n",
      "  The url to track the job: http://master:8088/proxy/application_1463787494457_0342/\n",
      "  Running job: job_1463787494457_0342\n",
      "  Job job_1463787494457_0342 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1463787494457_0342 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisitors.hadoop.20160606.073016.889574/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=14318\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=255\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=310\n",
      "\t\tFILE: Number of bytes written=361807\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=14682\n",
      "\t\tHDFS: Number of bytes written=255\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7001088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2180096\n",
      "\t\tTotal time spent by all map tasks (ms)=6837\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6837\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2129\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2129\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6837\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2129\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1460\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=232\n",
      "\t\tInput split bytes=364\n",
      "\t\tMap input records=326\n",
      "\t\tMap output bytes=272\n",
      "\t\tMap output materialized bytes=316\n",
      "\t\tMap output records=16\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=652144640\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=15\n",
      "\t\tReduce shuffle bytes=316\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=32\n",
      "\t\tTotal committed heap usage (bytes)=509083648\n",
      "\t\tVirtual memory (bytes) snapshot=6291603456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar8536001751018509176/] [] /tmp/streamjob7911927506365048491.jar tmpDir=null\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1463787494457_0343\n",
      "  Submitted application application_1463787494457_0343\n",
      "  The url to track the job: http://master:8088/proxy/application_1463787494457_0343/\n",
      "  Running job: job_1463787494457_0343\n",
      "  Job job_1463787494457_0343 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1463787494457_0343 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisitors.hadoop.20160606.073016.889574/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=383\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=170\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=306\n",
      "\t\tFILE: Number of bytes written=363410\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=715\n",
      "\t\tHDFS: Number of bytes written=170\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7351296\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2162688\n",
      "\t\tTotal time spent by all map tasks (ms)=7179\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7179\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2112\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2112\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7179\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2112\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1240\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=263\n",
      "\t\tInput split bytes=332\n",
      "\t\tMap input records=15\n",
      "\t\tMap output bytes=270\n",
      "\t\tMap output materialized bytes=312\n",
      "\t\tMap output records=15\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=645189632\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=312\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=30\n",
      "\t\tTotal committed heap usage (bytes)=504889344\n",
      "\t\tVirtual memory (bytes) snapshot=6288654336\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisitors.hadoop.20160606.073016.889574/output...\n",
      "\"(1001.10002)\"\t2\n",
      "\"(1000.10001)\"\t1\n",
      "\"(1001.10001)\"\t1\n",
      "\"(1002.10001)\"\t1\n",
      "\"(1003.10002)\"\t1\n",
      "\"(1004.10003)\"\t1\n",
      "\"(1005.10004)\"\t1\n",
      "\"(1006.10005)\"\t1\n",
      "\"(1007.10007)\"\t1\n",
      "\"(1008.10009)\"\t1\n",
      "STDERR: 16/06/06 02:31:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/MostFrequentVisitors.hadoop.20160606.073016.889574...\n",
      "Removing temp directory /tmp/MostFrequentVisitors.hadoop.20160606.073016.889574...\n"
     ]
    }
   ],
   "source": [
    "##### forget about sorting right now.  I think maybe the comma is messing it up because I have 2 commas there\n",
    "##### try getting it working without sorting then add that back in fixing the comma issue\n",
    "##### without sorting you can go back to local mode and things will run faster.\n",
    "\n",
    "# The job must be run with args '-r hadoop' to enable step-level jobconf\n",
    "#!python MostFrequentVisitors.py anonymous-msweb_converted_small.data\n",
    "!python MostFrequentVisitors.py -r hadoop anonymous-msweb_converted_small.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING:mrjob.hadoop:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.hadoop:  Job not successful!\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/05 14:30:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "ERROR:mrjob.fs.hadoop:STDERR: 16/06/05 14:30:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:mrjob.fs.hadoop:STDERR: ls: `hdfs:///tmp/hadoop-yarn/staging/userlogs/application_1463787494457_0316': No such file or directory\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Could not check path hdfs:///tmp/hadoop-yarn/staging/userlogs/application_1463787494457_0316",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-99aee6cbb873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmr_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMRMostFrequentVisitors\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'anonymous-msweb_converted_small.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hadoop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# stream_output and print each line of the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_job_files_for_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upload_local_files_to_hdfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job_in_hadoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_input_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_run_job_in_hadoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturncode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pick_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     log.error('Probable cause of failure:\\n\\n%s\\n' %\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/mixin.pyc\u001b[0m in \u001b[0;36m_pick_error\u001b[1;34m(self, log_interpretation)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_step_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_history_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_task_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pick_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interpretation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/mixin.pyc\u001b[0m in \u001b[0;36m_interpret_task_logs\u001b[1;34m(self, log_interpretation, partial)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 output_dir=output_dir),\n\u001b[0;32m    187\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             stderr_callback=_log_parsing_task_stderr)\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     def _ls_task_syslogs(\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/task.pyc\u001b[0m in \u001b[0;36m_interpret_task_logs\u001b[1;34m(fs, matches, partial, stderr_callback)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0msyslog_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/mixin.pyc\u001b[0m in \u001b[0;36m_ls_task_syslogs\u001b[1;34m(self, application_id, job_id, output_dir)\u001b[0m\n\u001b[0;32m    196\u001b[0m                     application_id=application_id, output_dir=output_dir),\n\u001b[0;32m    197\u001b[0m                 \u001b[0mapplication_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapplication_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                 job_id=job_id):\n\u001b[0m\u001b[0;32m    199\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  Parsing task syslog: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/task.pyc\u001b[0m in \u001b[0;36m_ls_task_syslogs\u001b[1;34m(fs, log_dir_stream, application_id, job_id)\u001b[0m\n\u001b[0;32m     67\u001b[0m     return _ls_logs(fs, log_dir_stream, _match_task_syslog_path,\n\u001b[0;32m     68\u001b[0m                     \u001b[0mapplication_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapplication_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                     job_id=job_id)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/logs/wrap.pyc\u001b[0m in \u001b[0;36m_ls_logs\u001b[1;34m(fs, log_dir_stream, matcher, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"couldn't ls() %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlog_dirs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlog_dir_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/hadoop.pyc\u001b[0m in \u001b[0;36m_stream_task_log_dirs\u001b[1;34m(self, application_id, output_dir)\u001b[0m\n\u001b[0;32m    585\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'userlogs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Looking for task syslogs in %s...'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/fs/composite.pyc\u001b[0m in \u001b[0;36mexists\u001b[1;34m(self, path_glob)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exists'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/mrjob/fs/composite.pyc\u001b[0m in \u001b[0;36m_do_action\u001b[1;34m(self, action, path, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't handle path: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mfirst_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: Could not check path hdfs:///tmp/hadoop-yarn/staging/userlogs/application_1463787494457_0316"
     ]
    }
   ],
   "source": [
    "from MostFrequentVisitors import MRMostFrequentVisitors\n",
    "# The job must be run with args '-r', 'hadoop' to enable step-level jobconf\n",
    "mr_job = MRMostFrequentVisitors (args=['anonymous-msweb_converted_small.data', '-r', 'hadoop'])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    counter = 0\n",
    "    # stream_output and print each line of the output\n",
    "    for line in runner.stream_output():\n",
    "        if counter < 5:\n",
    "            print mr_job.parse_output_line(line)\n",
    "        else:\n",
    "            break\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.5 Clustering Tweet Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will use a different dataset consisting of word-frequency distributions for 1,000 Twitter users. These Twitter users use language in very different ways, and were classified by hand according to the criteria:  \n",
    "* 0: Human, where only basic human-human communication is observed.\n",
    "* 1: Cyborg, where language is primarily borrowed from other sources (e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "* 2: Robot, where language is formulaically derived from unrelated sources (e.g., weather/seismology, police/fire event logs, etc...).\n",
    "* 3: Spammer, where language is replicated to high multiplicity (e.g., celebrity obsessions, personal promotion, etc... )  \n",
    "\n",
    "Check out the preprints of recent research, which spawned this dataset:  \n",
    "http://arxiv.org/abs/1505.04342 http://arxiv.org/abs/1508.01843  \n",
    "\n",
    "The main data lie in the accompanying file:  topUsers_Apr-Jul_2014_1000-words.txt  \n",
    "and are of the form:  \n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...  \n",
    "where  \n",
    "* USERID = unique user identifier\n",
    "* CODE = 0/1/2/3 class code\n",
    "* TOTAL = sum of the word counts  \n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users by their 1000-dimensional word stripes/vectors using several centroid initializations and values of K.  Note that each \"point\" is a user as represented by 1000 words, and that word-frequency distributions are generally heavy-tailed power-laws (often called Zipf distributions), and are very rare in the larger class of discrete, random distributions. For each user you will have to normalize by its \"TOTAL\" column. Try several parameterizations and initializations:  \n",
    "* (A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "* (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "* (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "* (D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the (row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).  \n",
    "Note that you do not have to compute the aggregated distribution or the class-aggregated distributions, which are rows in the auxiliary file: topUsers_Apr-Jul_2014_1000-words_summaries.txt  \n",
    "* Row 1: Words\n",
    "* Row 2: Aggregated distribution across all classes\n",
    "* Row 3-6 class-aggregated distributions for clases 0-3  \n",
    "For (A), we select 4 users randomly from a uniform distribution [1,...,1,000].  For (B), (C), and (D) you will have to use data from the auxiliary file:  topUsers_Apr-Jul_2014_1000-words_summaries.txt  \n",
    "\n",
    "This file contains 5 special word-frequency distributions:  \n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations in parts (B) and (C), and  \n",
    "(2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)  \n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate (after initially normalizing by its sum, which is also provided). So if in (B) you want to create 2 perturbations of the aggregate, start with (1), normalize, and generate 1000 random numbers uniformly from the unit interval (0,1) twice (for two centroids), using:  \n",
    "from numpy import random numbers = random.sample(1000)  \n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate, and then renormalize to obtain one of your aggregate-perturbed initial centroids.  \n",
    "\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached. After convergence, print out a summary of the classes present in each cluster. In particular, report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a normalized version of the data topUsers_Apr-Jul_2014_1000-words.txt file\n",
    "infile = \"topUsers_Apr-Jul_2014_1000-words.txt\"\n",
    "outfile = \"topUsers_Apr-Jul_2014_1000-words_normalized.txt\"\n",
    "with open(infile, 'r') as rf, open(outfile, 'w') as wf:\n",
    "    for line in rf.readlines():\n",
    "        splt = line.strip().split(',')\n",
    "        total = float(splt[2])\n",
    "        wf.write('%s,%s,%s,' % (splt[0], splt[1], splt[2]))\n",
    "        for i in range(3,len(splt)-1):\n",
    "            wf.write('%f,' % (float(splt[i])/total))\n",
    "        wf.write('%f' % (float(splt[len(splt)-1])/total))\n",
    "        wf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a normalized version of the data topUsers_Apr-Jul_2014_1000-words.txt file\n",
    "# Exclude the first 3 columns of the data.\n",
    "infile = \"topUsers_Apr-Jul_2014_1000-words.txt\"\n",
    "outfile = \"topUsers_Apr-Jul_2014_1000-words_normalized_only.txt\"\n",
    "with open(infile, 'r') as rf, open(outfile, 'w') as wf:\n",
    "    for line in rf.readlines():\n",
    "        splt = line.strip().split(',')\n",
    "        total = float(splt[2])\n",
    "        for i in range(3,len(splt)-1):\n",
    "            wf.write('%f,' % (float(splt[i])/total))\n",
    "        wf.write('%f' % (float(splt[len(splt)-1])/total))\n",
    "        wf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a normalized version of the data topUsers_Apr-Jul_2014_1000-words.txt file\n",
    "# Exclude the entire first row and the first 3 columns of the data.\n",
    "infile = \"topUsers_Apr-Jul_2014_1000-words_summaries.txt\"\n",
    "outfile = \"topUsers_Apr-Jul_2014_1000-words_summaries_normalized_only.txt\"\n",
    "with open(infile, 'r') as rf, open(outfile, 'w') as wf:\n",
    "    counter = 0\n",
    "    for line in rf.readlines():\n",
    "        if counter != 0:\n",
    "            splt = line.strip().split(',')\n",
    "            total = float(splt[2])\n",
    "            for i in range(3,len(splt)-1):\n",
    "                wf.write('%f,' % (float(splt[i])/total))\n",
    "            wf.write('%f' % (float(splt[len(splt)-1])/total))\n",
    "            wf.write('\\n')\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    #print(datapoint.shape)\n",
    "    centroid_points = array(centroid_points)\n",
    "    #print(centroid_points.shape)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=4\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        # This is the line that breaks things with multiple mappers\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        \n",
    "        #print \"Centroids: \", self.centroid_points\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        yield int(MinDist(D,self.centroid_points)), (D,1)\n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        num = 0\n",
    "        # ?? see if you can parameterize the 1000\n",
    "        sumD = [0.0]*1000\n",
    "        for D,n in inputdata:\n",
    "            num += n\n",
    "            sumD = [x + y for x, y in zip(sumD,D)]\n",
    "        yield idx,(sumD,num)\n",
    "        \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        # DOES THIS GET MESSED UP WITH MULTIPLE REDUCERS??\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        # ?? see if you can parameterize the 1000\n",
    "        for i in range(self.k):\n",
    "            centroids.append([0.0]*1000)\n",
    "        for D, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx] = [x + y for x, y in zip(centroids[idx],D)]\n",
    "        centroids[idx] = [i / float(num[idx]) for i in centroids[idx]]\n",
    "        \n",
    "        #print 'centroids updates:', centroids\n",
    "        \n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            #f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "            #f.writelines(','.join(str(j) for j in centroids[idx]) + '\\n')\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroids)\n",
    "        yield idx, centroids[idx]\n",
    "      \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Geneate initial centroids FOR PART A\n",
    "def centroid_init_A(k, filename):\n",
    "    centroid_points = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(k):\n",
    "            rint = random.randint(0, 999)\n",
    "            #rint = random.randint(0, 9)\n",
    "            rline = lines[rint].strip().split(',')\n",
    "            centroid_points.append([float(s) for s in rline[3:len(rline)]])\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    return centroid_points\n",
    "\n",
    "###################################################################################\n",
    "## Geneate random initial centroids around the global aggregate\n",
    "## Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "def centroid_init_BC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        # Note correction from Kevin from Boulder\n",
    "        if counter == 1:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroids)\n",
    "    return centroids\n",
    "\n",
    "def centroid_init_D(k, filename):\n",
    "    centroid_points = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(1,k+1):\n",
    "            rline = lines[i].strip().split(',')\n",
    "            centroid_points.append([float(s) for s in rline])\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    return centroid_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 2, len: 1000\n",
      "key: 3, len: 1000\n",
      "key: 0, len: 1000\n",
      "key: 1, len: 1000\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration5:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration6:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration7:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n",
      "iteration8:\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "Current path: /home/hadoop/w261-Assignments/hw4\n",
      "key: 0, len: 1000\n",
      "key: 3, len: 1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy import random\n",
    "from Kmeans import MRKmeans, stop_criterion, MinDist\n",
    "mr_job = MRKmeans(args=['topUsers_Apr-Jul_2014_1000-words_normalized_only.txt', '--file=Centroids.txt'])\n",
    "## how do I pass arguments to this??,  I need to pass k\n",
    "\n",
    "def write_centroids (centroids, hw, fnum, iter_num):\n",
    "    filename = 'centroid_results_' + hw + str(fnum) + '.txt'\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.write('Number of Iterations: %d\\n' % (iter_num))\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroids)\n",
    "\n",
    "HW_PART = 'D'\n",
    "k = 4\n",
    "if HW_PART == 'A':\n",
    "    centroid_points = centroid_init_A(k, 'topUsers_Apr-Jul_2014_1000-words_normalized.txt')\n",
    "elif HW_PART == 'B':\n",
    "    # Part B is the only part with k=2\n",
    "    k = 2\n",
    "    centroid_points = centroid_init_BC(k)\n",
    "elif HW_PART == 'C':\n",
    "    centroid_points = centroid_init_BC(k)\n",
    "elif HW_PART == 'D':\n",
    "    centroid_points = centroid_init_D(k, 'topUsers_Apr-Jul_2014_1000-words_summaries_normalized_only.txt')\n",
    "    \n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "#while(i<=2):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print('key: %d, len: %d' % (key, len(value)))\n",
    "            #print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i += 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    "\n",
    "#print \"Centroids\\n\"\n",
    "#for j in range(len(centroid_points)):\n",
    "#    print centroid_points[j]\n",
    "#    print\n",
    "\n",
    "# Write the centroid_points to a file\n",
    "write_centroids(centroid_points, HW_PART, 2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: 9\n",
      "Cluster 0: \n",
      "[1, 0, 12, 1]\n",
      "[0.0013297872340425532, 0.0, 0.2222222222222222, 0.009708737864077669]\n",
      "Cluster 1: \n",
      "[0, 52, 2, 0]\n",
      "[0.0, 0.5714285714285714, 0.037037037037037035, 0.0]\n",
      "Cluster 2: \n",
      "[1, 36, 38, 4]\n",
      "[0.0013297872340425532, 0.3956043956043956, 0.7037037037037037, 0.038834951456310676]\n",
      "Cluster 3: \n",
      "[750, 3, 2, 98]\n",
      "[0.9973404255319149, 0.03296703296703297, 0.037037037037037035, 0.9514563106796117]\n"
     ]
    }
   ],
   "source": [
    "from Kmeans import MinDist\n",
    "\n",
    "# Reads centroids from a file called filename and returns them as a list of lists\n",
    "def read_centroids (filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        num_iter = lines[0].strip().split()[3]\n",
    "        centroids = [map(float,s.split('\\n')[0].split(',')) for s in lines[1:]]\n",
    "    return centroids, num_iter\n",
    "\n",
    "centroid_points, num_iter = read_centroids('centroid_results_D2.txt')\n",
    "\n",
    "# Summarize the class\n",
    "# Initialize a results array\n",
    "total_codes = [752.0,91.0,54.0,103.0]\n",
    "results_a = []\n",
    "for i in range(len(centroid_points)):\n",
    "    results_a.append([0]*len(total_codes))\n",
    "with open('topUsers_Apr-Jul_2014_1000-words_normalized.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        rline = line.strip().split(',')\n",
    "        userid = rline[0]\n",
    "        code = int(rline[1])\n",
    "        D = [float(s) for s in rline[3:len(rline)]]\n",
    "        cluster = MinDist(D, centroid_points)\n",
    "        results_a[cluster][code] += 1\n",
    "\n",
    "results_b = []\n",
    "print('Number of Iterations: %s' % (num_iter))\n",
    "for i in range(len(results_a)):\n",
    "    results_b.append([x / y for x, y in zip(results_a[i],total_codes)])\n",
    "    print('Cluster %d: ' % (i))\n",
    "    print(results_a[i])\n",
    "    print(results_b[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### TEST CELL ####\n",
    "filename = \"topUsers_Apr-Jul_2014_1000-words_normalized.txt\"\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        s = line.split(',')\n",
    "        #print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "#### TEST CELL ####\n",
    "l1 = [1, 2, 3, 4]\n",
    "l2 = [5, 6, 7, 8]\n",
    "l3 = [x + y for x, y in zip(l1,l2)]\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(4, 1000)\n",
      "[[ -4.82270000e-02  -1.43480000e-02   3.33740000e-02 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  2.67210000e-02  -4.06280000e-02  -7.14600000e-03 ...,   0.00000000e+00\n",
      "   -8.30000000e-05  -8.30000000e-05]\n",
      " [  4.30300000e-02  -5.04220000e-02   8.33900000e-03 ...,   0.00000000e+00\n",
      "   -6.70000000e-04  -1.51000000e-04]\n",
      " [ -8.89080000e-02  -4.29300000e-03   3.34010000e-02 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#### TEST CELL ####\n",
    "from numpy import argmin, array, random\n",
    "from itertools import chain\n",
    "import os\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "\n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    print(datapoint.shape)\n",
    "    #print(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    print(centroid_points.shape)\n",
    "    #print(centroid_points)\n",
    "    diff = datapoint - centroid_points\n",
    "    print(diff)\n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Geneate initial centroids FOR PART A\n",
    "centroid_points = []\n",
    "k = 4\n",
    "with open('topUsers_Apr-Jul_2014_1000-words_normalized.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(k):\n",
    "        rint = random.randint(0, 999)\n",
    "        rline = lines[rint].strip().split(',')\n",
    "        centroid_points.append([float(s) for s in rline[3:len(rline)]])\n",
    "    rline = lines[0].strip().split(',')\n",
    "    datapoint = [float(s) for s in rline[3:len(rline)]]\n",
    "\n",
    "minidx = MinDist(datapoint, centroid_points)\n",
    "print(minidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_visitor_pairs = (page_id,visitor_id), 1\n",
    "page_id, visitor_id = page_visitor_pairs.strip('()').split(',', 2)\n",
    "print(page_id)\n",
    "print(visitor_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
