{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2-V2 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  2, revision\n",
    "* **Date:**  6/4/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2 Version 2 Question:** What would happen if you use two reducers for learning a NB classifier? How would that complicate things?  \n",
    "\n",
    "ANSWER:  If multiple reducers were used for the training of the model, then the following applies:\n",
    "* The total count of documents can just passed to one of the reducers.  This is only used for the calculation of the prior probabilities and that calculation is only done once and then the priors could be written to the model output.\n",
    "* The total count of words for each class would need to be sent to all reducers because when calculating the conditional probabilities for the words, it is divided by the total word counts of its class.\n",
    "* Since the MapReduce framework guarantees that all instances of the same key get sent to the same reducer, the solution will still be correct if the keys are broken up over multiple reducers.  As long as the total count of words is sent to each reducer, the conditional probabilities for each term will be correct.    \n",
    "\n",
    "One way to ensure that the count of words is sent to each reducer is to partition all of the words by prepending a label like 'a' and 'b' to each of the words.  And then prepending each label to the total count of words, so there would be an output 'a, total_word_count' and 'b, total_word_count'.  This way the total word count will be sent to the reducer that handles the 'a' keys and to the reducer that handles the 'b' keys.  If there were more than 2 reducers, more partitioning labels could be used.  As long as all of the labels are prepended to the total word counts the solution would work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.0.**  \n",
    "**HW2.0.1.** What is a race condition in the context of parallel computation? Give an example.  \n",
    "ANSWER:  A race condition is a potential software bug introduced when writing parallel computer programs (programs that do more than one thing at a time).  It is when the program does two tasks at the same time when the tasks actually need to be done in a particular sequence for the solution to be correct.  \n",
    "\n",
    "Example:  A website does a 5-minute data upload with new data every hour on the hour.  A user reads data from this website exactly 3 minutes after the hour every hour and the user expects the most up-to-date data.  This is a race condition.  Two things are being done at the same time:  writing and reading.  However, in order for the solution to be correct (that the user get the most up-to-date data), the program should finish writing the new data before it allows the user to read the data.\n",
    "\n",
    "**HW2.0.2.** What is MapReduce?  \n",
    "ANSWER:  MapReduce can refer to three distinct but related concepts:\n",
    "1. MapReduce is a programming model for processing and generating large data sets with a parallel, distributed algorithm on a cluster.\n",
    "2. MapReduce can refer to the execution framework that coordinates the execution of programs written in this particular style.\n",
    "3. MapReduce can refer to the software implementation of the programming model and the execution framework: for example, Google's proprietary implementation vs. the open-source Hadoop implementation in Java.\n",
    "\n",
    "**HW2.0.3.** How does it differ from Hadoop?  \n",
    "ANSWER:  MapReduce is the generic term for the programming model or framework.  Hadoop (or Apache Hadoop) implements a popular open-source version of the MapReduce framework.  Other organizations have created implementations in many programming languages for the MapReduce framework with different levels of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.0.1.** Which programming paradigm is Hadoop based on? Explain and give a simple example of functional programming in raw python code and show the code running. E.g., in raw python find the average length of a string in and of strings using a python \"map-reduce\" (functional programming) job (similar in style to the above). Alternatively, you can do this in python Hadoop Streaming.  strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]  \n",
    "\n",
    "ANSWER:  \n",
    "Average String Length: 10.000000\n",
    "\n",
    "**HW2.0.1.1** Which programming paradigm is Hadoop based on? Explain.  \n",
    "ANSWER:  Hadoop (or the MapReduce portion of it anyway) is based on functional programming.  Functional programming is a style of programming that avoids changing state and changing data.  The output of a functional program depends only on the input coming in.  There are no side effects (changes in state or data that do not depend on the inputs).  If you run a function more than once on the same inputs, then the function will produce the same outputs every time.  Hadoop is based on this programming model to facilitate parallelization of problems.  If the jobs that are being run in parallel are functional programs then it is known that no state or data is being changed.  Only the output from the program is returned.  If state and data are changed while programs are being run in parallel, this can create synchronization issues with incorrect state or data being used by some of the programs.  These synchronization issues are more complicated to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the location of the Hadoop Streaming jar\n",
    "hadoopStreamingJar =  '/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/20 22:10:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Create a directory in HDFS to store the data for this problem\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir /user\n",
    "!/usr/local/hadoop/bin/hdfs dfs -mkdir /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 09:20:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Read the strings.txt input file into HDFS\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal strings.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW2.0.1\n",
    "## This code reads the input file and writesthe length of each string for the reducer\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # write the length of the string for the mapper\n",
    "    print('%d' % (len(line.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW2.0.1\n",
    "## This code reads the lengths of strings from the mapper and sums them.\n",
    "## Then it divides that sum by the count of inputs that came from the mapper\n",
    "## to get an average length of the strings.\n",
    "\n",
    "import sys\n",
    "\n",
    "count=0\n",
    "total_length = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    total_length += int(line.strip())\n",
    "    count += 1\n",
    "average_length = total_length / float(count)\n",
    "print('Average String Length: %f' % (average_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:33:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:33:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-0-1/_SUCCESS\n",
      "16/06/04 13:33:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-0-1/part-00000\n",
      "16/06/04 13:33:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:33:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar2614560373457695083/] [] /tmp/streamjob7666007817116497229.jar tmpDir=null\n",
      "16/06/04 13:33:24 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:33:24 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:33:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:33:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:33:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0272\n",
      "16/06/04 13:33:25 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0272\n",
      "16/06/04 13:33:25 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0272/\n",
      "16/06/04 13:33:25 INFO mapreduce.Job: Running job: job_1463787494457_0272\n",
      "16/06/04 13:33:30 INFO mapreduce.Job: Job job_1463787494457_0272 running in uber mode : false\n",
      "16/06/04 13:33:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:33:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:33:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:33:40 INFO mapreduce.Job: Job job_1463787494457_0272 completed successfully\n",
      "16/06/04 13:33:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27\n",
      "\t\tFILE: Number of bytes written=355850\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=244\n",
      "\t\tHDFS: Number of bytes written=34\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6761\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1953\n",
      "\t\tTotal time spent by all map tasks (ms)=6761\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1953\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6761\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1953\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6923264\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1999872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=13\n",
      "\t\tMap output materialized bytes=33\n",
      "\t\tInput split bytes=178\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=33\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=204\n",
      "\t\tCPU time spent (ms)=1490\n",
      "\t\tPhysical memory (bytes) snapshot=651456512\n",
      "\t\tVirtual memory (bytes) snapshot=6295502848\n",
      "\t\tTotal committed heap usage (bytes)=506986496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=66\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "16/06/04 13:33:41 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-0-1\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-0-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-0-1\n",
    "# Run a Hadoop streaming job.  The input file 'strings.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/strings.txt -output /user/hadoop/outputHW2-0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:34:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-04 13:33 /user/hadoop/outputHW2-0-1/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup         34 2016-06-04 13:33 /user/hadoop/outputHW2-0-1/part-00000\n",
      "16/06/04 13:34:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Average String Length: 10.000000\t\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-0-1\n",
    "# Print the output file\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-0-1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.1.** Sort in Hadoop MapReduce.  **Given as input:** Records of the form (integer, “NA”), where integer is any integer, and “NA” is just the empty string.  **Output:** sorted key value pairs of the form (integer, “NA”) in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "**HW2.1.1.** What happens if you have multiple reducers? Do you need additional steps? Explain.  \n",
    "ANSWER:  If there are multiple reducers, then there will be multiple output files - one output file for each reducer.  Even though this is the case, additional MapReduce jobs are not required for a correct solution.  The summary of the solution is as follows.  The mapper can be programmed to add a tag to the front of each of its outputs.  This will be the new key.  The tag will be the partition rule.  The tag is assigned according to how the integers should be routed to the reducers (example:  an A could be assigned for integers less than 0 and a B could be assigned for integers greater than or equal to 0.).  Then in the Hadoop shuffle phase everything with the same tag will be routed to the same reducer.  In this way, the programmer can ensure that all the integers of a certain range (example:  -infinity to 0) are routed to one reducer and all of the integers of other ranges (example:  0 to infinity) are routed to other reducers.  When the integers are routed by range in this fashion, then all of the output files from the reducers can be aggregated for each range and they will all be sorted correctly.  Once the integers are sent to the correct reducer, then in the Hadoop shuffle phase, they can be sorted by value (this would be the 2nd element of the key, value pair).  Then when they arrive at the reducer, they will already be in sorted order, so the reducer will just need to print them out.  All of the sorting can still be done in the Hadoop shuffle phase even if there are multiple reducers.\n",
    "\n",
    "**Part 1** Write code to generate N random records of the form (integer, “NA”). Let N = 10,000.\n",
    "\n",
    "**Part 2** Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers.  (See output below.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part 1.  Write code to generate N  random records of the form (integer, “NA”). Let N = 10,000.\n",
    "# This code generates 10,000 numbers in the range of [-10,000, 10,000] and writes it to a file called\n",
    "# \"random_integers.txt\".  It uses the Python library random to generate the random integers.\n",
    "import random\n",
    "\n",
    "N=10000\n",
    "min_int = -100000\n",
    "max_int = 100000\n",
    "filename = \"random_integers.txt\"\n",
    "with open(filename, 'w') as f:\n",
    "    for i in range(N):\n",
    "        f.write('<%d, \"\">\\n' % (random.randint(min_int, max_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/20 22:11:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-20 22:10 /user/hadoop/outputHW2-1\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the random_integers.txt input file into HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal random_integers.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/20 22:33:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure the file is there.\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW2.1.\n",
    "## This code reads the input file and writes each integer to STDOUT for the reducer to use.\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove the angle brackets\n",
    "    line = line.strip('<>\\n')\n",
    "    # split the line into words\n",
    "    words = line.split(',')\n",
    "    # write the results to STDOUT (standard output);\n",
    "    # what we output here will be the input for the\n",
    "    # Reduce step, i.e. the input for reducer.py\n",
    "    print('%s' % (words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW2.1.\n",
    "## This code prints out the biggest 10 and smallest 10 numbers from the input.\n",
    "## The Hadoop shuffle phase already sorts the inputs, so the numbers are already sorted\n",
    "## when they reach the reducer.  The reducer prints the 1st 10 numbers it encounters which\n",
    "## would be the largest and the last 10 numbers it encounters which would be the smallest.\n",
    "\n",
    "import sys\n",
    "\n",
    "N=10000\n",
    "i=0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    if (i>=0 and i<10) or (i>=(N-10) and i<N):\n",
    "        print('<%s, \"\">' % (line.strip()))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:34:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:34:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-1/_SUCCESS\n",
      "16/06/04 13:34:55 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-1/part-00000\n",
      "16/06/04 13:34:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:34:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5419556954724451299/] [] /tmp/streamjob1811880082683433351.jar tmpDir=null\n",
      "16/06/04 13:34:58 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:34:58 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:34:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:34:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:34:58 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/06/04 13:34:58 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/06/04 13:34:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0273\n",
      "16/06/04 13:34:58 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0273\n",
      "16/06/04 13:34:58 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0273/\n",
      "16/06/04 13:34:58 INFO mapreduce.Job: Running job: job_1463787494457_0273\n",
      "16/06/04 13:35:02 INFO mapreduce.Job: Job job_1463787494457_0273 running in uber mode : false\n",
      "16/06/04 13:35:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:35:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:35:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:35:14 INFO mapreduce.Job: Job job_1463787494457_0273 completed successfully\n",
      "16/06/04 13:35:14 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=93856\n",
      "\t\tFILE: Number of bytes written=544495\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=127655\n",
      "\t\tHDFS: Number of bytes written=270\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6824\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2412\n",
      "\t\tTotal time spent by all map tasks (ms)=6824\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2412\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6824\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2412\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6987776\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2469888\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=73850\n",
      "\t\tMap output materialized bytes=93862\n",
      "\t\tInput split bytes=194\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9762\n",
      "\t\tReduce shuffle bytes=93862\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=211\n",
      "\t\tCPU time spent (ms)=1850\n",
      "\t\tPhysical memory (bytes) snapshot=659443712\n",
      "\t\tVirtual memory (bytes) snapshot=6294286336\n",
      "\t\tTotal committed heap usage (bytes)=500170752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=127461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=270\n",
      "16/06/04 13:35:14 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-1\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-1\n",
    "# Run a Hadoop streaming job.  The input file 'random_integers.txt' is passed as input.\n",
    "# The '-D mapred.text.key.comparator.options=-nr' means that in the Hadoop shuffle the keys will be treated as numbers (-n)\n",
    "# and not strings and the sort order (-r) will be descending and not ascending.\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=-nr \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/random_integers.txt -output /user/hadoop/outputHW2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:35:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-04 13:35 /user/hadoop/outputHW2-1/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup        270 2016-06-04 13:35 /user/hadoop/outputHW2-1/part-00000\n",
      "16/06/04 13:35:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "<99999, \"\">\t\n",
      "<99995, \"\">\t\n",
      "<99982, \"\">\t\n",
      "<99974, \"\">\t\n",
      "<99969, \"\">\t\n",
      "<99942, \"\">\t\n",
      "<99929, \"\">\t\n",
      "<99892, \"\">\t\n",
      "<99887, \"\">\t\n",
      "<99882, \"\">\t\n",
      "<-99775, \"\">\t\n",
      "<-99842, \"\">\t\n",
      "<-99867, \"\">\t\n",
      "<-99889, \"\">\t\n",
      "<-99912, \"\">\t\n",
      "<-99939, \"\">\t\n",
      "<-99943, \"\">\t\n",
      "<-99961, \"\">\t\n",
      "<-99984, \"\">\t\n",
      "<-99994, \"\">\t\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-1\n",
    "# Print the output file which contains the sorted numbers\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.2.**  WORDCOUNT.  Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "       \n",
    "ANSWER:  Word count for the word \"assistance\" is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 1 code for HW2.2\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    if len(items) > 1:\n",
    "        # Ignore the 1st 2 values of the line, the ID and TRUTH value as they will not be counted\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_all = re.findall(WORD_RE, items[2].lower())\n",
    "        # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        if len(items) == 4:\n",
    "            words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    else:\n",
    "        # This handles line 60 which is only a partial input line and contains only one item\n",
    "        words_all = re.findall(WORD_RE, items[0].lower())\n",
    "    # Loop through each of the terms in the words of this line and add the counts to the \n",
    "    # total word counts.\n",
    "    for term in words_all:\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = 1\n",
    "        else:\n",
    "            count_vocab_terms[term] += 1\n",
    "            \n",
    "# Print each term in the vocab and then its count\n",
    "for term in count_vocab_terms:\n",
    "    print('%s\\t%d' % (term, count_vocab_terms[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "\n",
    "# Initialize a dictionary to store the term counts\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    items = line.split(\"\\t\")\n",
    "    # The 1st value in the line is the term\n",
    "    term = items[0]\n",
    "    # Read the term counts from the mapper and aggregate them.\n",
    "    if term not in count_vocab_terms:\n",
    "        count_vocab_terms[term] = int(items[1])\n",
    "    else:\n",
    "        count_vocab_terms[term] += int(items[1])\n",
    "\n",
    "# Print each term in the vocab and then its count\n",
    "for term in count_vocab_terms:\n",
    "    print('%s\\t%d' % (term, count_vocab_terms[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:35:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:35:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-2/_SUCCESS\n",
      "16/06/04 13:35:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-2/part-00000\n",
      "16/06/04 13:35:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:35:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar144073123672936299/] [] /tmp/streamjob4167535583384888779.jar tmpDir=null\n",
      "16/06/04 13:35:49 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:35:49 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:35:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:35:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:35:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0274\n",
      "16/06/04 13:35:51 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0274\n",
      "16/06/04 13:35:51 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0274/\n",
      "16/06/04 13:35:51 INFO mapreduce.Job: Running job: job_1463787494457_0274\n",
      "16/06/04 13:35:55 INFO mapreduce.Job: Job job_1463787494457_0274 running in uber mode : false\n",
      "16/06/04 13:35:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:36:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:36:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:36:06 INFO mapreduce.Job: Job job_1463787494457_0274 completed successfully\n",
      "16/06/04 13:36:06 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=81448\n",
      "\t\tFILE: Number of bytes written=518707\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=53483\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6829\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2382\n",
      "\t\tTotal time spent by all map tasks (ms)=6829\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2382\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6829\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2382\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6992896\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2439168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=7061\n",
      "\t\tMap output bytes=67320\n",
      "\t\tMap output materialized bytes=81454\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5490\n",
      "\t\tReduce shuffle bytes=81454\n",
      "\t\tReduce input records=7061\n",
      "\t\tReduce output records=5490\n",
      "\t\tSpilled Records=14122\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=189\n",
      "\t\tCPU time spent (ms)=1640\n",
      "\t\tPhysical memory (bytes) snapshot=650088448\n",
      "\t\tVirtual memory (bytes) snapshot=6295896064\n",
      "\t\tTotal committed heap usage (bytes)=506986496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53483\n",
      "16/06/04 13:36:06 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-2\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-2/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-2\n",
    "# Run a Hadoop streaming job.  The input file 'enronemail_1h.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/25 23:33:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-25 23:33 /user/hadoop/outputHW2-2/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup      53483 2016-05-25 23:33 /user/hadoop/outputHW2-2/part-00000\n",
      "16/05/25 23:33:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "yellow\t1\n",
      "four\t8\n",
      "prefix\t1\n",
      "railing\t1\n",
      "looking\t4\n",
      "granting\t1\n",
      "electricity\t1\n",
      "originality\t1\n",
      "homemakers\t1\n",
      "hormone\t2\n",
      "regional\t11\n",
      "screaming\t1\n",
      "barraged\t1\n",
      "internally\t2\n",
      "prize\t1\n",
      "customizable\t1\n",
      "wednesday\t6\n",
      "cyberopps\t2\n",
      "charter\t2\n",
      "tired\t2\n",
      "miller\t1\n",
      "bacon\t1\n",
      "tires\t3\n",
      "second\t4\n",
      "errors\t6\n",
      "thunder\t1\n",
      "daysor\t1\n",
      "bonuses\t4\n",
      "increasing\t4\n",
      "duns\t2\n",
      "mailings\t2\n",
      "here\t55\n",
      "china\t2\n",
      "borrowers\t1\n",
      "k\t4\n",
      "reports\t1\n",
      "oxymoron\t1\n",
      "i'd\t2\n",
      "i'm\t7\n",
      "explained\t1\n",
      "lengthen\t1\n",
      "replace\t2\n",
      "brought\t1\n",
      "stern\t1\n",
      "055\t2\n",
      "txu\t2\n",
      "classifieds\t1\n",
      "unit\t7\n",
      "derivatives\t3\n",
      "spoke\t2\n",
      "dnb\t1\n",
      "catchy\t1\n",
      "music\t1\n",
      "therefore\t7\n",
      "passport\t1\n",
      "until\t17\n",
      "'swbe\t1\n",
      "successful\t17\n",
      "brings\t5\n",
      "yahoo\t7\n",
      "99\t64\n",
      "98\t1\n",
      "90\t3\n",
      "hold\t3\n",
      "95\t15\n",
      "notis\t2\n",
      "96\t1\n",
      "espcially\t1\n",
      "example\t7\n",
      "currency\t1\n",
      "reviewing\t1\n",
      "want\t36\n",
      "absolute\t2\n",
      "travel\t3\n",
      "feature\t1\n",
      "machine\t1\n",
      "how\t36\n",
      "hot\t2\n",
      "hou\t206\n",
      "significance\t1\n",
      "turk\t3\n",
      "wrong\t3\n",
      "chums\t1\n",
      "siberia\t1\n",
      "vance\t2\n",
      "hzriubp\t1\n",
      "effective\t11\n",
      "baggage\t1\n",
      "democratic\t2\n",
      "wing\t1\n",
      "wind\t1\n",
      "kvie\t1\n",
      "vary\t1\n",
      "kickoff\t2\n",
      "fit\t2\n",
      "rankings\t1\n",
      "striking\t1\n",
      "pilkington\t1\n",
      "davis's\t1\n",
      "shenkman\t1\n",
      "accrual\t2\n",
      "effects\t1\n",
      "ezine______________________________________________________\t1\n",
      "undeveloped\t1\n",
      "silver\t1\n",
      "windowsentities\t1\n",
      "interfering\t1\n",
      "development\t14\n",
      "financial\t12\n",
      "secret's\t2\n",
      "392\t2\n",
      "we'd\t1\n",
      "399\t1\n",
      "message\t20\n",
      "uncollected\t1\n",
      "rw\t1\n",
      "rx\t1\n",
      "0281\t1\n",
      "rd\t1\n",
      "re\t30\n",
      "encourage\t4\n",
      "rm\t1\n",
      "strategist\t2\n",
      "managerial\t2\n",
      "estimate\t2\n",
      "enormous\t1\n",
      "ate\t3\n",
      "litteneker\t1\n",
      "shipped\t1\n",
      "today's\t6\n",
      "speeds\t1\n",
      "nigel\t1\n",
      "52109\t1\n",
      "wash\t1\n",
      "instruct\t1\n",
      "clarity\t1\n",
      "cheaep\t1\n",
      "service\t48\n",
      "needed\t9\n",
      "master\t3\n",
      "listed\t7\n",
      "gilbert\t1\n",
      "legs\t3\n",
      "profitbanners\t4\n",
      "listen\t2\n",
      "intelt\t1\n",
      "willbe\t1\n",
      "tree\t2\n",
      "project\t18\n",
      "feeling\t3\n",
      "acquisition\t1\n",
      "affairs\t1\n",
      "responsible\t4\n",
      "eiben\t1\n",
      "taxation\t1\n",
      "uafn\t1\n",
      "shall\t9\n",
      "limitless\t2\n",
      "enron's\t2\n",
      "microsale\t8\n",
      "letter\t27\n",
      "organization\t6\n",
      "morality\t1\n",
      "bradford\t1\n",
      "snooping\t1\n",
      "emailing\t2\n",
      "professor\t1\n",
      "camp\t7\n",
      "teco\t2\n",
      "came\t2\n",
      "saying\t4\n",
      "meetings\t7\n",
      "nominated\t4\n",
      "'cenochs\t1\n",
      "participate\t4\n",
      "tempted\t1\n",
      "busy\t1\n",
      "layout\t3\n",
      "louise\t8\n",
      "headline\t2\n",
      "than\t32\n",
      "touched\t1\n",
      "rich\t1\n",
      "emphatically\t1\n",
      "alhaji\t2\n",
      "cushion\t1\n",
      "advertisments\t1\n",
      "net\t35\n",
      "volatility\t4\n",
      "'celias\t1\n",
      "release\t1\n",
      "respond\t8\n",
      "kfbtyra\t1\n",
      "fair\t2\n",
      "branding\t8\n",
      "'singleton\t1\n",
      "result\t2\n",
      "clem\t1\n",
      "financials\t1\n",
      "oceania\t1\n",
      "pressured\t2\n",
      "lots\t2\n",
      "delphi\t1\n",
      "score\t1\n",
      "timesheets\t1\n",
      "screwball\t1\n",
      "men\t5\n",
      "extend\t1\n",
      "nature\t2\n",
      "783019\t1\n",
      "affiliates\t6\n",
      "extent\t1\n",
      "debt\t12\n",
      "country\t23\n",
      "heating\t2\n",
      "sap's\t1\n",
      "adapted\t1\n",
      "asked\t3\n",
      "pre\t3\n",
      "mossel\t1\n",
      "250\t1\n",
      "256\t1\n",
      "dycmpf\t1\n",
      "union\t2\n",
      "twain\t1\n",
      "64610\t2\n",
      "much\t25\n",
      "100038\t1\n",
      "michele\t1\n",
      "location\t11\n",
      "life\t12\n",
      "eastern\t1\n",
      "dave\t9\n",
      "lift\t1\n",
      "chile\t1\n",
      "doubts\t1\n",
      "worked\t3\n",
      "diverted\t1\n",
      "strangas\t1\n",
      "brainstorm\t1\n",
      "played\t1\n",
      "ycon\t2\n",
      "elmira\t1\n",
      "player\t2\n",
      "crespigny\t2\n",
      "trusted\t2\n",
      "hone\t1\n",
      "things\t8\n",
      "damages\t1\n",
      "european\t2\n",
      "'njwa\t1\n",
      "workforce\t1\n",
      "josey\t7\n",
      "ownership\t2\n",
      "hrice\t2\n",
      "melodick\t1\n",
      "corporate\t8\n",
      "capitol\t1\n",
      "succeeding\t1\n",
      "previous\t9\n",
      "shiip\t2\n",
      "alum\t1\n",
      "enters\t1\n",
      "had\t20\n",
      "easy\t16\n",
      "ramupgradeable\t1\n",
      "has\t67\n",
      "keeley\t2\n",
      "osman\t4\n",
      "disagreement\t1\n",
      "possible\t17\n",
      "possibly\t2\n",
      "unique\t2\n",
      "desire\t3\n",
      "remind\t2\n",
      "steps\t1\n",
      "squill\t1\n",
      "right\t17\n",
      "old\t7\n",
      "crowd\t2\n",
      "people\t38\n",
      "8038\t1\n",
      "attendees\t2\n",
      "for\t373\n",
      "bottom\t2\n",
      "individuals\t4\n",
      "foo\t1\n",
      "zaako\t1\n",
      "cochilco\t1\n",
      "losing\t2\n",
      "valium\t3\n",
      "marketers\t1\n",
      "dollars\t7\n",
      "citizens\t2\n",
      "o\t5\n",
      "disqualified\t1\n",
      "slightly\t1\n",
      "raised\t2\n",
      "statements\t1\n",
      "martensite\t1\n",
      "facility\t1\n",
      "son\t8\n",
      "respectable\t1\n",
      "sos\t1\n",
      "raises\t2\n",
      "4748\t1\n",
      "support\t16\n",
      "constantly\t1\n",
      "114427\t1\n",
      "joseph\t7\n",
      "authorized\t1\n",
      "resulted\t2\n",
      "call\t15\n",
      "jana\t3\n",
      "happy\t4\n",
      "offer\t26\n",
      "'ryanmcgeachie\t1\n",
      "hemisphere\t2\n",
      "encarta\t1\n",
      "for'sponsoring\t1\n",
      "congratulations\t6\n",
      "inside\t2\n",
      "pest\t1\n",
      "examples\t1\n",
      "151\t1\n",
      "tonne\t1\n",
      "proven\t1\n",
      "exist\t2\n",
      "accounting\t5\n",
      "solicitation\t2\n",
      "floor\t4\n",
      "actor\t1\n",
      "flood\t2\n",
      "quickens\t1\n",
      "role\t18\n",
      "roll\t1\n",
      "intend\t3\n",
      "lollipops\t1\n",
      "invested\t1\n",
      "time\t44\n",
      "weeks's\t2\n",
      "dextails\t1\n",
      "92\t1\n",
      "dahlienweg\t1\n",
      "97\t1\n",
      "platitudes\t1\n",
      "poors\t1\n",
      "circumstances\t1\n",
      "816\t2\n",
      "jauregui\t1\n",
      "consolidation\t6\n",
      "choice\t2\n",
      "exact\t2\n",
      "minute\t4\n",
      "tear\t1\n",
      "leave\t4\n",
      "solved\t1\n",
      "team\t19\n",
      "loads\t1\n",
      "prevent\t1\n",
      "sign\t4\n",
      "trisha\t1\n",
      "bc\t1\n",
      "current\t9\n",
      "axel\t2\n",
      "dxqrgu\t1\n",
      "boost\t1\n",
      "alsdorf\t1\n",
      "understanding\t9\n",
      "mcbeal\t1\n",
      "address\t27\n",
      "alone\t2\n",
      "along\t4\n",
      "benson\t1\n",
      "hurtling\t1\n",
      "impacted\t1\n",
      "funded\t1\n",
      "absenteeism\t1\n",
      "myers\t1\n",
      "love\t2\n",
      "pentium\t1\n",
      "ipp\t2\n",
      "relating\t2\n",
      "working\t10\n",
      "angry\t1\n",
      "wood\t1\n",
      "wondering\t1\n",
      "scope\t2\n",
      "introducing\t1\n",
      "afford\t1\n",
      "sierra\t5\n",
      "visual\t1\n",
      "davis'end\t1\n",
      "valued\t1\n",
      "originally\t1\n",
      "zolam\t2\n",
      "believes\t1\n",
      "believed\t2\n",
      "remitted\t1\n",
      "awesome\t1\n",
      "234\t1\n",
      "bgmlm\t1\n",
      "230\t1\n",
      "agendas\t1\n",
      "divided\t1\n",
      "thanking\t3\n",
      "25711\t2\n",
      "sotware\t1\n",
      "explorer\t3\n",
      "spot\t5\n",
      "unclaimed\t1\n",
      "improving\t1\n",
      "such\t9\n",
      "waggons\t1\n",
      "stress\t5\n",
      "surfing\t1\n",
      "natural\t7\n",
      "ss\t1\n",
      "sr\t1\n",
      "sp\t1\n",
      "st\t3\n",
      "so\t60\n",
      "sc\t5\n",
      "sa\t4\n",
      "pulled\t1\n",
      "webpage\t4\n",
      "years\t22\n",
      "course\t11\n",
      "engageenergy\t1\n",
      "tejones\t2\n",
      "jim\t3\n",
      "decreases\t1\n",
      "suspicion\t1\n",
      "932\t2\n",
      "troubled\t1\n",
      "into\t36\n",
      "instantly\t2\n",
      "matches\t4\n",
      "origination\t1\n",
      "aeopublishing\t29\n",
      "insomnia\t1\n",
      "subscribers\t1\n",
      "quarter\t1\n",
      "stalling\t1\n",
      "revised\t2\n",
      "breakthrough\t1\n",
      "sponsor\t4\n",
      "entering\t2\n",
      "793\t1\n",
      "9381\t1\n",
      "ilug\t3\n",
      "sibilant\t1\n",
      "internet\t30\n",
      "formula\t2\n",
      "'rsbaker\t1\n",
      "ckgby\t1\n",
      "undercollection\t4\n",
      "million\t17\n",
      "possibility\t4\n",
      "quite\t4\n",
      "complicated\t3\n",
      "grandma\t1\n",
      "remainder\t2\n",
      "marla\t1\n",
      "training\t9\n",
      "bulleted\t2\n",
      "dunns\t1\n",
      "routes\t1\n",
      "emotion\t1\n",
      "saving\t2\n",
      "8507\t1\n",
      "spoken\t2\n",
      "kollaros\t1\n",
      "one\t67\n",
      "submit\t10\n",
      "vote\t7\n",
      "open\t7\n",
      "city\t5\n",
      "'wisew\t1\n",
      "2\t80\n",
      "draft\t2\n",
      "williams\t7\n",
      "tuneful\t1\n",
      "michelago\t2\n",
      "ridiculous\t1\n",
      "surged\t1\n",
      "whom\t1\n",
      "future\t14\n",
      "individualized\t3\n",
      "damned\t1\n",
      "addressing\t1\n",
      "illness\t1\n",
      "san\t4\n",
      "sam\t3\n",
      "turned\t1\n",
      "argument\t2\n",
      "prestigious\t1\n",
      "say\t6\n",
      "allen\t5\n",
      "turner\t3\n",
      "sap\t16\n",
      "saw\t1\n",
      "sat\t1\n",
      "aside\t4\n",
      "instructed\t3\n",
      "note\t9\n",
      "intends\t2\n",
      "take\t19\n",
      "gibner\t8\n",
      "wanting\t4\n",
      "mayes\t1\n",
      "printer\t23\n",
      "opposite\t1\n",
      "otc's\t1\n",
      "pages\t9\n",
      "lawn\t3\n",
      "eprm\t1\n",
      "average\t4\n",
      "drive\t2\n",
      "managing\t1\n",
      "bade\t1\n",
      "axe\t1\n",
      "salt\t1\n",
      "sylg\t1\n",
      "merchandisethat\t1\n",
      "aggressive\t1\n",
      "slow\t3\n",
      "transact\t1\n",
      "tears\t1\n",
      "going\t27\n",
      "assistant\t2\n",
      "platte\t1\n",
      "settingt\t1\n",
      "krishna\t1\n",
      "resource\t5\n",
      "bombs\t3\n",
      "uncover\t1\n",
      "where\t13\n",
      "morayt\t1\n",
      "surgery\t1\n",
      "receipt\t6\n",
      "harvey\t4\n",
      "sites\t31\n",
      "sponsorship\t9\n",
      "listinfo\t3\n",
      "testimonials\t2\n",
      "screen\t1\n",
      "westerngas\t1\n",
      "spare\t1\n",
      "______________________________________________________your\t1\n",
      "spark\t2\n",
      "classrom\t1\n",
      "many\t24\n",
      "personal\t12\n",
      "s\t40\n",
      "expended\t1\n",
      "3000\t2\n",
      "ante\t1\n",
      "suppressant\t1\n",
      "stretch\t3\n",
      "west\t5\n",
      "locally\t2\n",
      "vacation\t2\n",
      "breath\t5\n",
      "wants\t2\n",
      "enable\t4\n",
      "gist\t2\n",
      "thousand\t2\n",
      "antivirus\t1\n",
      "'dmao\t1\n",
      "davis'attempt\t1\n",
      "consultant\t3\n",
      "single\t4\n",
      "fiducial\t1\n",
      "situation\t4\n",
      "spotlight\t1\n",
      "ascii\t1\n",
      "fame\t3\n",
      "expenditures\t1\n",
      "31615304791\t1\n",
      "'travis\t1\n",
      "95394\t1\n",
      "visually\t1\n",
      "thousainds\t1\n",
      "cheers\t1\n",
      "harris\t2\n",
      "advertisement\t4\n",
      "wpd\t2\n",
      "renewable\t2\n",
      "costs\t2\n",
      "summer\t5\n",
      "212\t3\n",
      "213\t1\n",
      "being\t14\n",
      "rest\t5\n",
      "weekly\t4\n",
      "instrument\t1\n",
      "around\t6\n",
      "assemblyman\t3\n",
      "kindall\t4\n",
      "world\t10\n",
      "postal\t1\n",
      "discarded\t3\n",
      "hcokyovrdsprayz\t1\n",
      "couldn't\t1\n",
      "presentation\t1\n",
      "lobster\t1\n",
      "tithable\t1\n",
      "adrianbold\t2\n",
      "noon\t1\n",
      "refer\t2\n",
      "scientific\t1\n",
      "power\t24\n",
      "retains\t3\n",
      "leadership\t13\n",
      "package\t4\n",
      "industry\t9\n",
      "'richarddaniel\t1\n",
      "favorite\t7\n",
      "newsboy\t2\n",
      "neighbor\t1\n",
      "act\t7\n",
      "johnston\t2\n",
      "luck\t2\n",
      "other\t43\n",
      "burning\t2\n",
      "'prez\t1\n",
      "image\t9\n",
      "parties\t4\n",
      "her\t12\n",
      "philadelphia\t1\n",
      "brazilian\t4\n",
      "freepicklotto\t2\n",
      "continents\t1\n",
      "with\t200\n",
      "buying\t1\n",
      "abused\t1\n",
      "pull\t5\n",
      "7168\t1\n",
      "site\t60\n",
      "vasant\t5\n",
      "trips\t1\n",
      "glenda\t1\n",
      "detailed\t4\n",
      "630\t1\n",
      "ab\t5\n",
      "ad\t31\n",
      "johnny\t2\n",
      "exhausted\t1\n",
      "certain\t1\n",
      "am\t86\n",
      "al\t2\n",
      "an\t77\n",
      "as\t138\n",
      "at\t146\n",
      "sitara\t3\n",
      "ax\t2\n",
      "allay\t1\n",
      "coreldraw\t2\n",
      "eworld\t4\n",
      "gifts\t2\n",
      "herbs\t1\n",
      "terry\t1\n",
      "resell\t1\n",
      "tricky\t1\n",
      "cpm\t6\n",
      "having\t6\n",
      "mass\t3\n",
      "original\t3\n",
      "externally\t2\n",
      "consider\t4\n",
      "riedel\t1\n",
      "beware\t1\n",
      "causey\t7\n",
      "andrea\t3\n",
      "tx\t3\n",
      "tw\t1\n",
      "tr\t2\n",
      "to\t963\n",
      "tm\t1\n",
      "chewing\t2\n",
      "th\t14\n",
      "fishbeck\t1\n",
      "ghana\t2\n",
      "case\t13\n",
      "returned\t2\n",
      "146907159\t1\n",
      "plushy\t1\n",
      "'mcclankg\t1\n",
      "labor\t2\n",
      "existing\t5\n",
      "condition\t2\n",
      "joined\t3\n",
      "large\t8\n",
      "sang\t1\n",
      "sand\t1\n",
      "small\t3\n",
      "paso\t2\n",
      "past\t3\n",
      "pass\t2\n",
      "investment\t18\n",
      "richard\t2\n",
      "forestall\t1\n",
      "section\t4\n",
      "tanya\t4\n",
      "method\t1\n",
      "full\t28\n",
      "hash\t1\n",
      "hours\t26\n",
      "9489\t1\n",
      "australiasds\t1\n",
      "resourcestocks\t1\n",
      "concluding\t2\n",
      "november\t3\n",
      "compliance\t1\n",
      "experience\t17\n",
      "prior\t8\n",
      "pick\t3\n",
      "action\t1\n",
      "warrant\t1\n",
      "vic\t2\n",
      "via\t13\n",
      "followed\t2\n",
      "attendance\t2\n",
      "freese\t1\n",
      "6\t24\n",
      "more\t65\n",
      "door\t5\n",
      "company\t22\n",
      "bank's\t1\n",
      "krill\t1\n",
      "keeping\t4\n",
      "kaminski\t11\n",
      "science\t1\n",
      "sent\t16\n",
      "learn\t4\n",
      "facelift\t1\n",
      "chairing\t1\n",
      "male\t3\n",
      "prompt\t1\n",
      "impacts\t1\n",
      "stated\t2\n",
      "rodriguez\t2\n",
      "suggestions\t5\n",
      "accept\t3\n",
      "states\t5\n",
      "net's\t1\n",
      "prbolem\t1\n",
      "sense\t3\n",
      "huge\t2\n",
      "respective\t2\n",
      "62163\t1\n",
      "timotheus\t1\n",
      "dismissed\t1\n",
      "plant\t2\n",
      "intended\t1\n",
      "brett\t3\n",
      "flutter\t1\n",
      "refuse\t1\n",
      "pinnamaneni\t4\n",
      "fundamental\t2\n",
      "geographic\t4\n",
      "'reason\t1\n",
      "barnard\t1\n",
      "attitudinal\t1\n",
      "tramadol\t2\n",
      "trade\t25\n",
      "paper\t3\n",
      "scott\t2\n",
      "499\t2\n",
      "brim\t1\n",
      "replies\t3\n",
      "27049\t2\n",
      "its\t27\n",
      "features\t6\n",
      "rapidly\t1\n",
      "info'and\t1\n",
      "deeds\t2\n",
      "alli\t1\n",
      "bypass\t1\n",
      "travelling\t1\n",
      "villarreal\t1\n",
      "fior\t2\n",
      "oconan\t2\n",
      "odin\t2\n",
      "always\t10\n",
      "yearno\t1\n",
      "courses\t1\n",
      "found\t6\n",
      "week\t38\n",
      "reduce\t1\n",
      "operation\t2\n",
      "really\t7\n",
      "ftp\t1\n",
      "148415904\t1\n",
      "missed\t2\n",
      "research\t5\n",
      "occurs\t1\n",
      "chumming\t1\n",
      "imaging\t1\n",
      "murmur\t1\n",
      "imagine\t1\n",
      "trevino\t4\n",
      "director\t6\n",
      "owners\t2\n",
      "devoid\t1\n",
      "2575\t4\n",
      "offsite\t2\n",
      "w\t12\n",
      "major\t5\n",
      "slipped\t1\n",
      "thereafter\t2\n",
      "number\t17\n",
      "goage\t1\n",
      "guess\t1\n",
      "973\t2\n",
      "leverage\t1\n",
      "calculations\t2\n",
      "relationship\t6\n",
      "immediate\t2\n",
      "appreciation\t2\n",
      "focusing\t4\n",
      "unrealistic\t2\n",
      "critically\t2\n",
      "pennsylvania\t3\n",
      "myshoppingplace\t4\n",
      "basically\t1\n",
      "sell\t8\n",
      "self\t4\n",
      "'stefkatz\t1\n",
      "also\t56\n",
      "tarrin\t1\n",
      "internal\t9\n",
      "hgh\t5\n",
      "ancillary\t2\n",
      "tarrif\t2\n",
      "plan\t17\n",
      "accepting\t2\n",
      "39510\t2\n",
      "omaha\t1\n",
      "seize\t2\n",
      "sometimes\t1\n",
      "cover\t2\n",
      "ext\t3\n",
      "1687\t1\n",
      "1689\t1\n",
      "microsoft\t6\n",
      "wording\t1\n",
      "despise\t1\n",
      "ugh\t1\n",
      "gold\t14\n",
      "midwestern\t1\n",
      "session\t1\n",
      "impact\t3\n",
      "indicator\t1\n",
      "'underga\t1\n",
      "equity\t5\n",
      "to'blow\t1\n",
      "hourhelp\t1\n",
      "gratis\t1\n",
      "liquid\t1\n",
      "closely\t5\n",
      "banking\t4\n",
      "cre\t1\n",
      "river\t2\n",
      "insane\t2\n",
      "set\t15\n",
      "overwhelm\t1\n",
      "312\t1\n",
      "activists\t1\n",
      "sex\t1\n",
      "signups\t1\n",
      "see\t34\n",
      "emai\t1\n",
      "sen\t1\n",
      "guinea\t1\n",
      "currently\t16\n",
      "with'center\t1\n",
      "rita\t3\n",
      "available\t23\n",
      "targeted\t11\n",
      "knights\t1\n",
      "'gary\t1\n",
      "legislature\t9\n",
      "prospects\t1\n",
      "last\t18\n",
      "pdx\t1\n",
      "barely\t1\n",
      "connection\t3\n",
      "scams\t1\n",
      "whole\t2\n",
      "sink\t1\n",
      "loan\t4\n",
      "community\t23\n",
      "worthless\t1\n",
      "publishing\t2\n",
      "monthly\t2\n",
      "lets\t1\n",
      "geoffrey\t1\n",
      "xan\t2\n",
      "firm\t5\n",
      "champion\t3\n",
      "niles\t3\n",
      "what's\t3\n",
      "nrg\t1\n",
      "bmlm\t4\n",
      "fund\t4\n",
      "nrw\t1\n",
      "crenshaw\t13\n",
      "straight\t4\n",
      "expanded\t1\n",
      "technical\t3\n",
      "cagey\t1\n",
      "evaluating\t1\n",
      "error\t8\n",
      "styles\t1\n",
      "pound\t2\n",
      "triple\t1\n",
      "barney\t3\n",
      "878\t1\n",
      "877\t3\n",
      "shorter\t1\n",
      "read\t18\n",
      "headachesmerle\t1\n",
      "comprehensive\t1\n",
      "stacy\t1\n",
      "levels\t2\n",
      "nickname\t1\n",
      "recent\t3\n",
      "amy\t2\n",
      "stephanie\t1\n",
      "person\t16\n",
      "leone\t5\n",
      "buka\t2\n",
      "readers\t2\n",
      "rebecca\t2\n",
      "advertisements\t1\n",
      "andorra\t1\n",
      "dhar\t4\n",
      "parents\t1\n",
      "rear\t1\n",
      "sydney\t2\n",
      "input\t6\n",
      "submissions\t4\n",
      "australia\t10\n",
      "format\t3\n",
      "couple\t2\n",
      "wives\t1\n",
      "ozarka\t1\n",
      "private\t5\n",
      "herewe\t1\n",
      "projects\t2\n",
      "formal\t1\n",
      "d\t26\n",
      "afirst\t1\n",
      "continue\t15\n",
      "ivan\t2\n",
      "methods\t1\n",
      "senate\t2\n",
      "spring\t1\n",
      "mighty\t1\n",
      "436425795822\t1\n",
      "sight\t1\n",
      "son's\t2\n",
      "bd\t6\n",
      "be\t221\n",
      "jturco\t2\n",
      "china's\t1\n",
      "agreement\t16\n",
      "bp\t3\n",
      "concealed\t2\n",
      "br\t2\n",
      "bs\t2\n",
      "carol\t1\n",
      "by\t143\n",
      "anything\t11\n",
      "ivernia\t2\n",
      "computational\t1\n",
      "intl\t2\n",
      "hemingway\t1\n",
      "servers\t2\n",
      "appropriate\t3\n",
      "spam\t7\n",
      "spending\t1\n",
      "specifically\t4\n",
      "custom\t1\n",
      "atop\t1\n",
      "link\t6\n",
      "competition\t2\n",
      "line\t21\n",
      "uz\t1\n",
      "up\t47\n",
      "us\t50\n",
      "ur\t1\n",
      "un\t1\n",
      "verification\t2\n",
      "uk\t4\n",
      "faded\t1\n",
      "steeves\t2\n",
      "supervisor\t4\n",
      "armstrong\t2\n",
      "defined\t1\n",
      "m\t26\n",
      "influence\t2\n",
      "char\t2\n",
      "uranium\t1\n",
      "carrera\t1\n",
      "kristin\t2\n",
      "energetic\t1\n",
      "ativan\t1\n",
      "sides\t1\n",
      "ago\t1\n",
      "land\t1\n",
      "valuables\t2\n",
      "consumer\t5\n",
      "age\t5\n",
      "walked\t1\n",
      "summit\t1\n",
      "hello\t6\n",
      "scratch\t1\n",
      "results\t6\n",
      "krgp\t2\n",
      "cobalt\t1\n",
      "dicine\t1\n",
      "transistion\t5\n",
      "seemed\t2\n",
      "concerned\t1\n",
      "verypowerful\t1\n",
      "young\t2\n",
      "send\t23\n",
      "resources\t23\n",
      "uploaded\t1\n",
      "continues\t3\n",
      "stocker\t1\n",
      "7517_bybtb\t1\n",
      "continued\t3\n",
      "minerals\t4\n",
      "timely\t2\n",
      "088889774\t1\n",
      "transmission\t10\n",
      "lynda\t1\n",
      "whooping\t1\n",
      "they've\t1\n",
      "try\t10\n",
      "race\t1\n",
      "599\t1\n",
      "linda's\t1\n",
      "prager\t1\n",
      "timshometownstories\t3\n",
      "hyde\t1\n",
      "dynamics\t2\n",
      "victor\t2\n",
      "expressed\t1\n",
      "consistently\t2\n",
      "proliferation\t3\n",
      "lee\t1\n",
      "delivering\t1\n",
      "let\t32\n",
      "licence\t1\n",
      "fifteen\t1\n",
      "great\t20\n",
      "1932\t1\n",
      "receive\t12\n",
      "involved\t7\n",
      "survey\t3\n",
      "defeat\t1\n",
      "opinion\t5\n",
      "residents\t1\n",
      "makes\t5\n",
      "involves\t1\n",
      "1935\t1\n",
      "rvsq\t1\n",
      "332\t1\n",
      "discoouunt\t1\n",
      "standing\t6\n",
      "confidence\t1\n",
      "1938\t1\n",
      "next\t27\n",
      "duplicate\t1\n",
      "doubt\t3\n",
      "rick\t11\n",
      "aphrodite\t1\n",
      "priices\t1\n",
      "sharply\t1\n",
      "baby\t2\n",
      "delegating\t1\n",
      "chinamen\t1\n",
      "iain\t1\n",
      "customer\t23\n",
      "this\t262\n",
      "challenge\t1\n",
      "clients\t1\n",
      "fulfill\t2\n",
      "process\t13\n",
      "lock\t2\n",
      "promotional\t2\n",
      "high\t15\n",
      "getyour\t1\n",
      "verbry\t1\n",
      "delay\t1\n",
      "await\t4\n",
      "jonathon\t1\n",
      "commodities\t1\n",
      "bussell\t1\n",
      "allow\t12\n",
      "counted\t1\n",
      "houston\t13\n",
      "lenders\t4\n",
      "producer\t1\n",
      "move\t8\n",
      "tungsten\t1\n",
      "triassic\t1\n",
      "perfect\t3\n",
      "chosen\t4\n",
      "instruments\t1\n",
      "physicians\t1\n",
      "designs\t4\n",
      "outlined\t1\n",
      "kish\t1\n",
      "cage\t1\n",
      "designl\t1\n",
      "presidential\t1\n",
      "glaspie\t1\n",
      "truth\t2\n",
      "rising\t1\n",
      "footmen\t1\n",
      "'mmce\t1\n",
      "doing\t6\n",
      "habitually\t1\n",
      "books\t2\n",
      "thirteen\t1\n",
      "'\t3\n",
      "shut\t1\n",
      "godly\t1\n",
      "ezine\t4\n",
      "could\t18\n",
      "david\t19\n",
      "length\t1\n",
      "hindering\t1\n",
      "davis\t7\n",
      "anita\t2\n",
      "owned\t5\n",
      "owner\t2\n",
      "blows\t1\n",
      "ihf\t1\n",
      "legislative\t1\n",
      "system\t13\n",
      "norton\t3\n",
      "interests\t7\n",
      "lot\t10\n",
      "unsubscribe\t16\n",
      "nowbetterthis\t2\n",
      "bother\t1\n",
      "roberts\t9\n",
      "mysiteinc\t4\n",
      "viewer\t1\n",
      "partnership\t1\n",
      "clearly\t1\n",
      "necessities\t1\n",
      "documents\t4\n",
      "dishes\t1\n",
      "mechanism\t1\n",
      "best\t27\n",
      "woolgar\t1\n",
      "linda\t4\n",
      "soap\t2\n",
      "persistence\t2\n",
      "worldwide\t11\n",
      "02\t19\n",
      "courtesy\t1\n",
      "said\t3\n",
      "device\t1\n",
      "amortize\t4\n",
      "placement\t2\n",
      "05\t4\n",
      "stronger\t2\n",
      "face\t1\n",
      "you'll\t5\n",
      "brex\t2\n",
      "fact\t8\n",
      "com______________________________________________________today's\t1\n",
      "terminate\t1\n",
      "bring\t1\n",
      "principal\t3\n",
      "pause\t1\n",
      "should\t27\n",
      "jan\t4\n",
      "planted\t1\n",
      "riding\t1\n",
      "hope\t9\n",
      "melissa\t3\n",
      "meant\t1\n",
      "handle\t1\n",
      "listened\t1\n",
      "bonds\t6\n",
      "lucky\t2\n",
      "exchanges\t1\n",
      "generalist\t1\n",
      "h\t7\n",
      "aec\t2\n",
      "stuff\t4\n",
      "'repling\t1\n",
      "btu\t2\n",
      "strengthened\t1\n",
      "frame\t1\n",
      "edition\t5\n",
      "email\t46\n",
      "retentive\t1\n",
      "'tgary\t1\n",
      "ends\t1\n",
      "freelinksnetwork\t4\n",
      "strides\t2\n",
      "configuration\t1\n",
      "restrictions\t1\n",
      "figured\t1\n",
      "drug\t1\n",
      "etc\t9\n",
      "streams\t2\n",
      "cj\t5\n",
      "co\t5\n",
      "cc\t40\n",
      "ca\t3\n",
      "cd\t2\n",
      "cs\t2\n",
      "allocated\t2\n",
      "cp\t1\n",
      "chromium\t1\n",
      "2807\t1\n",
      "mailto\t11\n",
      "mailman\t3\n",
      "crapbedspring\t1\n",
      "willing\t1\n",
      "hottest\t1\n",
      "waste\t1\n",
      "moved\t2\n",
      "dfur\t1\n",
      "sales\t43\n",
      "5555\t2\n",
      "graphics\t5\n",
      "justperform\t1\n",
      "discretely\t1\n",
      "intranet\t3\n",
      "ve\t2\n",
      "here____________________________________________________________________________________________________________be\t1\n",
      "vi\t2\n",
      "retarding\t1\n",
      "hardware\t1\n",
      "vs\t2\n",
      "optimistic\t1\n",
      "terrorist\t1\n",
      "paulo\t5\n",
      "digging\t1\n",
      "mpkemyrxlpq\t1\n",
      "raymond\t7\n",
      "upon\t9\n",
      "forecasts\t2\n",
      "emeet\t1\n",
      "php\t3\n",
      "audit\t3\n",
      "argue\t1\n",
      "discounted\t1\n",
      "colour\t1\n",
      "audio\t2\n",
      "drawing\t5\n",
      "less\t20\n",
      "moments\t1\n",
      "glut\t1\n",
      "xes\t1\n",
      "paul\t2\n",
      "web\t23\n",
      "baccarat\t1\n",
      "realize\t3\n",
      "arrest\t4\n",
      "69545\t1\n",
      "increased\t5\n",
      "government\t7\n",
      "checking\t4\n",
      "increases\t2\n",
      "five\t3\n",
      "desk\t12\n",
      "belgium\t1\n",
      "password\t5\n",
      "emma\t1\n",
      "softtwares\t2\n",
      "afeee\t1\n",
      "259\t1\n",
      "become\t6\n",
      "gaining\t1\n",
      "dynegy\t11\n",
      "macromedia\t6\n",
      "ballot\t1\n",
      "recognition\t1\n",
      "literally\t1\n",
      "avoid\t10\n",
      "does\t11\n",
      "anabel\t1\n",
      "salaried\t1\n",
      "schedule\t8\n",
      "33465\t1\n",
      "pressure\t5\n",
      "accompanied\t1\n",
      "stage\t1\n",
      "asks\t1\n",
      "kri\t2\n",
      "insufficient\t1\n",
      "executives\t1\n",
      "letters\t11\n",
      "referendum\t6\n",
      "screamed\t1\n",
      "joyce\t1\n",
      "crooked\t1\n",
      "commission\t3\n",
      "00450\t1\n",
      "frustration\t1\n",
      "rosenfield\t6\n",
      "flynn\t1\n",
      "function\t5\n",
      "delivery\t11\n",
      "construction\t1\n",
      "admin_magnumo\t1\n",
      "apollo\t1\n",
      "places\t1\n",
      "official\t1\n",
      "knudsen\t1\n",
      "placed\t1\n",
      "convince\t2\n",
      "distribution\t3\n",
      "meyers\t1\n",
      "irish\t1\n",
      "rightly\t1\n",
      "dissemination\t1\n",
      "hearers\t1\n",
      "inc\t12\n",
      "compared\t1\n",
      "variety\t1\n",
      "trials\t1\n",
      "overgaard\t1\n",
      "details\t3\n",
      "hpl\t12\n",
      "francisco\t1\n",
      "monday\t15\n",
      "chance\t6\n",
      "repeal\t1\n",
      "vein\t1\n",
      "exposure\t2\n",
      "rule\t2\n",
      "compete\t2\n",
      "474\t1\n",
      "'mcosta\t1\n",
      "child\t3\n",
      "35782\t1\n",
      "integrity\t2\n",
      "'chuck\t1\n",
      "voted\t1\n",
      "chill\t1\n",
      "worth\t1\n",
      "overpaying\t1\n",
      "distort\t1\n",
      "we'll\t11\n",
      "exaggerate\t1\n",
      "upload\t1\n",
      "casinos\t2\n",
      "299\t1\n",
      "indices\t1\n",
      "southernenergy\t2\n",
      "dobmeos\t1\n",
      "established\t3\n",
      "faclities\t1\n",
      "establishes\t1\n",
      "toll\t3\n",
      "bharat\t1\n",
      "officially\t3\n",
      "consisting\t1\n",
      "told\t6\n",
      "textures\t1\n",
      "interne\t1\n",
      "celebration\t3\n",
      "obtained\t1\n",
      "items\t15\n",
      "employees\t7\n",
      "foolishness\t1\n",
      "aka\t1\n",
      "secure\t3\n",
      "horn'and\t1\n",
      "angrily\t1\n",
      "highly\t1\n",
      "thursday\t10\n",
      "total\t8\n",
      "negative\t1\n",
      "com______________________________________________________\t1\n",
      "award\t1\n",
      "aware\t1\n",
      "memo's\t1\n",
      "blocking\t2\n",
      "stearns\t2\n",
      "1990\t2\n",
      "1992\t3\n",
      "word\t9\n",
      "1997\t3\n",
      "1999\t10\n",
      "1998\t2\n",
      "work\t32\n",
      "ere\t1\n",
      "elbow\t1\n",
      "indicated\t1\n",
      "2868\t1\n",
      "bennett\t1\n",
      "india\t1\n",
      "553\t1\n",
      "walsh\t2\n",
      "dutchess\t1\n",
      "recovery\t2\n",
      "provide\t21\n",
      "verify\t1\n",
      "indubitable\t1\n",
      "after\t19\n",
      "law\t5\n",
      "'benewm\t1\n",
      "appreciate\t2\n",
      "encompassing\t1\n",
      "order\t44\n",
      "office\t26\n",
      "satisfied\t4\n",
      "innovative\t2\n",
      "japan\t3\n",
      "production\t8\n",
      "ruanda\t1\n",
      "workable\t1\n",
      "split\t1\n",
      "versus\t2\n",
      "then\t28\n",
      "them\t56\n",
      "bazzd\t1\n",
      "safe\t5\n",
      "break\t5\n",
      "they\t60\n",
      "payback\t1\n",
      "strategic\t3\n",
      "bank\t6\n",
      "spigot\t1\n",
      "backgrounds\t1\n",
      "l\t12\n",
      "reasonable\t2\n",
      "flyers\t1\n",
      "375\t1\n",
      "carlos\t5\n",
      "lifted\t1\n",
      "ssb\t1\n",
      "chirano\t4\n",
      "impede\t1\n",
      "hosted\t1\n",
      "ypfpb\t1\n",
      "network\t1\n",
      "forty\t1\n",
      "daniel\t2\n",
      "barrier\t2\n",
      "standard\t3\n",
      "cheeap\t1\n",
      "created\t7\n",
      "refugee\t3\n",
      "luong\t1\n",
      "quadrangular\t1\n",
      "creates\t2\n",
      "oppose\t1\n",
      "incorporated\t1\n",
      "'hoferc\t1\n",
      "organize\t2\n",
      "another\t15\n",
      "illustrate\t2\n",
      "rated\t3\n",
      "damorgarjr\t1\n",
      "approximately\t4\n",
      "plentiful\t1\n",
      "airs\t1\n",
      "rates\t7\n",
      "thoughts\t2\n",
      "purposefully\t1\n",
      "john\t21\n",
      "enhancing\t1\n",
      "toronto\t1\n",
      "target\t9\n",
      "hike\t1\n",
      "nbi\t1\n",
      "iron\t2\n",
      "solely\t1\n",
      "rewrite\t2\n",
      "789118270\t1\n",
      "forced\t1\n",
      "strength\t1\n",
      "genuine\t2\n",
      "convenient\t1\n",
      "practices\t3\n",
      "forces\t5\n",
      "magnum\t1\n",
      "ebook\t3\n",
      "what\t65\n",
      "cordes\t1\n",
      "involving\t2\n",
      "germany\t3\n",
      "802\t1\n",
      "newcomers\t2\n",
      "voting\t2\n",
      "reserve\t2\n",
      "keywords\t2\n",
      "implementation\t9\n",
      "helpdesk\t2\n",
      "screening\t1\n",
      "pursuing\t1\n",
      "personality\t4\n",
      "do\t41\n",
      "endorsements\t1\n",
      "dc\t1\n",
      "preferred\t2\n",
      "watson\t2\n",
      "wynne\t2\n",
      "415\t4\n",
      "steak\t1\n",
      "418\t4\n",
      "419\t1\n",
      "'doug\t1\n",
      "reread\t1\n",
      "fattroglodyte\t1\n",
      "'john\t1\n",
      "miserable\t1\n",
      "techniques\t5\n",
      "away\t9\n",
      "calamity\t2\n",
      "unable\t3\n",
      "cooperation\t3\n",
      "drawn\t2\n",
      "neyeded\t1\n",
      "encounters\t1\n",
      "we\t164\n",
      "terms\t5\n",
      "wa\t2\n",
      "ws\t1\n",
      "wr\t1\n",
      "huang\t4\n",
      "kitchen\t3\n",
      "received\t5\n",
      "essentially\t1\n",
      "ncaa\t1\n",
      "coe\t4\n",
      "receives\t4\n",
      "brad\t2\n",
      "com\t156\n",
      "schafer\t1\n",
      "negotiation\t1\n",
      "tone\t1\n",
      "trunk\t2\n",
      "speak\t8\n",
      "engines\t2\n",
      "drafting\t1\n",
      "dozens\t1\n",
      "stinson\t8\n",
      "families\t2\n",
      "attacked\t1\n",
      "concerning\t8\n",
      "applied\t5\n",
      "east\t1\n",
      "air\t6\n",
      "information\t71\n",
      "whelan\t1\n",
      "aid\t1\n",
      "property\t2\n",
      "davenport\t1\n",
      "9610\t1\n",
      "conn\t1\n",
      "1814\t1\n",
      "confirmed\t1\n",
      "perform\t1\n",
      "b's\t1\n",
      "incorrectly\t1\n",
      "counterparty\t2\n",
      "nim\t2\n",
      "hang\t1\n",
      "hand\t5\n",
      "greedily\t1\n",
      "kept\t2\n",
      "whereby\t2\n",
      "scenario\t1\n",
      "thu\t8\n",
      "humble\t1\n",
      "kyle\t1\n",
      "ellendale\t2\n",
      "client\t1\n",
      "the\t1247\n",
      "mclean\t1\n",
      "quoted\t4\n",
      "photo\t1\n",
      "quotes\t3\n",
      "'hans\t1\n",
      "thanks\t34\n",
      "macquarie\t2\n",
      "adding\t2\n",
      "spread\t1\n",
      "board\t3\n",
      "lcd\t1\n",
      "wardsgiftshop\t1\n",
      "cooler\t1\n",
      "grizzly\t1\n",
      "securities\t5\n",
      "night\t4\n",
      "security\t10\n",
      "pting\t1\n",
      "flatter\t1\n",
      "hassle\t2\n",
      "critique\t1\n",
      "confusing\t1\n",
      "cede\t1\n",
      "priicce\t1\n",
      "unavoidable\t1\n",
      "comments\t13\n",
      "trademarked\t2\n",
      "asking\t10\n",
      "colloquy\t1\n",
      "denied\t1\n",
      "'colliw\t1\n",
      "participation\t1\n",
      "peel\t2\n",
      "marketing\t28\n",
      "illustration\t1\n",
      "constructive\t1\n",
      "post\t9\n",
      "descendent\t1\n",
      "928976257\t1\n",
      "banked\t1\n",
      "coral\t4\n",
      "months\t10\n",
      "accepts\t1\n",
      "banker\t3\n",
      "prilosec\t1\n",
      "samer\t5\n",
      "bound\t2\n",
      "bonnard\t1\n",
      "anheuser\t1\n",
      "theinvestment\t1\n",
      "etacitne\t1\n",
      "fight\t2\n",
      "accordingly\t1\n",
      "way\t10\n",
      "was\t68\n",
      "war\t2\n",
      "lowest\t1\n",
      "diligence\t1\n",
      "dead\t3\n",
      "true\t9\n",
      "absent\t1\n",
      "0643\t1\n",
      "markets\t5\n",
      "crystal\t2\n",
      "emotional\t1\n",
      "digression\t1\n",
      "moore\t39\n",
      "inclusive\t1\n",
      "expertise\t8\n",
      "promised\t1\n",
      "distributions\t2\n",
      "archive\t1\n",
      "physical\t6\n",
      "stake\t2\n",
      "cliffhanger\t2\n",
      "interested\t10\n",
      "unsubscribed\t1\n",
      "holding\t1\n",
      "test\t1\n",
      "hoffman\t2\n",
      "62413\t1\n",
      "dirtier\t1\n",
      "theqgrefor\t1\n",
      "welcome\t2\n",
      "update\t13\n",
      "let's\t3\n",
      "walton\t2\n",
      "cheated\t4\n",
      "paces\t1\n",
      "modules\t4\n",
      "together\t7\n",
      "beds\t3\n",
      "loyal\t2\n",
      "reception\t1\n",
      "precedent\t1\n",
      "songs\t1\n",
      "concept\t1\n",
      "global\t44\n",
      "hris\t1\n",
      "supposedly\t1\n",
      "killings\t2\n",
      "flash\t2\n",
      "southwest\t1\n",
      "brown\t3\n",
      "scotty\t1\n",
      "entries\t3\n",
      "aggressively\t3\n",
      "perceived\t1\n",
      "presented\t1\n",
      "orgoto\t1\n",
      "p\t24\n",
      "guy\t2\n",
      "revolution\t2\n",
      "htm\t2\n",
      "discover\t3\n",
      "initializes\t1\n",
      "cost\t8\n",
      "helpless\t2\n",
      "'william\t1\n",
      "appear\t7\n",
      "assistance\t10\n",
      "shares\t1\n",
      "shared\t4\n",
      "1016\t1\n",
      "alertness\t1\n",
      "supporting\t6\n",
      "apachi\t1\n",
      "teacher\t1\n",
      "change\t16\n",
      "sending\t1\n",
      "thu's\t2\n",
      "trial\t3\n",
      "431\t1\n",
      "regardless\t4\n",
      "extra\t7\n",
      "sincerely\t4\n",
      "snowboard\t1\n",
      "mobil\t1\n",
      "market\t12\n",
      "prove\t2\n",
      "reclaimers\t1\n",
      "live\t1\n",
      "ca'\t3\n",
      "club\t1\n",
      "nguyen\t3\n",
      "isalso\t1\n",
      "advs\t1\n",
      "car\t6\n",
      "cat\t2\n",
      "can\t90\n",
      "cal\t3\n",
      "trans\t1\n",
      "heart\t1\n",
      "spp\t1\n",
      "december\t19\n",
      "chip\t1\n",
      "didn't\t4\n",
      "topic\t2\n",
      "heard\t2\n",
      "occur\t1\n",
      "adobee\t1\n",
      "fortunately\t2\n",
      "discussion\t8\n",
      "spreads\t1\n",
      "write\t13\n",
      "economy\t1\n",
      "product\t36\n",
      "iteam\t1\n",
      "aol\t6\n",
      "southern\t1\n",
      "produce\t2\n",
      "nomination\t4\n",
      "remember\t13\n",
      "ghz\t1\n",
      "rather\t6\n",
      "accelerated\t1\n",
      "displayed\t1\n",
      "tablets\t1\n",
      "managers\t1\n",
      "cold\t2\n",
      "still\t12\n",
      "ethic\t1\n",
      "readers'mind\t1\n",
      "forms\t1\n",
      "window\t1\n",
      "unblocking\t4\n",
      "2152\t1\n",
      "factual\t1\n",
      "emigrants\t1\n",
      "correspondence\t2\n",
      "nom\t8\n",
      "non\t5\n",
      "disparate\t1\n",
      "thrid\t1\n",
      "introduce\t1\n",
      "transmissions\t1\n",
      "nox\t2\n",
      "1953\t1\n",
      "half\t2\n",
      "not\t126\n",
      "nov\t5\n",
      "now\t29\n",
      "discuss\t6\n",
      "enacted\t2\n",
      "lindiwe\t2\n",
      "january\t2\n",
      "entirely\t2\n",
      "kerb\t1\n",
      "el\t2\n",
      "domain\t5\n",
      "en\t3\n",
      "eh\t1\n",
      "ee\t2\n",
      "ed\t1\n",
      "directing\t1\n",
      "yeah\t2\n",
      "eb\t16\n",
      "challenges\t1\n",
      "replay\t2\n",
      "phentermine\t3\n",
      "year\t25\n",
      "happen\t4\n",
      "avoided\t1\n",
      "3300\t1\n",
      "sportsbetting\t2\n",
      "shown\t2\n",
      "medications\t1\n",
      "space\t7\n",
      "furthermore\t1\n",
      "fw\t3\n",
      "increase\t12\n",
      "receiving\t5\n",
      "shows\t1\n",
      "slotting\t1\n",
      "cars\t4\n",
      "6614102\t1\n",
      "cart\t2\n",
      "quark\t1\n",
      "advantages\t1\n",
      "rebel\t2\n",
      "obligation\t2\n",
      "card\t5\n",
      "care\t9\n",
      "contemplation\t1\n",
      "transition\t4\n",
      "honest\t1\n",
      "519\t1\n",
      "invitation\t1\n",
      "promotion\t1\n",
      "xm\t1\n",
      "zac\t2\n",
      "yourself\t11\n",
      "directly\t7\n",
      "impossible\t1\n",
      "xp\t6\n",
      "forwarding\t1\n",
      "cpuc\t3\n",
      "size\t5\n",
      "officials\t1\n",
      "caught\t1\n",
      "eclassifiedshq\t1\n",
      "freebie\t2\n",
      "moates\t1\n",
      "friend\t2\n",
      "mostly\t1\n",
      "that\t222\n",
      "randle\t1\n",
      "eliminates\t1\n",
      "expanse\t1\n",
      "jane\t1\n",
      "appreciated\t1\n",
      "professionals\t3\n",
      "television\t1\n",
      "tangible\t1\n",
      "bauxite\t1\n",
      "transferred\t3\n",
      "discounnt\t1\n",
      "magnesium\t1\n",
      "browser\t2\n",
      "investors\t1\n",
      "craig\t1\n",
      "correction\t2\n",
      "deliv\t2\n",
      "recover\t2\n",
      "publicity\t1\n",
      "form\t11\n",
      "veteran\t1\n",
      "manufacturers\t1\n",
      "terrific\t1\n",
      "online\t20\n",
      "begin\t4\n",
      "price\t27\n",
      "importantly\t1\n",
      "roxio\t1\n",
      "america\t15\n",
      "forever\t3\n",
      "wacked\t1\n",
      "mrs\t8\n",
      "slaver\t1\n",
      "professional\t14\n",
      "filing\t11\n",
      "excerpts\t1\n",
      "maine\t1\n",
      "fifth\t1\n",
      "ground\t1\n",
      "224\t1\n",
      "ratio\t1\n",
      "title\t4\n",
      "texture\t1\n",
      "only\t32\n",
      "thompson\t1\n",
      "211075433222\t2\n",
      "interstate\t1\n",
      "upgradeable\t3\n",
      "samuel\t1\n",
      "truly\t1\n",
      "cannot\t5\n",
      "celebrate\t1\n",
      "ontacted\t1\n",
      "keyes\t1\n",
      "husband\t4\n",
      "salomon\t2\n",
      "burst\t1\n",
      "inseminate\t1\n",
      "concern\t1\n",
      "equivelant\t1\n",
      "3\t36\n",
      "between\t7\n",
      "33155\t1\n",
      "complications\t1\n",
      "dhyngem\t1\n",
      "notice\t4\n",
      "benchmarks\t3\n",
      "article\t3\n",
      "emigrant\t1\n",
      "misspelled\t1\n",
      "haunch\t1\n",
      "comes\t6\n",
      "nearby\t1\n",
      "150\t1\n",
      "learning\t3\n",
      "cares\t1\n",
      "informed\t4\n",
      "systems\t6\n",
      "notiffiyved\t1\n",
      "'pwarden\t1\n",
      "2'00\t2\n",
      "bannerco\t4\n",
      "developing\t2\n",
      "these\t50\n",
      "maryam\t1\n",
      "editorial\t1\n",
      "heno\t1\n",
      "stage'in\t1\n",
      "cheapsoft\t3\n",
      "frusco\t3\n",
      "worry\t2\n",
      "media\t3\n",
      "delusive\t1\n",
      "hrgovcic\t5\n",
      "powerquest\t1\n",
      "respected\t1\n",
      "fruit\t1\n",
      "validate\t1\n",
      "tradition\t1\n",
      "charges\t1\n",
      "users'group\t1\n",
      "severe\t1\n",
      "coordinate\t5\n",
      "cialis\t2\n",
      "noah\t1\n",
      "valuable\t9\n",
      "greetings\t1\n",
      "ounces\t1\n",
      "speed\t2\n",
      "rob's\t1\n",
      "death\t3\n",
      "commentaryto\t1\n",
      "thinking\t1\n",
      "xent\t2\n",
      "desktop\t1\n",
      "lender\t2\n",
      "yjoou\t1\n",
      "treatment\t1\n",
      "xeni\t2\n",
      "struck\t2\n",
      "real\t14\n",
      "rules\t1\n",
      "amb\t2\n",
      "discontinue\t2\n",
      "early\t8\n",
      "detract\t1\n",
      "using\t18\n",
      "execution\t1\n",
      "recipient\t2\n",
      "hornbuckle\t1\n",
      "pst\t1\n",
      "pounds\t1\n",
      "suppliers\t6\n",
      "6396\t1\n",
      "benefit\t10\n",
      "t\t7\n",
      "ie\t4\n",
      "abidjan\t2\n",
      "bless\t3\n",
      "regina\t6\n",
      "recorded\t1\n",
      "willie\t1\n",
      "youcan\t1\n",
      "assembly\t2\n",
      "business\t64\n",
      "asap\t1\n",
      "1933\t1\n",
      "credits\t1\n",
      "1930\t1\n",
      "1936\t1\n",
      "3267\t1\n",
      "1934\t1\n",
      "exciting\t1\n",
      "throw\t1\n",
      "brighton\t1\n",
      "datacenter\t1\n",
      "wfxu\t1\n",
      "central\t3\n",
      "powered\t3\n",
      "srs\t1\n",
      "fell\t2\n",
      "greatly\t1\n",
      "getterscan\t1\n",
      "backup\t2\n",
      "processor\t1\n",
      "stratagem\t1\n",
      "outlook\t3\n",
      "involvement\t1\n",
      "your\t394\n",
      "forwarded\t29\n",
      "area\t4\n",
      "removing\t1\n",
      "start\t9\n",
      "low\t10\n",
      "stars\t1\n",
      "billion\t6\n",
      "delayed\t2\n",
      "gunners\t1\n",
      "trying\t3\n",
      "proposal\t9\n",
      "netsbestinfo\t2\n",
      "hire\t2\n",
      "fraud\t1\n",
      "illustrator\t2\n",
      "improved\t5\n",
      "miguel\t1\n",
      "600\t1\n",
      "602\t1\n",
      "609\t1\n",
      "1200\t2\n",
      "bawled\t1\n",
      "nero\t1\n",
      "moves\t1\n",
      "cabinets\t1\n",
      "credibility\t1\n",
      "establishthe\t1\n",
      "valid\t4\n",
      "you\t432\n",
      "forcing\t1\n",
      "poor\t3\n",
      "unocal\t1\n",
      "dubuque\t3\n",
      "coverage\t3\n",
      "queensland\t2\n",
      "banners\t1\n",
      "building\t7\n",
      "corel\t4\n",
      "here'underneath\t1\n",
      "corey\t2\n",
      "ward's\t1\n",
      "splitting\t1\n",
      "month\t17\n",
      "ceased\t1\n",
      "unblock\t1\n",
      "deadline\t6\n",
      "pricing\t4\n",
      "rtol\t1\n",
      "danielle\t2\n",
      "solicit\t1\n",
      "confidential\t5\n",
      "very\t31\n",
      "horror\t1\n",
      "7675213911\t1\n",
      "deficient\t1\n",
      "manager\t8\n",
      "transcanada\t2\n",
      "clemons\t1\n",
      "anal\t1\n",
      "locomotive\t1\n",
      "i'll\t5\n",
      "busine\t1\n",
      "streets\t1\n",
      "prostaff\t1\n",
      "businessopps\t2\n",
      "attributable\t1\n",
      "conscience\t1\n",
      "____________________________________________________________________________________________________________you\t1\n",
      "consumers\t3\n",
      "thickness\t1\n",
      "certificates\t1\n",
      "learned\t2\n",
      "khanna\t1\n",
      "answers\t7\n",
      "promoted\t1\n",
      "tracks\t2\n",
      "excess\t1\n",
      "strong\t2\n",
      "colored\t2\n",
      "ahead\t1\n",
      "mirant\t1\n",
      "amount\t7\n",
      "bestowal\t1\n",
      "advertising\t7\n",
      "musically\t1\n",
      "family\t3\n",
      "tenderer\t1\n",
      "requiring\t1\n",
      "put\t12\n",
      "takes\t7\n",
      "answered\t1\n",
      "contains\t4\n",
      "taken\t2\n",
      "'jlopes\t1\n",
      "commenced\t1\n",
      "impulse\t1\n",
      "mounted\t1\n",
      "producing\t1\n",
      "vp\t1\n",
      "33282\t1\n",
      "startbgmlmezine\t2\n",
      "nine\t1\n",
      "history\t2\n",
      "pushes\t1\n",
      "pushed\t1\n",
      "stayed\t1\n",
      "calme\t1\n",
      "leederville\t1\n",
      "fp\t1\n",
      "ft\t2\n",
      "tried\t5\n",
      "communicating\t2\n",
      "sneak\t2\n",
      "rudl\t2\n",
      "fh\t2\n",
      "fl\t1\n",
      "'sallen\t1\n",
      "a\t542\n",
      "workstation\t1\n",
      "overnight\t2\n",
      "ego\t2\n",
      "banks\t1\n",
      "photoshop\t3\n",
      "missss\t1\n",
      "help\t16\n",
      "soon\t9\n",
      "held\t6\n",
      "committee\t2\n",
      "uvd\t1\n",
      "herod\t1\n",
      "actually\t2\n",
      "disclosed\t2\n",
      "goodmorning\t2\n",
      "malina\t4\n",
      "evening\t3\n",
      "ceo\t1\n",
      "you've\t4\n",
      "ppmfztdtet\t1\n",
      "foot\t3\n",
      "bryan\t6\n",
      "fully\t3\n",
      "capability\t3\n",
      "isn't\t3\n",
      "referred\t2\n",
      "restless\t1\n",
      "fees\t1\n",
      "todd\t1\n",
      "beyond\t3\n",
      "robert\t7\n",
      "since\t13\n",
      "safety\t2\n",
      "7\t32\n",
      "issue\t21\n",
      "pun\t1\n",
      "puc\t3\n",
      "reason\t6\n",
      "base\t4\n",
      "ask\t9\n",
      "revolutionary\t1\n",
      "launch\t2\n",
      "persists\t1\n",
      "terrible\t1\n",
      "round\t1\n",
      "american\t2\n",
      "expecting\t2\n",
      "blush\t1\n",
      "fischer\t1\n",
      "encoding\t2\n",
      "reflected\t1\n",
      "a'click\t1\n",
      "miss\t2\n",
      "horse\t2\n",
      "generalities\t1\n",
      "station\t2\n",
      "himhe\t1\n",
      "hundred\t2\n",
      "selling\t11\n",
      "norma\t1\n",
      "authors\t1\n",
      "off\t8\n",
      "financing\t4\n",
      "kindly\t1\n",
      "anticipate\t1\n",
      "azurix\t10\n",
      "gssgeomatics\t1\n",
      "greg\t3\n",
      "toward\t1\n",
      "urgently\t1\n",
      "awaking\t1\n",
      "randomly\t1\n",
      "substantial\t2\n",
      "sentences\t4\n",
      "lin\t6\n",
      "empowered\t1\n",
      "lit\t1\n",
      "showcases\t6\n",
      "preneur\t2\n",
      "liz\t3\n",
      "towards\t4\n",
      "quote\t1\n",
      "'hal\t1\n",
      "sponsored\t1\n",
      "'ctise\t1\n",
      "odlx\t1\n",
      "kay\t5\n",
      "approved\t6\n",
      "optionscontrol\t1\n",
      "mobile\t1\n",
      "clear\t12\n",
      "innovatus\t1\n",
      "booze\t1\n",
      "completion\t3\n",
      "northern\t3\n",
      "pretty\t4\n",
      "relieves\t1\n",
      "hotmail\t2\n",
      "8434\t1\n",
      "meanwhile\t3\n",
      "famous\t1\n",
      "padron\t1\n",
      "competing\t3\n",
      "during\t9\n",
      "accomodates\t1\n",
      "oxx\t2\n",
      "hplr\t1\n",
      "millenium\t1\n",
      "wynpublishing\t1\n",
      "glover\t1\n",
      "recommendations\t4\n",
      "hype\t1\n",
      "x\t20\n",
      "hplc\t1\n",
      "delux\t1\n",
      "prosper\t3\n",
      "close\t6\n",
      "locals\t1\n",
      "switchman\t1\n",
      "pictures\t8\n",
      "woo\t1\n",
      "won\t1\n",
      "probably\t7\n",
      "missing\t1\n",
      "both\t15\n",
      "mega\t2\n",
      "secrets\t5\n",
      "wouldn't\t4\n",
      "sensitive\t2\n",
      "forgotten\t1\n",
      "vault\t1\n",
      "rodgers\t1\n",
      "liked\t1\n",
      "jeff\t6\n",
      "whatever\t4\n",
      "likes\t1\n",
      "linux\t4\n",
      "described\t2\n",
      "wait\t2\n",
      "generating\t2\n",
      "888\t5\n",
      "azepam\t2\n",
      "else\t2\n",
      "409055\t1\n",
      "novak\t1\n",
      "look\t14\n",
      "governor\t5\n",
      "while\t14\n",
      "intercreditor\t1\n",
      "gimg\t1\n",
      "reado\t1\n",
      "reads\t2\n",
      "ready\t5\n",
      "payers\t1\n",
      "d'arcy\t7\n",
      "grant\t6\n",
      "belong\t1\n",
      "grand\t3\n",
      "modification\t1\n",
      "conflict\t1\n",
      "used\t11\n",
      "bonus\t5\n",
      "001\t2\n",
      "000\t52\n",
      "uses\t1\n",
      "user\t8\n",
      "assortment\t1\n",
      "database\t1\n",
      "obviously\t4\n",
      "chronicles\t1\n",
      "auditor\t1\n",
      "savvy\t1\n",
      "bondholders\t1\n",
      "2963\t1\n",
      "763\t1\n",
      "settlements\t2\n",
      "reviewed\t4\n",
      "hje\t1\n",
      "retirement\t1\n",
      "guaranteed\t4\n",
      "palaeo\t1\n",
      "remaining\t3\n",
      "motivating\t1\n",
      "finalize\t1\n",
      "evaluate\t2\n",
      "game\t4\n",
      "3130\t1\n",
      "damage\t1\n",
      "listmaster\t1\n",
      "success\t25\n",
      "lemelman\t1\n",
      "undercollected\t1\n",
      "ees\t10\n",
      "'harry_wijsman\t1\n",
      "popular\t2\n",
      "soma\t1\n",
      "fathers\t1\n",
      "creation\t3\n",
      "some\t68\n",
      "pierre\t1\n",
      "urgent\t7\n",
      "lips\t1\n",
      "added\t2\n",
      "gracie\t2\n",
      "delivered\t5\n",
      "xacnax\t1\n",
      "slash\t1\n",
      "minimal\t1\n",
      "cgi\t5\n",
      "eating\t2\n",
      "rub\t1\n",
      "tlapek\t5\n",
      "processing\t8\n",
      "step\t2\n",
      "lasts\t1\n",
      "pitchers\t1\n",
      "faith\t1\n",
      "roibot\t3\n",
      "ids\t1\n",
      "papua\t1\n",
      "beetcn\t1\n",
      "experiences\t1\n",
      "block\t5\n",
      "seeing\t1\n",
      "within\t25\n",
      "pillsburywinthrop\t2\n",
      "computer\t7\n",
      "entergy's\t2\n",
      "accountability\t1\n",
      "consultation\t2\n",
      "himself\t1\n",
      "registered\t4\n",
      "'bredd\t1\n",
      "properly\t2\n",
      "info\t8\n",
      "vicqodin\t1\n",
      "ordered\t6\n",
      "amounts\t2\n",
      "'randy\t1\n",
      "'richard\t1\n",
      "nas\t1\n",
      "politicians\t1\n",
      "transport\t3\n",
      "bmar\t1\n",
      "department\t1\n",
      "draw\t5\n",
      "reportedly\t2\n",
      "visits\t1\n",
      "regulations\t3\n",
      "reservation\t1\n",
      "structure\t5\n",
      "e\t60\n",
      "dvd\t1\n",
      "required\t2\n",
      "requires\t1\n",
      "pinion\t1\n",
      "gs\t1\n",
      "oxley\t3\n",
      "gb\t1\n",
      "go\t11\n",
      "behalf\t1\n",
      "suite\t5\n",
      "kcs\t2\n",
      "telling\t1\n",
      "tonai\t1\n",
      "positions\t1\n",
      "button\t1\n",
      "tokyo\t1\n",
      "michael\t8\n",
      "inspection\t1\n",
      "zo\t2\n",
      "zk\t1\n",
      "boots\t1\n",
      "booth\t1\n",
      "download\t1\n",
      "click\t40\n",
      "cell\t1\n",
      "experiment\t1\n",
      "cele\t2\n",
      "marketer\t1\n",
      "commercial\t20\n",
      "following\t27\n",
      "copyright\t7\n",
      "642\t1\n",
      "products\t23\n",
      "overseer\t1\n",
      "examining\t1\n",
      "addresses\t5\n",
      "clark\t1\n",
      "clare\t1\n",
      "win\t18\n",
      "manage\t3\n",
      "qualityproducts\t1\n",
      "addressed\t1\n",
      "singing\t1\n",
      "cheeky\t1\n",
      "means\t6\n",
      "expressway\t1\n",
      "'msessa\t1\n",
      "hydro\t2\n",
      "denys\t1\n",
      "ecom\t1\n",
      "nowthe\t1\n",
      "ride\t2\n",
      "meet\t9\n",
      "control\t8\n",
      "links\t30\n",
      "providers\t2\n",
      "whalley\t3\n",
      "pulling\t1\n",
      "documenting\t2\n",
      "wonderful\t1\n",
      "cliickk\t2\n",
      "hesitate\t4\n",
      "arrangement\t2\n",
      "located\t4\n",
      "farm\t1\n",
      "spelling\t3\n",
      "advocates\t1\n",
      "encyclopedia\t1\n",
      "software\t13\n",
      "lounsbury\t1\n",
      "charset\t11\n",
      "including\t16\n",
      "mentioned\t5\n",
      "massey\t1\n",
      "guerrilla\t1\n",
      "acrobaat\t1\n",
      "lagrasta\t1\n",
      "hands\t2\n",
      "front\t3\n",
      "coleman\t1\n",
      "lacrecia\t1\n",
      "university\t1\n",
      "priice\t1\n",
      "wugn\t1\n",
      "mode\t1\n",
      "upward\t1\n",
      "706\t2\n",
      "700\t3\n",
      "settanni\t1\n",
      "703\t1\n",
      "measure\t1\n",
      "pavluk\t1\n",
      "special\t14\n",
      "cause\t1\n",
      "one's\t1\n",
      "attending\t1\n",
      "completely\t2\n",
      "ect\t382\n",
      "'blong\t1\n",
      "chatham\t1\n",
      "aspermont\t3\n",
      "drowning\t1\n",
      "adolescent\t1\n",
      "diminished\t1\n",
      "keep\t21\n",
      "keen\t1\n",
      "busines\t1\n",
      "succeeds\t1\n",
      "powerful\t7\n",
      "agouti\t1\n",
      "date\t5\n",
      "quality\t15\n",
      "you're\t2\n",
      "management\t54\n",
      "privacy\t6\n",
      "sellens\t1\n",
      "relations\t3\n",
      "attack\t1\n",
      "29155\t1\n",
      "stroock\t1\n",
      "final\t6\n",
      "beard\t1\n",
      "exactly\t4\n",
      "rsa\t1\n",
      "lists\t2\n",
      "outline\t2\n",
      "submitted\t1\n",
      "ber\t1\n",
      "providing\t6\n",
      "exhibit\t5\n",
      "putpeel\t8\n",
      "need\t54\n",
      "manitoba\t1\n",
      "'kelly\t1\n",
      "eops\t1\n",
      "able\t14\n",
      "purchasing\t2\n",
      "rossman\t12\n",
      "dcoit\t1\n",
      "'randall\t1\n",
      "montana\t1\n",
      "connected\t1\n",
      "expired\t1\n",
      "fabaclbo\t1\n",
      "urg\t2\n",
      "url\t3\n",
      "ecomog\t3\n",
      "impression\t3\n",
      "emerging\t2\n",
      "anyway\t2\n",
      "bybb\t1\n",
      "partners\t4\n",
      "envy\t1\n",
      "based\t15\n",
      "tire\t1\n",
      "bodyfrankfurter\t1\n",
      "ngo\t2\n",
      "employer\t1\n",
      "employee\t6\n",
      "employed\t2\n",
      "ngx\t1\n",
      "achieve\t1\n",
      "overall\t2\n",
      "120\t1\n",
      "joint\t1\n",
      "122\t1\n",
      "123\t3\n",
      "124\t3\n",
      "125\t2\n",
      "126\t2\n",
      "procedures\t3\n",
      "endless\t1\n",
      "gray\t2\n",
      "evolution\t1\n",
      "tobacco\t2\n",
      "shy\t1\n",
      "tuned\t1\n",
      "felipe\t2\n",
      "processed\t2\n",
      "wish\t7\n",
      "widow\t2\n",
      "freeze\t3\n",
      "latin\t1\n",
      "changes\t4\n",
      "desperate\t1\n",
      "7247\t1\n",
      "stats\t2\n",
      "written\t6\n",
      "luo\t1\n",
      "amadol\t2\n",
      "efficiency\t2\n",
      "key\t10\n",
      "approval\t1\n",
      "precious\t1\n",
      "nominations\t1\n",
      "thank\t9\n",
      "kincaid\t2\n",
      "claiis\t1\n",
      "ami\t1\n",
      "stratton\t1\n",
      "cernosek\t1\n",
      "jersey\t1\n",
      "34357\t1\n",
      "tuesday\t4\n",
      "addition\t5\n",
      "cent\t1\n",
      "37159\t2\n",
      "slowly\t1\n",
      "treat\t1\n",
      "neuweiler\t2\n",
      "controlled\t2\n",
      "vince\t17\n",
      "smoking\t7\n",
      "novel\t1\n",
      "stockhouse\t1\n",
      "owns\t1\n",
      "1517\t1\n",
      "xpress\t1\n",
      "generic\t6\n",
      "began\t6\n",
      "'ilydiatt\t1\n",
      "proceeds\t1\n",
      "parts\t1\n",
      "underground\t1\n",
      "party\t3\n",
      "http\t65\n",
      "absorb\t1\n",
      "effect\t2\n",
      "frequently\t1\n",
      "transaction\t5\n",
      "i\t246\n",
      "schumack\t3\n",
      "desmeules\t1\n",
      "nebulous\t1\n",
      "corporation\t4\n",
      "vastar\t8\n",
      "restore\t1\n",
      "increasingly\t1\n",
      "oak\t1\n",
      "didrex\t1\n",
      "detail\t2\n",
      "sources\t7\n",
      "advertised\t1\n",
      "coordination\t3\n",
      "mircosoft\t1\n",
      "density\t1\n",
      "deposits\t2\n",
      "accomodate\t4\n",
      "kick\t1\n",
      "akkabay\t4\n",
      "disappearance\t1\n",
      "263\t2\n",
      "260\t2\n",
      "netwww\t1\n",
      "immediately\t22\n",
      "prominent\t2\n",
      "loss\t1\n",
      "necessary\t8\n",
      "alternatively\t1\n",
      "payments\t8\n",
      "lose\t2\n",
      "52477\t1\n",
      "page\t23\n",
      "arbitrage\t4\n",
      "982\t1\n",
      "belonged\t1\n",
      "home\t20\n",
      "peter\t3\n",
      "etringer\t2\n",
      "moderates\t2\n",
      "borland\t1\n",
      "broad\t2\n",
      "rewarding\t2\n",
      "fanny\t1\n",
      "limitations\t1\n",
      "reaching\t1\n",
      "goatee\t1\n",
      "prohibited\t1\n",
      "048\t1\n",
      "eatables\t1\n",
      "carolyn\t1\n",
      "previously\t2\n",
      "washington\t1\n",
      "terence\t1\n",
      "strategy\t2\n",
      "utility\t8\n",
      "prozac\t1\n",
      "rejoice\t1\n",
      "additional\t13\n",
      "riverdeep\t1\n",
      "daniels\t1\n",
      "relegated\t1\n",
      "imperial\t2\n",
      "netherland\t1\n",
      "north\t12\n",
      "hr\t14\n",
      "hp\t1\n",
      "ht\t4\n",
      "hi\t2\n",
      "goals\t1\n",
      "hm\t1\n",
      "hc\t2\n",
      "ph\t3\n",
      "he\t46\n",
      "signed\t4\n",
      "carriage\t1\n",
      "stories\t4\n",
      "piece\t1\n",
      "universal\t1\n",
      "functions\t5\n",
      "123395\t1\n",
      "epcm\t2\n",
      "kokas\t1\n",
      "ashburton\t3\n",
      "canada\t4\n",
      "74949\t1\n",
      "star\t2\n",
      "insults\t1\n",
      "stad\t1\n",
      "additionally\t2\n",
      "agonizing\t1\n",
      "friends\t3\n",
      "ssmb\t1\n",
      "heathman\t1\n",
      "portion\t1\n",
      "'smithc\t1\n",
      "runkel\t1\n",
      "commencement\t1\n",
      "entergyr\t1\n",
      "dynamic\t2\n",
      "knelt\t1\n",
      "protest\t2\n",
      "whose\t2\n",
      "fronts\t1\n",
      "swap\t2\n",
      "sorry\t2\n",
      "updated\t1\n",
      "collaborate\t3\n",
      "affect\t2\n",
      "similes\t1\n",
      "pibrochs\t1\n",
      "skills\t2\n",
      "companies\t8\n",
      "solution\t2\n",
      "convenience\t1\n",
      "cholesterol\t1\n",
      "enhance\t3\n",
      "heading\t1\n",
      "force\t5\n",
      "'jeff\t1\n",
      "likely\t7\n",
      "even\t16\n",
      "preferences\t2\n",
      "entergy\t4\n",
      "asia\t1\n",
      "new\t66\n",
      "tips\t4\n",
      "ever\t5\n",
      "deemed\t2\n",
      "dryblower\t1\n",
      "never\t7\n",
      "drew\t3\n",
      "108\t1\n",
      "active\t1\n",
      "103\t1\n",
      "100\t18\n",
      "aren't\t3\n",
      "107\t1\n",
      "economics\t1\n",
      "credit\t20\n",
      "cindy\t5\n",
      "mime\t1\n",
      "fantastic\t1\n",
      "ipps\t6\n",
      "benedicta\t2\n",
      "welch\t1\n",
      "receivable\t1\n",
      "cough\t1\n",
      "7268\t3\n",
      "pulp\t1\n",
      "cali\t1\n",
      "6207\t1\n",
      "tele\t1\n",
      "recommend\t1\n",
      "type\t12\n",
      "tell\t13\n",
      "posting\t1\n",
      "expose\t1\n",
      "tenacity\t1\n",
      "carmody\t1\n",
      "adult\t3\n",
      "rook\t1\n",
      "room\t3\n",
      "rights\t4\n",
      "setup\t5\n",
      "roof\t2\n",
      "give\t17\n",
      "caused\t1\n",
      "stupidity\t1\n",
      "answer\t7\n",
      "loyalty\t1\n",
      "president\t9\n",
      "purchase\t11\n",
      "attempt\t1\n",
      "third\t3\n",
      "recieved\t2\n",
      "janie\t1\n",
      "maintain\t2\n",
      "goodbye\t1\n",
      "operate\t1\n",
      "operations\t41\n",
      "iname\t1\n",
      "chunk\t1\n",
      "golnaraghi\t1\n",
      "keyboard\t2\n",
      "before\t23\n",
      "symaantec\t1\n",
      "better\t10\n",
      "persist\t1\n",
      "weeks\t6\n",
      "schmidt\t4\n",
      "ecarmst\t1\n",
      "weakness\t4\n",
      "workout\t1\n",
      "grammar\t1\n",
      "arrested\t1\n",
      "went\t3\n",
      "side\t2\n",
      "bone\t1\n",
      "mean\t1\n",
      "adobe\t12\n",
      "taught\t1\n",
      "trading\t29\n",
      "aids\t1\n",
      "dawn\t1\n",
      "mandated\t1\n",
      "merchants\t1\n",
      "forhome\t1\n",
      "content\t13\n",
      "adhesion\t1\n",
      "reader\t21\n",
      "consummating\t1\n",
      "resume\t3\n",
      "normet\t2\n",
      "lineal\t1\n",
      "249\t2\n",
      "isp\t8\n",
      "iso\t10\n",
      "signature\t1\n",
      "moving\t5\n",
      "incomeunlimited\t1\n",
      "eileen\t1\n",
      "solmonson\t1\n",
      "grade\t2\n",
      "thereof\t1\n",
      "hook\t2\n",
      "featured\t2\n",
      "ditch\t1\n",
      "hood\t1\n",
      "keeley's\t3\n",
      "psychotic\t1\n",
      "recruiting\t3\n",
      "chile's\t1\n",
      "somewhat\t1\n",
      "begins\t2\n",
      "distance\t5\n",
      "congratulatory\t1\n",
      "keyword\t4\n",
      "matter\t6\n",
      "skilling\t5\n",
      "rennie\t1\n",
      "mini\t1\n",
      "mind\t15\n",
      "mine\t7\n",
      "biwven\t1\n",
      "seen\t4\n",
      "seem\t3\n",
      "tells\t1\n",
      "ckily\t1\n",
      "clonazepam\t1\n",
      "treasonous\t1\n",
      "dol\t1\n",
      "doc\t3\n",
      "medical\t1\n",
      "nighttime\t2\n",
      "dog\t1\n",
      "timekeeping\t2\n",
      "points\t3\n",
      "principle\t1\n",
      "mailing\t3\n",
      "urgency\t1\n",
      "hunger\t1\n",
      "visitor\t3\n",
      "retire\t3\n",
      "ending\t2\n",
      "judgments\t1\n",
      "explain\t2\n",
      "debts\t2\n",
      "sezgen\t4\n",
      "sugar\t3\n",
      "yowman\t1\n",
      "folks\t3\n",
      "above\t11\n",
      "stop\t15\n",
      "earn\t6\n",
      "bar\t1\n",
      "fields\t1\n",
      "facilitators\t1\n",
      "bad\t5\n",
      "beneteau\t1\n",
      "attest\t1\n",
      "decides\t1\n",
      "reference\t2\n",
      "imitate\t2\n",
      "decided\t5\n",
      "subject\t53\n",
      "brazil\t8\n",
      "03\t3\n",
      "00\t31\n",
      "01\t24\n",
      "06\t21\n",
      "07\t10\n",
      "04\t7\n",
      "inappropriate\t1\n",
      "08\t22\n",
      "09\t21\n",
      "connective\t1\n",
      "reassigned\t1\n",
      "there's\t2\n",
      "amitava\t4\n",
      "shults\t1\n",
      "credi\t1\n",
      "samarium\t1\n",
      "suggested\t1\n",
      "presccription\t1\n",
      "against\t6\n",
      "contribution\t1\n",
      "presumably\t1\n",
      "gwen\t1\n",
      "lanterns\t1\n",
      "initiative\t3\n",
      "disordersclump\t1\n",
      "34710\t1\n",
      "targeting\t1\n",
      "puts\t1\n",
      "basis\t5\n",
      "three\t17\n",
      "walpole\t1\n",
      "trigger\t2\n",
      "interest\t12\n",
      "entered\t10\n",
      "website\t28\n",
      "gerry\t2\n",
      "deeper\t1\n",
      "mcf\t2\n",
      "personally\t1\n",
      "aviation\t1\n",
      "161\t1\n",
      "exception\t1\n",
      "tang\t5\n",
      "near\t6\n",
      "neat\t1\n",
      "balance\t1\n",
      "seven\t5\n",
      "mexico\t1\n",
      "cano\t1\n",
      "is\t249\n",
      "it\t162\n",
      "ii\t3\n",
      "im\t1\n",
      "clinging\t1\n",
      "in\t417\n",
      "mouse\t5\n",
      "id\t13\n",
      "if\t107\n",
      "grown\t2\n",
      "make\t35\n",
      "trista\t2\n",
      "czkkrxht\t1\n",
      "ello\t1\n",
      "managment\t4\n",
      "delight\t1\n",
      "kim\t3\n",
      "opportunity\t9\n",
      "adminder\t2\n",
      "kid\t1\n",
      "programs\t14\n",
      "settled\t3\n",
      "strangulate\t1\n",
      "smoker\t1\n",
      "8859\t10\n",
      "materials\t2\n",
      "qualities\t1\n",
      "claims\t4\n",
      "800\t3\n",
      "yours\t3\n",
      "left\t5\n",
      "protocol\t1\n",
      "just\t47\n",
      "sentence\t2\n",
      "identify\t2\n",
      "human\t14\n",
      "yes\t7\n",
      "yet\t5\n",
      "candidate\t1\n",
      "nicolay\t1\n",
      "glencore\t2\n",
      "save\t8\n",
      "opt\t2\n",
      "involuntary\t8\n",
      "supervisors\t5\n",
      "background\t9\n",
      "destroy\t1\n",
      "shoulder\t1\n",
      "dignity\t1\n",
      "fioricet\t1\n",
      "manual\t2\n",
      "unnecessary\t1\n",
      "www\t46\n",
      "dean\t1\n",
      "deal\t15\n",
      "somehow\t1\n",
      "elderly\t1\n",
      "paragraphs\t8\n",
      "dear\t6\n",
      "amiable\t1\n",
      "tracked\t1\n",
      "grading\t1\n",
      "notification\t1\n",
      "normal\t8\n",
      "daytime\t2\n",
      "shakespeare\t3\n",
      "knew\t3\n",
      "bold\t2\n",
      "novelties\t1\n",
      "subscriptions\t1\n",
      "issler\t5\n",
      "super\t5\n",
      "transalta\t1\n",
      "afternoon\t4\n",
      "restate\t1\n",
      "252050406\t1\n",
      "automatically\t7\n",
      "whats\t1\n",
      "down\t11\n",
      "doctrine\t4\n",
      "amsterdam\t1\n",
      "frightened\t1\n",
      "initial\t7\n",
      "6761\t1\n",
      "editor\t3\n",
      "fraction\t4\n",
      "fork\t2\n",
      "offering\t7\n",
      "fireworks\t1\n",
      "unmanly\t1\n",
      "ford\t1\n",
      "229\t2\n",
      "syndicate\t1\n",
      "forp\t1\n",
      "analyst\t2\n",
      "fort\t1\n",
      "attached\t8\n",
      "whiskey\t1\n",
      "economic\t2\n",
      "sapphire\t1\n",
      "delete\t8\n",
      "strengthening\t1\n",
      "classic\t1\n",
      "sale\t10\n",
      "magellan\t1\n",
      "fraternal\t1\n",
      "agreeing\t1\n",
      "digital\t1\n",
      "2500\t1\n",
      "alleviates\t1\n",
      "081\t2\n",
      "felt\t2\n",
      "utilities\t19\n",
      "outback\t1\n",
      "journey\t1\n",
      "authorities\t1\n",
      "weekend\t3\n",
      "jones\t2\n",
      "assume\t1\n",
      "94105\t1\n",
      "daily\t3\n",
      "premiums\t2\n",
      "'michael_huse\t1\n",
      "profits\t1\n",
      "managed\t7\n",
      "woodwork\t1\n",
      "mild\t2\n",
      "manages\t3\n",
      "skin\t1\n",
      "lauri\t4\n",
      "commentaries\t3\n",
      "resourceful\t1\n",
      "hijacked\t2\n",
      "reader's\t2\n",
      "father\t4\n",
      "0\t6\n",
      "finally\t3\n",
      "marks\t2\n",
      "jubilar\t1\n",
      "advises\t1\n",
      "me\t75\n",
      "lynch\t1\n",
      "ma\t2\n",
      "did\t8\n",
      "specials\t4\n",
      "item\t3\n",
      "tortoises\t1\n",
      "talked\t1\n",
      "sergeev\t4\n",
      "run\t2\n",
      "bacterial\t1\n",
      "annals\t1\n",
      "herem\t1\n",
      "international\t7\n",
      "filled\t2\n",
      "8901_bd_shwc\t1\n",
      "transportation\t2\n",
      "elses\t2\n",
      "monies\t1\n",
      "softwares\t1\n",
      "box\t20\n",
      "boy\t2\n",
      "canadian\t1\n",
      "shift\t2\n",
      "cargill\t2\n",
      "bob\t7\n",
      "useful\t4\n",
      "merely\t1\n",
      "licensing\t1\n",
      "mscf\t1\n",
      "wealth\t2\n",
      "saave\t7\n",
      "visit\t30\n",
      "epsc\t1\n",
      "insider\t2\n",
      "sharing\t3\n",
      "acceptable\t1\n",
      "curriculum\t5\n",
      "effort\t6\n",
      "mclarney\t2\n",
      "ult\t2\n",
      "reviews\t6\n",
      "fln\t1\n",
      "growing\t1\n",
      "making\t17\n",
      "csikos\t1\n",
      "claim\t3\n",
      "crazy\t1\n",
      "agent\t3\n",
      "sample\t6\n",
      "confuses\t1\n",
      "dennis\t3\n",
      "subconsciously\t1\n",
      "get\t76\n",
      "range\t2\n",
      "till\t1\n",
      "pure\t1\n",
      "inputs\t1\n",
      "mcsherry\t1\n",
      "may\t27\n",
      "149\t1\n",
      "sustainability\t1\n",
      "membership\t19\n",
      "designed\t5\n",
      "guys\t3\n",
      "man\t5\n",
      "neck\t1\n",
      "reseller\t1\n",
      "maybe\t4\n",
      "q\t5\n",
      "switch\t1\n",
      "deposit\t4\n",
      "thorns\t1\n",
      "tall\t1\n",
      "talk\t7\n",
      "lsc\t1\n",
      "wishing\t1\n",
      "entity\t3\n",
      "stability\t3\n",
      "lst\t2\n",
      "cuts\t1\n",
      "group\t21\n",
      "monitor\t1\n",
      "lavorato\t6\n",
      "interesting\t3\n",
      "coaching\t1\n",
      "policy\t6\n",
      "mail\t32\n",
      "main\t1\n",
      "texas\t1\n",
      "finance\t4\n",
      "killer\t1\n",
      "lunch\t2\n",
      "killed\t2\n",
      "teleseminar\t2\n",
      "directories\t2\n",
      "rock\t2\n",
      "fooxr\t1\n",
      "girl\t4\n",
      "summary\t5\n",
      "gonzalez\t1\n",
      "living\t3\n",
      "bailout\t8\n",
      "pamela\t2\n",
      "emerged\t1\n",
      "tidbits\t1\n",
      "earlier\t4\n",
      "california\t12\n",
      "org\t4\n",
      "ore\t3\n",
      "advance\t4\n",
      "language\t3\n",
      "steve's\t1\n",
      "3011\t1\n",
      "925\t2\n",
      "thing\t13\n",
      "vision\t1\n",
      "cellulite\t1\n",
      "think\t11\n",
      "first\t34\n",
      "long\t18\n",
      "carry\t1\n",
      "calpine\t3\n",
      "sounds\t1\n",
      "little\t9\n",
      "anyone\t5\n",
      "esiear\t2\n",
      "speaking\t1\n",
      "mining\t7\n",
      "'bcli\t1\n",
      "memo\t3\n",
      "altra\t1\n",
      "here's\t4\n",
      "11\t15\n",
      "10\t44\n",
      "13\t13\n",
      "12\t74\n",
      "15\t25\n",
      "14\t33\n",
      "17\t19\n",
      "16\t5\n",
      "19\t9\n",
      "18\t18\n",
      "knives\t1\n",
      "venture\t5\n",
      "were\t26\n",
      "topics\t1\n",
      "voices\t1\n",
      "kevin\t48\n",
      "answering\t3\n",
      "foreigner\t1\n",
      "potential\t8\n",
      "performance\t7\n",
      "channel\t1\n",
      "201\t2\n",
      "200\t3\n",
      "pain\t2\n",
      "investigations\t1\n",
      "track\t3\n",
      "paid\t5\n",
      "assault\t1\n",
      "skinner\t3\n",
      "especially\t4\n",
      "shop\t3\n",
      "rating\t2\n",
      "shot\t1\n",
      "show\t7\n",
      "representatives\t2\n",
      "moderator\t4\n",
      "label\t2\n",
      "cornet\t2\n",
      "admixture\t1\n",
      "ges\t2\n",
      "4887_saa\t1\n",
      "contracts\t12\n",
      "nearly\t1\n",
      "gee\t1\n",
      "'llittle\t1\n",
      "morning\t12\n",
      "stupid\t1\n",
      "finishing\t1\n",
      "worrying\t1\n",
      "honesty\t1\n",
      "relative\t2\n",
      "wonder\t2\n",
      "forecast\t1\n",
      "enough\t6\n",
      "diamonds\t1\n",
      "reading\t1\n",
      "across\t10\n",
      "infrastructure\t1\n",
      "august\t1\n",
      "'mperkins\t1\n",
      "jm\t6\n",
      "jo\t1\n",
      "ja\t1\n",
      "dates\t4\n",
      "trader\t4\n",
      "according\t2\n",
      "holders\t1\n",
      "among\t7\n",
      "maintainer\t1\n",
      "exhibits\t2\n",
      "cancel\t1\n",
      "technicalities\t1\n",
      "custody\t1\n",
      "tuning\t1\n",
      "mark\t8\n",
      "mary\t7\n",
      "bmf\t1\n",
      "convincible\t1\n",
      "offered\t3\n",
      "dramatic\t1\n",
      "those\t15\n",
      "sound\t3\n",
      "litterbug\t1\n",
      "bucolic\t1\n",
      "'five\t1\n",
      "ongoing\t2\n",
      "promising\t1\n",
      "gullweig\t1\n",
      "conferences\t1\n",
      "eventually\t2\n",
      "coffee\t5\n",
      "strain\t1\n",
      "sillily\t1\n",
      "wisely\t2\n",
      "everyday\t2\n",
      "par\t2\n",
      "doctor\t2\n",
      "pay\t5\n",
      "same\t12\n",
      "intermediary\t2\n",
      "oil\t1\n",
      "assist\t7\n",
      "running\t7\n",
      "sempratrading\t1\n",
      "makecashonline\t1\n",
      "begin_split\t1\n",
      "prriicegreat\t1\n",
      "teddy's\t1\n",
      "amazed\t1\n",
      "money\t35\n",
      "enrononline\t4\n",
      "griddle\t1\n",
      "doctoral\t1\n",
      "asthma\t1\n",
      "application\t4\n",
      "6902\t1\n",
      "perth\t1\n",
      "4\t35\n",
      "income\t4\n",
      "disappointed\t1\n",
      "orleantraders\t1\n",
      "macarthur\t1\n",
      "adams\t1\n",
      "christine\t1\n",
      "server\t5\n",
      "audience\t10\n",
      "either\t9\n",
      "served\t1\n",
      "dictating\t1\n",
      "joao\t2\n",
      "users\t6\n",
      "gross\t3\n",
      "critical\t10\n",
      "inject\t1\n",
      "moderate\t2\n",
      "measuring\t3\n",
      "tomorrow\t5\n",
      "broker\t3\n",
      "847\t1\n",
      "840\t1\n",
      "island\t2\n",
      "teddy\t2\n",
      "knave\t3\n",
      "road\t2\n",
      "blessed\t2\n",
      "fuohqjlsjcqp\t1\n",
      "references\t1\n",
      "inbox\t1\n",
      "distracted\t1\n",
      "deadlines\t1\n",
      "egep\t1\n",
      "arranged\t1\n",
      "romantic\t1\n",
      "deep\t1\n",
      "general\t5\n",
      "examing\t1\n",
      "examine\t3\n",
      "file\t6\n",
      "shopping\t5\n",
      "allocating\t2\n",
      "fill\t6\n",
      "again\t12\n",
      "personnel\t7\n",
      "field\t3\n",
      "xanax\t3\n",
      "students\t4\n",
      "important\t12\n",
      "chris\t1\n",
      "pitifully\t1\n",
      "consternation\t1\n",
      "assets\t14\n",
      "zimin\t5\n",
      "869279893\t1\n",
      "u\t3\n",
      "starting\t1\n",
      "liar\t2\n",
      "forget\t1\n",
      "founder\t1\n",
      "dollar\t1\n",
      "systemslogical\t1\n",
      "zinc\t1\n",
      "talks\t3\n",
      "children\t3\n",
      "ium\t2\n",
      "civilizirano\t1\n",
      "scout\t1\n",
      "fall\t2\n",
      "wsc\t1\n",
      "washing\t4\n",
      "economically\t1\n",
      "exemptions\t2\n",
      "grabs\t1\n",
      "zero\t4\n",
      "titles\t3\n",
      "perspective\t7\n",
      "further\t9\n",
      "cigars\t1\n",
      "residue\t1\n",
      "dial\t2\n",
      "stood\t1\n",
      "abn\t1\n",
      "diaz\t2\n",
      "bedfellow\t1\n",
      "public\t2\n",
      "movement\t2\n",
      "5000\t3\n",
      "thursday's\t1\n",
      "christi\t1\n",
      "klei\t1\n",
      "favourably\t1\n",
      "nation's\t1\n",
      "curricula\t1\n",
      "publishes\t1\n",
      "900\t1\n",
      "generalizing\t1\n",
      "stelly\t1\n",
      "908\t1\n",
      "jackie\t1\n",
      "stella\t6\n",
      "airport\t2\n",
      "published\t5\n",
      "africa\t10\n",
      "establish\t8\n",
      "legislator\t1\n",
      "eye\t1\n",
      "destination\t2\n",
      "two\t24\n",
      "diamond\t2\n",
      "caboose\t1\n",
      "particular\t2\n",
      "finalization\t2\n",
      "town\t4\n",
      "none\t2\n",
      "hour\t1\n",
      "remain\t1\n",
      "paragraph\t6\n",
      "dea\t1\n",
      "dec\t11\n",
      "purchases\t3\n",
      "compare\t6\n",
      "share\t8\n",
      "socal\t4\n",
      "minimum\t2\n",
      "numbers\t9\n",
      "purchased\t1\n",
      "beale\t1\n",
      "stil\t2\n",
      "technology\t3\n",
      "affectate\t1\n",
      "'dfelsinger\t1\n",
      "cst\t1\n",
      "33597\t1\n",
      "advice\t4\n",
      "damorganjr\t1\n",
      "different\t5\n",
      "blood\t1\n",
      "coming\t5\n",
      "response\t4\n",
      "mislead\t1\n",
      "coal\t2\n",
      "wishes\t1\n",
      "responsibility\t10\n",
      "pleasure\t1\n",
      "that's\t5\n",
      "appeals\t4\n",
      "through\t18\n",
      "'dronn\t1\n",
      "24\t27\n",
      "25\t4\n",
      "26\t5\n",
      "27\t6\n",
      "20\t20\n",
      "21\t3\n",
      "22\t8\n",
      "23\t4\n",
      "pcp\t1\n",
      "28\t10\n",
      "29\t10\n",
      "late\t5\n",
      "upping\t1\n",
      "dolls\t1\n",
      "good\t23\n",
      "77056\t2\n",
      "nospam\t1\n",
      "mccullough\t1\n",
      "foday\t1\n",
      "association\t3\n",
      "easily\t4\n",
      "epmi\t2\n",
      "ports\t1\n",
      "clamp\t1\n",
      "everyone\t15\n",
      "mental\t2\n",
      "house\t1\n",
      "energy\t44\n",
      "hard\t10\n",
      "idea\t5\n",
      "extended\t3\n",
      "insurance\t6\n",
      "creditor\t1\n",
      "outpacing\t1\n",
      "participants\t2\n",
      "they'll\t2\n",
      "print\t1\n",
      "evaluation\t1\n",
      "frqee\t1\n",
      "pathetic\t1\n",
      "cmenergy\t1\n",
      "canyonu's\t1\n",
      "isps\t3\n",
      "pleasant\t1\n",
      "difficulty\t1\n",
      "members\t14\n",
      "backed\t1\n",
      "beginning\t4\n",
      "allocation\t6\n",
      "benefits\t11\n",
      "finland\t1\n",
      "kaskaskia\t1\n",
      "copper\t7\n",
      "oilshale\t1\n",
      "dont\t1\n",
      "done\t6\n",
      "utilities'transmission\t2\n",
      "twenty\t4\n",
      "least\t12\n",
      "zadorozhny\t4\n",
      "assumption\t1\n",
      "statement\t1\n",
      "observations\t2\n",
      "coastenergy\t1\n",
      "part\t9\n",
      "lehr\t1\n",
      "believe\t14\n",
      "'jonesg\t1\n",
      "b\t15\n",
      "compliments\t1\n",
      "horizons\t1\n",
      "aff_id\t1\n",
      "2020\t1\n",
      "2753\t1\n",
      "orders\t1\n",
      "bestwaytoshop\t1\n",
      "depending\t2\n",
      "100's\t1\n",
      "lifts\t1\n",
      "build\t12\n",
      "commentary\t18\n",
      "province\t1\n",
      "chart\t1\n",
      "most\t31\n",
      "significant\t4\n",
      "services\t16\n",
      "linkpaks\t4\n",
      "kg\t3\n",
      "yanowski\t1\n",
      "kk\t1\n",
      "reflections\t1\n",
      "joining\t4\n",
      "sector\t3\n",
      "particularly\t3\n",
      "rebels\t2\n",
      "obligations\t2\n",
      "commissions\t2\n",
      "relation\t1\n",
      "carefully\t4\n",
      "headlines\t4\n",
      "fine\t2\n",
      "networks\t6\n",
      "refusals\t1\n",
      "ruin\t1\n",
      "distributed\t1\n",
      "generators\t2\n",
      "8\t28\n",
      "bendickson\t2\n",
      "express\t4\n",
      "merchandise\t3\n",
      "resolve\t4\n",
      "remove\t6\n",
      "multilanguage\t1\n",
      "common\t2\n",
      "allocations\t4\n",
      "individual\t8\n",
      "please\t84\n",
      "_nextpart_001_0080_01\t1\n",
      "seeded\t1\n",
      "pushing\t1\n",
      "historical\t3\n",
      "scuttle\t1\n",
      "upgraded\t1\n",
      "supply\t4\n",
      "simple\t10\n",
      "simply\t5\n",
      "shutdown\t1\n",
      "upgrades\t2\n",
      "throughout\t4\n",
      "raise\t2\n",
      "create\t13\n",
      "creativity\t1\n",
      "secret\t6\n",
      "meeting\t19\n",
      "kathy\t1\n",
      "5022\t1\n",
      "gas\t37\n",
      "understand\t7\n",
      "prices\t8\n",
      "unnecessarily\t2\n",
      "964\t2\n",
      "965\t1\n",
      "unify\t3\n",
      "solid\t1\n",
      "bill\t8\n",
      "gxxu\t1\n",
      "elections\t1\n",
      "fun\t5\n",
      "century\t1\n",
      "cents\t7\n",
      "monday's\t1\n",
      "itself\t3\n",
      "aches\t1\n",
      "getthe\t1\n",
      "893\t1\n",
      "migraine\t1\n",
      "lilly\t1\n",
      "emphasize\t1\n",
      "copying\t1\n",
      "645\t2\n",
      "anonymise\t1\n",
      "keys\t1\n",
      "assignment\t1\n",
      "yesterday\t7\n",
      "desks\t2\n",
      "moment\t3\n",
      "purpose\t6\n",
      "task\t2\n",
      "spent\t4\n",
      "gilchrist\t1\n",
      "swiss\t2\n",
      "entry\t1\n",
      "8919\t1\n",
      "spend\t8\n",
      "howl\t1\n",
      "lottery\t6\n",
      "prevatt\t4\n",
      "dubhe\t1\n",
      "alternative\t1\n",
      "source\t5\n",
      "rge\t1\n",
      "excited\t3\n",
      "big\t10\n",
      "bid\t1\n",
      "biz\t4\n",
      "foreigners\t1\n",
      "bit\t2\n",
      "ambrose\t1\n",
      "eincomplete\t1\n",
      "indication\t1\n",
      "often\t9\n",
      "absolutely\t1\n",
      "back\t26\n",
      "accelerate\t1\n",
      "mirror\t3\n",
      "ourselves\t1\n",
      "brent\t2\n",
      "contacts\t1\n",
      "pet\t1\n",
      "decision\t2\n",
      "per\t11\n",
      "eliminate\t3\n",
      "apoligize\t2\n",
      "35615\t1\n",
      "300\t9\n",
      "continuing\t1\n",
      "virtues\t1\n",
      "306\t2\n",
      "usd\t2\n",
      "oem\t3\n",
      "from\t135\n",
      "goods\t1\n",
      "beans\t1\n",
      "consequently\t1\n",
      "jimmy\t2\n",
      "martinez\t1\n",
      "frames\t1\n",
      "lesson\t5\n",
      "eneric\t1\n",
      "bankruptcy\t20\n",
      "forward\t13\n",
      "'kenny\t1\n",
      "invite\t6\n",
      "sds\t1\n",
      "icet\t2\n",
      "hopes\t1\n",
      "directed\t1\n",
      "requesting\t1\n",
      "clips\t1\n",
      "exploration\t2\n",
      "linktocash\t1\n",
      "metal\t1\n",
      "9100\t1\n",
      "bjwl\t1\n",
      "engineering\t2\n",
      "cure\t2\n",
      "organizational\t1\n",
      "formally\t2\n",
      "lyn\t7\n",
      "d'souza\t1\n",
      "started\t3\n",
      "functionality\t4\n",
      "explaining\t2\n",
      "lend\t2\n",
      "prepared\t2\n",
      "desert\t1\n",
      "2002\t2\n",
      "2003\t19\n",
      "2000\t30\n",
      "2001\t29\n",
      "2004\t16\n",
      "2005\t3\n",
      "jason\t5\n",
      "cynthia\t1\n",
      "disintegration\t1\n",
      "graves\t2\n",
      "mower\t1\n",
      "helps\t2\n",
      "ehronline\t1\n",
      "gravel\t1\n",
      "assay\t1\n",
      "arrange\t5\n",
      "entire\t2\n",
      "joel\t4\n",
      "min\t2\n",
      "layouts\t1\n",
      "uncomfortable\t1\n",
      "copenhagen\t1\n",
      "carlo\t1\n",
      "depression\t1\n",
      "orrick\t1\n",
      "edison\t3\n",
      "confided\t1\n",
      "pipes\t2\n",
      "access\t12\n",
      "body\t9\n",
      "449\t1\n",
      "mauricio\t1\n",
      "borlan\t1\n",
      "exchange\t13\n",
      "healing\t1\n",
      "others\t4\n",
      "extreme\t1\n",
      "39\t5\n",
      "38\t5\n",
      "talent\t2\n",
      "32\t12\n",
      "31\t4\n",
      "30\t20\n",
      "37\t1\n",
      "36\t2\n",
      "35\t2\n",
      "34\t4\n",
      "tantalum\t1\n",
      "implement\t1\n",
      "sanibel\t1\n",
      "formulated\t1\n",
      "same'setbacks'\t1\n",
      "baskets\t5\n",
      "names\t7\n",
      "jpxdltmuk\t1\n",
      "cha\t1\n",
      "butler\t1\n",
      "themselves\t4\n",
      "upcoming\t1\n",
      "ecole\t1\n",
      "sokolov\t4\n",
      "payne\t2\n",
      "train\t7\n",
      "billed\t2\n",
      "pathos\t1\n",
      "subscription\t10\n",
      "account\t36\n",
      "f\t1\n",
      "tyone\t1\n",
      "hopkins\t1\n",
      "hydroxylate\t1\n",
      "chancesto\t1\n",
      "4073\t1\n",
      "mesopotamia\t1\n",
      "reserved\t3\n",
      "invoices\t1\n",
      "weissman\t5\n",
      "effectively\t5\n",
      "lamb\t1\n",
      "regions\t6\n",
      "responsibilities\t7\n",
      "winners\t2\n",
      "forest\t1\n",
      "'swidner\t1\n",
      "stock\t5\n",
      "profile\t3\n",
      "buildings\t1\n",
      "de\t3\n",
      "fran\t2\n",
      "highs\t1\n",
      "944\t1\n",
      "guru\t2\n",
      "lines\t1\n",
      "gigabyte\t1\n",
      "chief\t4\n",
      "metaphors\t1\n",
      "subsequently\t1\n",
      "engineergeodetic\t1\n",
      "meter\t13\n",
      "industries\t1\n",
      "rizzi\t2\n",
      "winning\t4\n",
      "lu\t5\n",
      "reliantenergy\t3\n",
      "lx\t1\n",
      "greater\t1\n",
      "pangs\t1\n",
      "spell\t2\n",
      "mention\t3\n",
      "daw\t1\n",
      "day\t22\n",
      "february\t10\n",
      "ondarza\t1\n",
      "identified\t1\n",
      "arlene\t2\n",
      "morris\t3\n",
      "lingerie\t2\n",
      "phonetic\t1\n",
      "identifies\t1\n",
      "activate\t4\n",
      "apologize\t3\n",
      "sexual\t2\n",
      "matt\t1\n",
      "rev\t4\n",
      "oriental\t1\n",
      "ref\t2\n",
      "red\t2\n",
      "584\t1\n",
      "phrasemake\t1\n",
      "frank\t1\n",
      "payroll\t5\n",
      "confirmations\t5\n",
      "likelihood\t2\n",
      "receipts\t1\n",
      "yard\t2\n",
      "angelova\t4\n",
      "cures\t1\n",
      "qualification\t3\n",
      "retail\t4\n",
      "facilitate\t4\n",
      "south\t15\n",
      "pgm\t1\n",
      "hey\t2\n",
      "krishnarao\t5\n",
      "reached\t3\n",
      "owning\t2\n",
      "maidens\t1\n",
      "completed\t2\n",
      "acquire\t1\n",
      "environmental\t1\n",
      "compensation\t2\n",
      "assays\t1\n",
      "mackey\t1\n",
      "visited\t1\n",
      "interrupted\t1\n",
      "have\t170\n",
      "dictate\t1\n",
      "frevert\t3\n",
      "apparently\t2\n",
      "complete\t4\n",
      "congrats\t1\n",
      "mid\t3\n",
      "mix\t2\n",
      "321\t1\n",
      "unless\t5\n",
      "eight\t2\n",
      "sally\t34\n",
      "payment\t1\n",
      "anonymizer\t1\n",
      "request\t14\n",
      "disease\t1\n",
      "occasion\t1\n",
      "trafficmultipliers\t1\n",
      "normally\t1\n",
      "selection\t4\n",
      "text\t15\n",
      "supported\t1\n",
      "staff\t5\n",
      "textual\t1\n",
      "knowledge\t4\n",
      "'glen\t1\n",
      "controls\t6\n",
      "anyone's\t2\n",
      "inferior\t1\n",
      "zaak\t2\n",
      "rush\t1\n",
      "bear\t4\n",
      "3405\t2\n",
      "3404\t1\n",
      "timing\t1\n",
      "exultant\t1\n",
      "areas\t3\n",
      "eyebrow\t2\n",
      "won't\t5\n",
      "fixed\t1\n",
      "gibson\t1\n",
      "madam\t2\n",
      "omission\t1\n",
      "checks\t1\n",
      "phases\t2\n",
      "clearing\t1\n",
      "discounts\t1\n",
      "niestrath\t1\n",
      "progress\t2\n",
      "boundary\t1\n",
      "gemstone\t1\n",
      "janet\t1\n",
      "deliver\t1\n",
      "regretful\t1\n",
      "implementing\t1\n",
      "politically\t1\n",
      "taking\t4\n",
      "agree\t3\n",
      "assure\t4\n",
      "forthis\t1\n",
      "otherwise\t1\n",
      "gone\t2\n",
      "brennan\t2\n",
      "earning\t1\n",
      "it's\t30\n",
      "darren\t2\n",
      "suzms\t1\n",
      "ah\t1\n",
      "curtain\t1\n",
      "1928\t1\n",
      "columnar\t1\n",
      "choose\t4\n",
      "cop\t2\n",
      "climate\t1\n",
      "finished\t6\n",
      "bull\t1\n",
      "fellow\t1\n",
      "6484\t1\n",
      "homes\t2\n",
      "holiday\t3\n",
      "plain\t4\n",
      "paidtosurf\t2\n",
      "appearance\t2\n",
      "value\t11\n",
      "hubs\t3\n",
      "wbom\t1\n",
      "almost\t3\n",
      "murphy\t1\n",
      "vzxoaxqhg\t1\n",
      "helped\t1\n",
      "ay\t1\n",
      "arrangements\t3\n",
      "watched\t1\n",
      "claimed\t1\n",
      "partner\t4\n",
      "administration\t3\n",
      "attatched\t2\n",
      "hendricks\t2\n",
      "reporting\t10\n",
      "ion\t1\n",
      "judgment\t1\n",
      "kimberley\t3\n",
      "insure\t7\n",
      "center\t12\n",
      "wedeliverparties\t1\n",
      "thought\t2\n",
      "procurement\t2\n",
      "position\t7\n",
      "muscle\t2\n",
      "stores\t6\n",
      "executive\t6\n",
      "domestic\t1\n",
      "adv\t1\n",
      "adr\t1\n",
      "ads\t5\n",
      "028\t1\n",
      "add\t12\n",
      "gmx\t2\n",
      "281\t1\n",
      "hurry\t4\n",
      "adm\t1\n",
      "match\t2\n",
      "resolved\t1\n",
      "fyi\t6\n",
      "fyl\t2\n",
      "like\t54\n",
      "excluding\t2\n",
      "crawled\t1\n",
      "admitted\t1\n",
      "works\t11\n",
      "authority\t1\n",
      "hair\t1\n",
      "7877\t6\n",
      "recommendation\t1\n",
      "39934\t2\n",
      "supportive\t1\n",
      "happens\t1\n",
      "est\t1\n",
      "discretion\t1\n",
      "esa\t1\n",
      "assuming\t1\n",
      "dci\t1\n",
      "host\t1\n",
      "expire\t1\n",
      "although\t3\n",
      "9643\t2\n",
      "panel\t1\n",
      "about\t52\n",
      "actual\t4\n",
      "introduced\t3\n",
      "ways\t10\n",
      "female\t3\n",
      "1980\t2\n",
      "rushed\t1\n",
      "webmaster's\t1\n",
      "biggest\t1\n",
      "globe\t3\n",
      "buy\t8\n",
      "560\t1\n",
      "bus\t4\n",
      "brand\t6\n",
      "but\t67\n",
      "embargo\t1\n",
      "transco\t2\n",
      "dangerous\t1\n",
      "j\t22\n",
      "flip\t1\n",
      "'priscilla\t1\n",
      "1488230796\t1\n",
      "minutes\t5\n",
      "replying\t1\n",
      "exceeded\t2\n",
      "hillmancarthage\t1\n",
      "relates\t1\n",
      "campus\t1\n",
      "________________________________________\t1\n",
      "49\t1\n",
      "46\t2\n",
      "47\t2\n",
      "44\t6\n",
      "45\t6\n",
      "claiming\t1\n",
      "43\t2\n",
      "40\t9\n",
      "41\t1\n",
      "out\t70\n",
      "delainey\t8\n",
      "wallow\t1\n",
      "sempra\t1\n",
      "morne\t1\n",
      "hikes\t4\n",
      "limited\t14\n",
      "sleep\t2\n",
      "invovled\t2\n",
      "appetite\t2\n",
      "hate\t4\n",
      "345\t1\n",
      "koromah\t2\n",
      "whatsoever\t3\n",
      "under\t23\n",
      "pride\t1\n",
      "349\t2\n",
      "merchant\t5\n",
      "legislators\t2\n",
      "risk\t49\n",
      "rise\t1\n",
      "every\t14\n",
      "school\t2\n",
      "'iccenergy\t1\n",
      "withers\t4\n",
      "xqirzd\t1\n",
      "enjoy\t2\n",
      "leaders\t1\n",
      "capitalisation\t1\n",
      "consistent\t1\n",
      "direct\t4\n",
      "surrounding\t1\n",
      "street\t1\n",
      "freehand\t1\n",
      "hidd\t2\n",
      "selected\t1\n",
      "zesto\t10\n",
      "conduct\t2\n",
      "supplier\t2\n",
      "hundreds\t2\n",
      "beck\t5\n",
      "studio\t2\n",
      "humility\t2\n",
      "path\t2\n",
      "beca\t1\n",
      "enron\t123\n",
      "kistler\t1\n",
      "voice\t6\n",
      "forum\t1\n",
      "changed\t1\n",
      "utilities'debts\t1\n",
      "julie\t5\n",
      "chaeap\t1\n",
      "feelings\t1\n",
      "philip\t1\n",
      "swings\t1\n",
      "would\t73\n",
      "asset\t1\n",
      "contratulations\t1\n",
      "breathe\t2\n",
      "grief\t1\n",
      "phone\t15\n",
      "excellent\t4\n",
      "outdated\t1\n",
      "must\t21\n",
      "shoot\t1\n",
      "mg\t2\n",
      "join\t10\n",
      "mc\t1\n",
      "mb\t1\n",
      "mm\t1\n",
      "mo\t1\n",
      "gxol\t1\n",
      "mw\t3\n",
      "mp\t1\n",
      "ms\t10\n",
      "mr\t10\n",
      "ernest\t1\n",
      "my\t102\n",
      "mx\t6\n",
      "986782\t1\n",
      "scheduling\t4\n",
      "premium\t3\n",
      "times\t5\n",
      "guarantee\t5\n",
      "ena\t10\n",
      "end\t18\n",
      "eng\t1\n",
      "atreus\t1\n",
      "costed\t1\n",
      "description\t2\n",
      "joel_rosel\t2\n",
      "825854664\t1\n",
      "lump\t1\n",
      "cankerworm\t1\n",
      "parallel\t1\n",
      "danny\t1\n",
      "ultimate\t1\n",
      "enter\t7\n",
      "entex\t14\n",
      "executed\t1\n",
      "over\t50\n",
      "expects\t3\n",
      "'jones\t1\n",
      "london\t4\n",
      "expectations\t4\n",
      "mongolia\t1\n",
      "writing\t7\n",
      "tilts\t1\n",
      "compelling\t1\n",
      "clu\t1\n",
      "iowa\t1\n",
      "comwww\t11\n",
      "bypasses\t1\n",
      "rental\t1\n",
      "netherlands\t2\n",
      "alex\t4\n",
      "fonts\t2\n",
      "sherlyn\t3\n",
      "filling\t1\n",
      "affiliate\t2\n",
      "each\t27\n",
      "signing\t1\n",
      "diane\t1\n",
      "saturday\t1\n",
      "god\t3\n",
      "gutierrez\t1\n",
      "got\t7\n",
      "gov\t1\n",
      "underwrite\t1\n",
      "arizona\t1\n",
      "associate\t3\n",
      "rail\t3\n",
      "eternal\t1\n",
      "'davidyi\t1\n",
      "free\t86\n",
      "fred\t1\n",
      "wanted\t6\n",
      "acrobat\t1\n",
      "crowdbut\t1\n",
      "days\t10\n",
      "filter\t1\n",
      "already\t13\n",
      "coding\t2\n",
      "primary\t3\n",
      "priority\t2\n",
      "toy\t2\n",
      "top\t9\n",
      "zxs\t1\n",
      "too\t12\n",
      "percentage\t2\n",
      "tool\t2\n",
      "serve\t6\n",
      "took\t3\n",
      "negotiating\t3\n",
      "036474336\t1\n",
      "western\t6\n",
      "classes\t1\n",
      "company's\t3\n",
      "stacey\t4\n",
      "masson\t5\n",
      "bridge\t1\n",
      "fashion\t1\n",
      "rac\t1\n",
      "ran\t2\n",
      "ram\t2\n",
      "talking\t2\n",
      "rat\t2\n",
      "ray\t4\n",
      "thoroughly\t1\n",
      "windred\t1\n",
      "contact\t31\n",
      "oblibgat\t1\n",
      "laughed\t1\n",
      "though\t6\n",
      "monadic\t1\n",
      "responsibilites\t2\n",
      "3359\t1\n",
      "peers\t1\n",
      "potency\t1\n",
      "treats\t1\n",
      "flow\t8\n",
      "buchsbaum\t1\n",
      "reputation\t2\n",
      "enterprise\t4\n",
      "takegreat\t1\n",
      "entertaining\t1\n",
      "curio\t1\n",
      "floe\t1\n",
      "cotroneo\t1\n",
      "pops\t2\n",
      "colors\t1\n",
      "earth\t1\n",
      "bail\t9\n",
      "availability\t2\n",
      "claude\t1\n",
      "delays\t2\n",
      "ocd\t1\n",
      "549\t1\n",
      "mixture\t1\n",
      "fluid\t1\n",
      "wallis\t1\n",
      "despite\t2\n",
      "report\t14\n",
      "countries\t2\n",
      "twice\t2\n",
      "searching\t2\n",
      "penance\t1\n",
      "sharpens\t1\n",
      "num\t1\n",
      "bed\t1\n",
      "capacity\t2\n",
      "finger\t1\n",
      "hopefully\t2\n",
      "approach\t2\n",
      "reiterate\t1\n",
      "jose\t1\n",
      "oxidation\t1\n",
      "however\t14\n",
      "boss\t1\n",
      "adage\t1\n",
      "news\t7\n",
      "improve\t1\n",
      "referral\t2\n",
      "fault\t1\n",
      "participating\t1\n",
      "faces\t1\n",
      "kwh\t2\n",
      "karen\t1\n",
      "towels\t1\n",
      "subscribing\t1\n",
      "trust\t8\n",
      "conference\t7\n",
      "been\t52\n",
      "quickly\t7\n",
      "beneficiary\t1\n",
      "confident\t1\n",
      "belongs\t2\n",
      "expected\t6\n",
      "loft\t2\n",
      "drugs\t2\n",
      "republicans\t2\n",
      "search\t4\n",
      "joinder\t1\n",
      "craft\t1\n",
      "westward\t1\n",
      "'jshorter\t1\n",
      "surfola\t4\n",
      "fallon\t1\n",
      "n\t13\n",
      "dept\t2\n",
      "teachers\t1\n",
      "ccprod\t3\n",
      "stopping\t3\n",
      "9434\t2\n",
      "slgavjmoqq\t1\n",
      "natives\t1\n",
      "5290\t2\n",
      "expenses\t3\n",
      "brutal\t1\n",
      "containing\t3\n",
      "suggest\t2\n",
      "wound\t1\n",
      "fe_s\t1\n",
      "several\t17\n",
      "greeted\t1\n",
      "9291\t1\n",
      "shortly\t1\n",
      "charlie\t1\n",
      "boxes\t1\n",
      "doff\t1\n",
      "savings\t1\n",
      "greatest\t2\n",
      "possesses\t1\n",
      "arousing\t1\n",
      "proposed\t3\n",
      "eliminationstop\t1\n",
      "transporting\t1\n",
      "trina\t1\n",
      "6992\t2\n",
      "cultural\t1\n",
      "judge\t3\n",
      "advanced\t1\n",
      "apart\t2\n",
      "59\t2\n",
      "58\t1\n",
      "gift\t13\n",
      "55\t4\n",
      "54\t4\n",
      "57\t1\n",
      "56\t1\n",
      "51\t1\n",
      "50\t15\n",
      "52\t3\n",
      "specific\t7\n",
      "informative\t1\n",
      "offices\t3\n",
      "officer\t3\n",
      "hunt\t1\n",
      "successfully\t1\n",
      "meters\t3\n",
      "election\t1\n",
      "escape\t2\n",
      "openpage\t1\n",
      "advertise\t2\n",
      "rhine\t1\n",
      "harriman\t1\n",
      "ica\t1\n",
      "everything\t8\n",
      "allocate\t1\n",
      "420\t1\n",
      "423\t2\n",
      "422\t4\n",
      "christmas\t20\n",
      "core\t1\n",
      "discount\t1\n",
      "lamps\t1\n",
      "eat\t3\n",
      "o'neal\t1\n",
      "dinner\t2\n",
      "plus\t4\n",
      "steadily\t1\n",
      "efforts\t2\n",
      "evelon\t1\n",
      "duke\t4\n",
      "presence\t3\n",
      "yoursuccess\t2\n",
      "giv\t1\n",
      "gis\t2\n",
      "kainantu\t2\n",
      "personage\t1\n",
      "class\t2\n",
      "instructs\t1\n",
      "head\t3\n",
      "hangover\t1\n",
      "heal\t1\n",
      "removes\t1\n",
      "pricked\t1\n",
      "hear\t3\n",
      "removed\t4\n",
      "nodded\t1\n",
      "bernice\t1\n",
      "attn\t2\n",
      "attl\t1\n",
      "frightrob\t1\n",
      "stomaching\t1\n",
      "sometime\t2\n",
      "gosnell\t1\n",
      "believable\t2\n",
      "annoying\t1\n",
      "trip\t2\n",
      "pipe\t1\n",
      "no\t52\n",
      "na\t5\n",
      "when\t33\n",
      "loosening\t1\n",
      "tin\t5\n",
      "setting\t3\n",
      "vicodin\t2\n",
      "papers\t2\n",
      "ns\t1\n",
      "nt\t1\n",
      "picture\t4\n",
      "maureen\t5\n",
      "sql\t1\n",
      "longer\t4\n",
      "bullet\t5\n",
      "serious\t1\n",
      "emove\t1\n",
      "rob\t2\n",
      "focus\t4\n",
      "deliveries\t2\n",
      "leads\t11\n",
      "llc\t2\n",
      "1949\t1\n",
      "clarification\t2\n",
      "1942\t1\n",
      "1941\t1\n",
      "1947\t1\n",
      "1944\t1\n",
      "1945\t1\n",
      "environment\t3\n",
      "quasi\t1\n",
      "promoting\t2\n",
      "stove\t1\n",
      "advantage\t1\n",
      "pompey\t1\n",
      "cook\t3\n",
      "roving\t6\n",
      "cool\t3\n",
      "customer's\t1\n",
      "impressive\t1\n",
      "level\t4\n",
      "posts\t1\n",
      "phillip\t1\n",
      "quick\t3\n",
      "slower\t1\n",
      "injunction\t1\n",
      "knowlton\t1\n",
      "epam\t2\n",
      "stands\t1\n",
      "bargaain\t1\n",
      "contracted\t2\n",
      "sandton\t2\n",
      "'bjeffrie\t1\n",
      "521\t1\n",
      "goes\t1\n",
      "reply\t5\n",
      "529\t3\n",
      "prescriptions\t1\n",
      "water\t2\n",
      "groups\t4\n",
      "strategies\t5\n",
      "toolbar\t7\n",
      "mediums\t1\n",
      "thirty\t3\n",
      "corp\t31\n",
      "janice\t1\n",
      "seward\t1\n",
      "winner\t3\n",
      "problematic\t1\n",
      "russell\t1\n",
      "mst\t1\n",
      "shanbhogue\t5\n",
      "96006681\t1\n",
      "memory\t3\n",
      "australian\t3\n",
      "today\t27\n",
      "unwarranted\t1\n",
      "roared\t1\n",
      "snhezkjzhisbpjhgx\t1\n",
      "conductor\t1\n",
      "sessions\t2\n",
      "clicking\t1\n",
      "fuel\t1\n",
      "drown\t1\n",
      "beers\t1\n",
      "cases\t2\n",
      "thousands\t3\n",
      "reflux\t1\n",
      "performer\t2\n",
      "figure\t1\n",
      "newsletters\t4\n",
      "requirements\t5\n",
      "secured\t1\n",
      "antoine\t1\n",
      "1\t68\n",
      "ensure\t3\n",
      "marketability\t2\n",
      "eighty\t1\n",
      "needs\t9\n",
      "4500\t2\n",
      "unbelievably\t1\n",
      "commodity\t8\n",
      "vanadium\t1\n",
      "2086\t1\n",
      "inform\t3\n",
      "imminent\t1\n",
      "exclusive\t1\n",
      "canary\t1\n",
      "blocked\t2\n",
      "he's\t1\n",
      "apex\t1\n",
      "organisation\t1\n",
      "platform\t2\n",
      "offers\t2\n",
      "farmer\t5\n",
      "conversations\t1\n",
      "processes\t2\n",
      "qualified\t1\n",
      "term\t8\n",
      "name\t22\n",
      "possibilities\t1\n",
      "authenticity\t1\n",
      "adequately\t1\n",
      "sunrise\t1\n",
      "plans\t2\n",
      "profit\t6\n",
      "she\t18\n",
      "attended\t1\n",
      "albuquerque\t2\n",
      "addtional\t1\n",
      "comclick\t1\n",
      "turn\t3\n",
      "place\t20\n",
      "redhat\t1\n",
      "revenue\t2\n",
      "acce\t1\n",
      "unattainable\t2\n",
      "george\t9\n",
      "millions\t2\n",
      "earths\t1\n",
      "district\t4\n",
      "ebl\t1\n",
      "exploring\t2\n",
      "releases\t2\n",
      "season\t1\n",
      "weightwheezy\t1\n",
      "released\t4\n",
      "shirley\t18\n",
      "copy\t12\n",
      "wide\t1\n",
      "require\t3\n",
      "r\t16\n",
      "sera\t2\n",
      "cards\t5\n",
      "'steve\t1\n",
      "and\t668\n",
      "pro\t2\n",
      "dynamically\t1\n",
      "exchanging\t1\n",
      "sells\t2\n",
      "rea's\t1\n",
      "ffffa\t2\n",
      "any\t77\n",
      "anz\t2\n",
      "superman\t1\n",
      "anyare\t1\n",
      "ideas\t6\n",
      "sellinternetaccess\t5\n",
      "emphasis\t1\n",
      "loczk\t1\n",
      "strengths\t2\n",
      "surf\t5\n",
      "icop\t1\n",
      "sure\t11\n",
      "multiple\t7\n",
      "printable\t2\n",
      "precluded\t1\n",
      "remunerate\t1\n",
      "visitors\t2\n",
      "icon\t1\n",
      "rosalie\t1\n",
      "considered\t1\n",
      "later\t12\n",
      "hungry\t1\n",
      "senior\t11\n",
      "cheap\t1\n",
      "orgtrade\t1\n",
      "alexios\t1\n",
      "state\t30\n",
      "incessantly\t1\n",
      "explicitly\t1\n",
      "believing\t1\n",
      "glandular\t1\n",
      "burton's\t2\n",
      "expectation\t2\n",
      "bareback\t1\n",
      "homestead\t1\n",
      "closure\t1\n",
      "regarding\t2\n",
      "pinnacle\t1\n",
      "floods\t1\n",
      "fainted\t1\n",
      "naked\t1\n",
      "cashpromotions\t4\n",
      "college\t3\n",
      "apologies\t2\n",
      "com'\t69\n",
      "encouraged\t3\n",
      "fails\t1\n",
      "voluntary\t1\n",
      "mortgage\t3\n",
      "federal\t4\n",
      "review\t12\n",
      "outside\t3\n",
      "arrival\t1\n",
      "byee\t2\n",
      "presas\t2\n",
      "come\t19\n",
      "successes\t3\n",
      "region\t6\n",
      "contract\t10\n",
      "senator\t5\n",
      "tendererlycopodium\t1\n",
      "merry\t2\n",
      "color\t19\n",
      "period\t8\n",
      "sampling\t1\n",
      "60\t6\n",
      "61\t2\n",
      "medication\t1\n",
      "64\t1\n",
      "66\t1\n",
      "67\t2\n",
      "68\t1\n",
      "69\t1\n",
      "dreamwaver\t2\n",
      "subscribed\t4\n",
      "howard\t14\n",
      "deposited\t3\n",
      "hardly\t2\n",
      "500\t8\n",
      "peaking\t1\n",
      "direction\t4\n",
      "careful\t1\n",
      "spirit\t2\n",
      "pilot\t6\n",
      "paying\t3\n",
      "myself\t1\n",
      "caucasians\t1\n",
      "cash\t18\n",
      "premature\t1\n",
      "telephone\t3\n",
      "someone\t11\n",
      "minds\t1\n",
      "erroneously\t1\n",
      "author\t4\n",
      "7578\t1\n",
      "granted\t1\n",
      "bolnisi\t1\n",
      "html\t9\n",
      "reminder\t1\n",
      "events\t2\n",
      "status\t1\n",
      "watchfully\t1\n",
      "drives\t1\n",
      "weed\t1\n",
      "producers\t3\n",
      "persons\t1\n",
      "changing\t1\n",
      "weep\t1\n",
      "minimize\t1\n",
      "ranch\t10\n",
      "without\t10\n",
      "components\t1\n",
      "tinned\t1\n",
      "modem\t1\n",
      "bodies\t1\n",
      "nd\t4\n",
      "clon\t2\n",
      "actress\t1\n",
      "harrison\t2\n",
      "suprervisagra\t1\n",
      "guides\t1\n",
      "announcement\t7\n",
      "wyn\t2\n",
      "aqoj\t1\n",
      "hint\t1\n",
      "we've\t5\n",
      "everyone's\t2\n",
      "seems\t4\n",
      "except\t1\n",
      "kimberly\t5\n",
      "thimble\t1\n",
      "ross\t4\n",
      "scheduled\t3\n",
      "comhttp\t1\n",
      "traffic\t10\n",
      "nepco\t1\n",
      "accepted\t1\n",
      "towel\t1\n",
      "reduces\t2\n",
      "tower\t2\n",
      "reduced\t1\n",
      "pacific\t3\n",
      "respect\t4\n",
      "thuraisingham\t6\n",
      "provided\t3\n",
      "mood\t1\n",
      "legal\t3\n",
      "whitehorse\t1\n",
      "provides\t2\n",
      "provider\t3\n",
      "quitting\t1\n",
      "communicate\t2\n",
      "mclafferty\t1\n",
      "oo\t6\n",
      "on\t271\n",
      "ok\t4\n",
      "oh\t2\n",
      "of\t566\n",
      "discussed\t2\n",
      "stand\t2\n",
      "heartburnwestfield\t1\n",
      "gregory\t1\n",
      "or\t166\n",
      "op\t4\n",
      "mcmullen\t2\n",
      "lycopodium\t1\n",
      "communication\t9\n",
      "accounts\t1\n",
      "determine\t1\n",
      "gary\t6\n",
      "burton\t11\n",
      "strictly\t1\n",
      "there\t54\n",
      "disposed\t1\n",
      "valley\t3\n",
      "regard\t9\n",
      "cabinet\t1\n",
      "promote\t2\n",
      "faster\t2\n",
      "skilling's\t2\n",
      "wholesale\t15\n",
      "toilet\t1\n",
      "taylor\t3\n",
      "intra\t1\n",
      "hit\t2\n",
      "onerous\t1\n",
      "blues\t2\n",
      "idiot\t1\n",
      "5\t34\n",
      "netrepreneur\t1\n",
      "announcements\t4\n",
      "trash\t1\n",
      "immunity\t1\n",
      "igh\t1\n",
      "separate\t7\n",
      "includes\t1\n",
      "mmbtu\t6\n",
      "smaller'\t1\n",
      "bullets\t1\n",
      "included\t8\n",
      "funding\t1\n",
      "seventhpower\t4\n",
      "procrustes\t1\n",
      "pagemaker\t2\n",
      "wife\t3\n",
      "invest\t4\n",
      "websites\t2\n",
      "refinances\t2\n",
      "missouri\t1\n",
      "all\t111\n",
      "chinese\t1\n",
      "lack\t1\n",
      "783518\t1\n",
      "executing\t2\n",
      "generalize\t1\n",
      "dish\t3\n",
      "follow\t10\n",
      "settlement\t2\n",
      "pts\t1\n",
      "opportunities\t7\n",
      "removal\t1\n",
      "program\t7\n",
      "present\t2\n",
      "crest\t1\n",
      "'taylorja\t1\n",
      "activities\t10\n",
      "fax\t9\n",
      "worse\t1\n",
      "far\t5\n",
      "myinput\t4\n",
      "fat\t1\n",
      "worst\t1\n",
      "ticket\t2\n",
      "uwe\t4\n",
      "list\t18\n",
      "lisa\t2\n",
      "clinches\t1\n",
      "tel\t3\n",
      "tea\t2\n",
      "reins\t1\n",
      "rate\t33\n",
      "design\t9\n",
      "sue\t3\n",
      "sum\t3\n",
      "eminent\t2\n",
      "version\t9\n",
      "sup\t1\n",
      "guns\t1\n",
      "christian\t1\n",
      "unprofessional\t1\n",
      "certainly\t4\n",
      "directions\t4\n",
      "1848\t1\n",
      "allows\t3\n",
      "schott\t1\n",
      "options\t2\n",
      "proxy\t7\n",
      "texts\t2\n",
      "semester\t1\n",
      "proceed\t2\n",
      "rmation\t1\n",
      "charge\t11\n",
      "branded\t1\n",
      "hurrah\t1\n",
      "minor\t1\n",
      "flat\t5\n",
      "knows\t2\n",
      "grabbing\t1\n",
      "redesigns\t1\n",
      "operators\t3\n",
      "problem\t5\n",
      "stick\t2\n",
      "known\t4\n",
      "glad\t1\n",
      "presumed\t1\n",
      "pilocystic\t1\n",
      "forbearance\t4\n",
      "v\t7\n",
      "challenging\t3\n",
      "division\t1\n",
      "court\t4\n",
      "goal\t5\n",
      "explaing\t2\n",
      "breaking\t1\n",
      "johannesburg\t2\n",
      "speeding\t1\n",
      "simplicity\t4\n",
      "reflect\t1\n",
      "catalog\t5\n",
      "disable\t1\n",
      "capitalize\t2\n",
      "concerns\t4\n",
      "short\t8\n",
      "susan\t9\n",
      "keywordcount\t1\n",
      "ruzika\t1\n",
      "255326837\t1\n",
      "rogram\t1\n",
      "don't\t32\n",
      "developed\t5\n",
      "style\t1\n",
      "pray\t1\n",
      "resort\t2\n",
      "strange\t2\n",
      "idirect\t1\n",
      "might\t12\n",
      "argentine\t4\n",
      "return\t8\n",
      "hunter\t2\n",
      "milestone\t1\n",
      "communities\t3\n",
      "vaughn\t3\n",
      "instructions\t4\n",
      "ranen\t1\n",
      "hinsch\t2\n",
      "weight\t2\n",
      "generation\t5\n",
      "tamarchenko\t4\n",
      "dclemons\t1\n",
      "expect\t9\n",
      "stilled\t1\n",
      "chartroom\t1\n",
      "aqmd\t2\n",
      "health\t2\n",
      "friday\t6\n",
      "randall\t1\n",
      "teach\t1\n",
      "generate\t1\n",
      "netnoteinc\t6\n",
      "threat\t4\n",
      "takriti\t5\n",
      "feel\t19\n",
      "feet\t1\n",
      "notify\t2\n",
      "epenergy\t5\n",
      "says\t3\n",
      "story\t17\n",
      "script\t3\n",
      "automobile\t1\n",
      "gpg\t1\n",
      "passed\t1\n",
      "gpt\t1\n",
      "store\t2\n",
      "headache\t1\n",
      "option\t1\n",
      "relieved\t1\n",
      "treasurer\t1\n",
      "7358\t2\n",
      "9237\t1\n",
      "rhinopez\t1\n",
      "54804\t1\n",
      "king\t5\n",
      "kind\t7\n",
      "viag\t1\n",
      "double\t2\n",
      "outrageous\t1\n",
      "architect\t1\n",
      "lease\t1\n",
      "dana\t1\n",
      "outstanding\t4\n",
      "well\t25\n",
      "wilson\t2\n",
      "securitization\t3\n",
      "check\t3\n",
      "sherri\t2\n",
      "efs\t1\n",
      "chairman\t4\n",
      "favorably\t1\n",
      "finding\t1\n",
      "competencies\t2\n",
      "cemented\t1\n",
      "electric\t1\n",
      "intervened\t2\n",
      "thronged\t1\n",
      "reach\t7\n",
      "77\t1\n",
      "76\t1\n",
      "75\t5\n",
      "jsp\t3\n",
      "71\t1\n",
      "70\t1\n",
      "nothing\t5\n",
      "78\t2\n",
      "mineral\t2\n",
      "windows\t12\n",
      "inlcuding\t1\n",
      "traditional\t2\n",
      "flustrated\t1\n",
      "institutions\t1\n",
      "journeys\t1\n",
      "rto\t1\n",
      "doctors\t2\n",
      "font\t3\n",
      "geomatics\t1\n",
      "delinquent\t1\n",
      "disordersshrink\t1\n",
      "viagra\t5\n",
      "betray\t1\n",
      "ien\t2\n",
      "accurate\t1\n",
      "clemmons\t1\n",
      "yourmembership\t2\n",
      "his\t39\n",
      "founding\t1\n",
      "cote\t3\n",
      "him\t9\n",
      "pbem\t1\n",
      "vincent\t5\n",
      "wrote\t3\n",
      "consciously\t1\n",
      "art\t2\n",
      "achieved\t1\n",
      "cashpo\t2\n",
      "intelligence\t1\n",
      "are\t169\n",
      "chambers\t1\n",
      "ark\t1\n",
      "arm\t1\n",
      "learns\t2\n",
      "threats\t2\n",
      "unions\t2\n",
      "numerous\t1\n",
      "tactically\t1\n",
      "korea\t1\n",
      "recently\t5\n",
      "creating\t2\n",
      "uuz\t1\n",
      "initially\t4\n",
      "sold\t2\n",
      "attention\t3\n",
      "succeed\t3\n",
      "opposition\t2\n",
      "c\t18\n",
      "oversee\t3\n",
      "obligationin\t1\n",
      "roman\t4\n",
      "became\t4\n",
      "annie\t2\n",
      "bond\t1\n",
      "dutyfreesoft\t1\n",
      "reasons\t1\n",
      "sweet\t3\n",
      "harbour\t1\n",
      "patricia\t4\n",
      "ravi\t6\n",
      "slots\t1\n",
      "sergeev's\t1\n",
      "ps\t1\n",
      "political\t3\n",
      "due\t9\n",
      "appears\t3\n",
      "pc\t5\n",
      "reduction\t4\n",
      "pg\t7\n",
      "brick\t5\n",
      "po\t1\n",
      "pl\t1\n",
      "pm\t37\n",
      "flight\t1\n",
      "buck\t1\n",
      "transferring\t2\n",
      "1864\t1\n",
      "demand\t3\n",
      "hillis\t1\n",
      "plants\t2\n",
      "georgia\t1\n",
      "8561513507\t1\n",
      "209318\t1\n",
      "batch\t3\n",
      "bachelors\t1\n",
      "hulmeville\t1\n",
      "annuallouy\t1\n",
      "rohan\t1\n",
      "rid\t1\n",
      "9\t41\n",
      "advise\t2\n",
      "higher\t4\n",
      "literature\t1\n",
      "holidays\t2\n",
      "liquids\t1\n",
      "birch\t1\n",
      "buybacks\t1\n",
      "lower\t1\n",
      "daren\t7\n",
      "dancing\t1\n",
      "anybody\t1\n",
      "analysis\t3\n",
      "edge\t1\n",
      "cryogenic\t1\n",
      "reliant\t4\n",
      "bytesize\t1\n",
      "competitive\t3\n",
      "questions\t36\n",
      "retired\t2\n",
      "profitable\t1\n",
      "tablet\t1\n",
      "recognizing\t4\n",
      "appellate\t1\n",
      "motero\t1\n",
      "herbalonline\t1\n",
      "customers\t4\n",
      "vendor\t2\n",
      "amended\t1\n",
      "674\t1\n",
      "ambien\t2\n",
      "advocate\t1\n",
      "unlimited\t4\n",
      "cowry\t1\n",
      "collect\t2\n",
      "arthur\t1\n",
      "pcenergy\t3\n",
      "catalyze\t2\n",
      "newsletter\t24\n",
      "gradually\t1\n",
      "innovation\t4\n",
      "teosrest\t1\n",
      "usavity\t1\n",
      "martin\t7\n",
      "intense\t1\n",
      "handled\t2\n",
      "pipeline\t3\n",
      "schaffer\t1\n",
      "completing\t1\n",
      "firing\t1\n",
      "nightgear\t1\n",
      "complimentary\t1\n",
      "blainey\t1\n",
      "'sdba\t1\n",
      "question\t11\n",
      "fast\t4\n",
      "vendors\t1\n",
      "we're\t1\n",
      "impresses\t1\n",
      "analyze\t1\n",
      "files\t3\n",
      "incredibily\t1\n",
      "discoount\t1\n",
      "copartnery\t1\n",
      "berkovitz\t2\n",
      "crank\t1\n",
      "repeatedly\t2\n",
      "filed\t6\n",
      "raising\t1\n",
      "angels\t2\n",
      "highlight\t3\n",
      "z\t1\n",
      "called\t3\n",
      "shandong\t1\n",
      "associated\t4\n",
      "worthwhile\t1\n",
      "0011\t1\n",
      "kathryn\t3\n",
      "coordinator\t1\n",
      "peace\t4\n",
      "gra\t3\n",
      "nice\t3\n",
      "dale\t1\n",
      "mustard\t1\n",
      "problems\t5\n",
      "helping\t3\n",
      "9213\t1\n",
      "wrinkle\t1\n",
      "vice\t9\n",
      "scroll\t3\n",
      "once\t8\n",
      "issued\t4\n",
      "resistance\t1\n",
      "steve\t1\n",
      "012\t4\n",
      "edt\t2\n",
      "issues\t19\n",
      "stable\t1\n",
      "include\t12\n",
      "confirmation\t3\n",
      "ague\t1\n",
      "713\t6\n",
      "foretell\t1\n",
      "drinking\t2\n",
      "neaarby\t1\n",
      "notes\t1\n",
      "deals\t3\n",
      "businesses\t4\n",
      "noted\t4\n",
      "disclaimer\t1\n",
      "pleas\t1\n",
      "smaller\t6\n",
      "you'd\t1\n",
      "alpra\t2\n",
      "traveling\t1\n",
      "acid\t1\n",
      "bdf\t1\n",
      "showcase\t20\n",
      "find\t5\n",
      "capital\t7\n",
      "userconf\t1\n",
      "chokshi\t1\n",
      "larger\t3\n",
      "leaving\t2\n",
      "engine\t3\n",
      "'larry\t1\n",
      "'don\t1\n",
      "sheila\t4\n",
      "soundmax\t1\n",
      "webmail\t1\n",
      "duck\t1\n",
      "apply\t4\n",
      "stukm\t1\n",
      "use\t43\n",
      "fee\t6\n",
      "feb\t3\n",
      "usb\t1\n",
      "few\t12\n",
      "txuelectric\t1\n",
      "examination\t1\n",
      "expanding\t1\n",
      "carroll\t6\n",
      "factor\t1\n",
      "edison's\t1\n",
      "jirapliegao\t1\n",
      "women\t3\n",
      "annoy\t1\n",
      "anywhere\t3\n",
      "lagars\t1\n",
      "tax\t1\n",
      "tag\t1\n",
      "something\t8\n",
      "serial\t2\n",
      "dwr\t3\n",
      "sir\t2\n",
      "united\t5\n",
      "sit\t1\n",
      "enron_development\t2\n",
      "six\t3\n",
      "traders\t1\n",
      "brian\t1\n",
      "instead\t3\n",
      "hurdles\t1\n",
      "attend\t7\n",
      "abuse\t2\n",
      "militarism\t1\n",
      "highlands\t2\n",
      "light\t1\n",
      "chapman\t4\n",
      "manganese\t1\n",
      "extremal\t1\n",
      "martina\t4\n",
      "americ\t2\n",
      "'dkohler\t1\n",
      "own\t15\n",
      "lamproite\t2\n",
      "looks\t5\n",
      "134\t1\n",
      "boosts\t1\n",
      "anonymously\t1\n",
      "inlet\t4\n",
      "ginsu\t1\n",
      "nostalgia\t1\n",
      "yesterdays\t1\n",
      "orange\t1\n",
      "bye\t1\n",
      "originator\t2\n",
      "citrus\t2\n",
      "material\t1\n",
      "untouchable\t1\n",
      "impaired\t1\n",
      "investor\t1\n",
      "articles\t2\n",
      "edit\t11\n",
      "lifetime\t1\n",
      "bills\t6\n",
      "freeonline\t1\n",
      "mitral\t1\n",
      "myinputare\t1\n",
      "nommensen\t1\n",
      "related\t2\n",
      "89\t1\n",
      "82\t1\n",
      "our\t119\n",
      "80\t5\n",
      "81\t2\n",
      "87\t3\n",
      "ebooks\t1\n",
      "category\t2\n",
      "sentiment\t1\n",
      "stentofon\t2\n",
      "supports\t3\n",
      "integrated\t2\n",
      "dictionary\t1\n",
      "anyhow\t5\n",
      "ipos\t1\n",
      "g\t32\n",
      "phrases\t3\n",
      "rabey\t1\n",
      "conversation\t1\n",
      "ltd\t2\n",
      "hence\t1\n",
      "noticed\t1\n",
      "doesn't\t10\n",
      "unknown\t6\n",
      "cinergy\t1\n",
      "their\t40\n",
      "1500\t1\n",
      "reversed\t2\n",
      "jvbe\t1\n",
      "regards\t7\n",
      "july\t2\n",
      "chisholm\t1\n",
      "commence\t2\n",
      "succeeded\t1\n",
      "riches\t1\n",
      "tommy\t1\n",
      "grandfathered\t1\n",
      "moviebuff\t2\n",
      "creditors\t4\n",
      "cohen\t1\n",
      "petteway\t1\n",
      "afraid\t4\n",
      "which\t47\n",
      "divert\t1\n",
      "'taylor\t1\n",
      "miningnews\t9\n",
      "who\t27\n",
      "why\t11\n",
      "neighboring\t2\n",
      "looked\t1\n",
      "ho\t3\n",
      "brenda\t1\n",
      "chances\t4\n",
      "agreed\t2\n",
      "democrats\t3\n",
      "fear\t4\n",
      "eyes\t2\n",
      "pleased\t3\n",
      "'connie\t1\n",
      "micros\t2\n",
      "operational\t6\n",
      "combo\t1\n",
      "local\t6\n",
      "'pete\t1\n",
      "7394\t1\n",
      "sengupta\t1\n",
      "999\t3\n",
      "words\t8\n",
      "coordinating\t1\n",
      "view\t11\n",
      "europe\t1\n",
      "requirement\t1\n",
      "'jnexon\t1\n",
      "module\t1\n",
      "multipart\t1\n",
      "ebs\t1\n",
      "responses\t8\n",
      "closer\t1\n",
      "superb\t1\n",
      "2666\t1\n",
      "favor\t4\n",
      "closed\t1\n",
      "bought\t3\n",
      "ability\t2\n",
      "opening\t4\n",
      "amex\t1\n",
      "job\t3\n",
      "takeover\t2\n",
      "joe\t1\n",
      "swift\t2\n",
      "jon\t1\n",
      "rodney\t1\n",
      "safely\t2\n",
      "unclear\t1\n",
      "wall\t6\n",
      "subscribe\t7\n",
      "table\t2\n",
      "735670\t1\n",
      "amigo\t1\n",
      "charts\t2\n",
      "731\t1\n",
      "agreements\t3\n",
      "olson\t2\n",
      "738\t1\n",
      "mike\t11\n",
      "nickel\t4\n",
      "concernig\t2\n",
      "ensuring\t2\n",
      "hme\t1\n",
      "painter\t1\n",
      "unlike\t1\n",
      "overuse\t1\n",
      "will\t234\n",
      "readies\t1\n",
      "thus\t1\n",
      "perhaps\t3\n",
      "vintage\t2\n",
      "cross\t3\n",
      "member\t21\n",
      "largest\t2\n",
      "units\t4\n",
      "gets\t3\n",
      "privileged\t1\n",
      "difficult\t5\n",
      "premiere\t2\n",
      "conceived\t1\n",
      "romeo\t1\n",
      "kaolin\t1\n",
      "laborious\t1\n",
      "lobby\t1\n",
      "valeria\t1\n",
      "katamail\t1\n",
      "banging\t1\n",
      "carrie\t1\n",
      "obtain\t4\n",
      "recap\t3\n",
      "'scampbell\t1\n",
      "sankoh\t3\n",
      "point\t15\n",
      "smith\t7\n",
      "capabilities\t1\n",
      "hall\t2\n",
      "nicki\t1\n",
      "attractive\t2\n",
      "usage\t4\n",
      "barrow\t1\n",
      "execute\t1\n",
      "know\t42\n",
      "press\t4\n",
      "redesign\t2\n",
      "incredibly\t1\n",
      "helpful\t2\n",
      "miami\t1\n",
      "2497\t1\n",
      "loses\t2\n",
      "georgel\t2\n",
      "exceed\t1\n",
      "because\t19\n",
      "d'ivoire\t3\n",
      "preposition\t1\n",
      "growth\t4\n",
      "employment\t1\n",
      "lead\t10\n",
      "mines\t1\n",
      "instantaneous\t1\n",
      "leap\t1\n",
      "analyses\t1\n",
      "stay\t2\n",
      "pike\t1\n",
      "rare\t1\n",
      "carried\t1\n",
      "extension\t1\n",
      "getting\t8\n",
      "shipping\t1\n",
      "mayerbrown\t1\n",
      "warranty\t1\n",
      "512517\t2\n",
      "kinda\t2\n",
      "owe\t1\n",
      "warrants\t2\n",
      "eo\t1\n",
      "weather\t2\n",
      "promise\t1\n",
      "fired\t1\n",
      "van\t3\n",
      "val\t3\n",
      "texaco\t6\n",
      "transfer\t12\n",
      "var\t1\n",
      "i've\t9\n",
      "funds\t10\n",
      "volume\t9\n",
      "bannersgomlm\t8\n",
      "automated\t2\n",
      "squeeze\t1\n",
      "made\t15\n",
      "whether\t4\n",
      "protesting\t1\n",
      "record\t2\n",
      "below\t16\n",
      "ruling\t2\n",
      "demonstrate\t2\n",
      "mutual\t1\n",
      "incredible\t1\n",
      "percent\t1\n",
      "book\t5\n",
      "sick\t1\n",
      "er\t2\n",
      "junk\t2\n",
      "kinds\t2\n",
      "june\t11\n",
      "inherently\t1\n",
      "ceos\t1\n",
      "incurred\t3\n",
      "txuenergy\t3\n",
      "ranks\t2\n",
      "volumes\t1\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-2\n",
    "# Print the output file which contains the sorted numbers\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 12:52:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 1st reducer to the local file system\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-2/part-00000 word_counts.txt\n",
    "# Read the word_counts.txt input file into HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal word_counts.txt /user/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.2.1** Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)\n",
    "\n",
    "ANSWER:  The top-10 occurring tokens are as follows:  \n",
    "1. the - 1247\n",
    "2. to - 963\n",
    "3. and - 668\n",
    "4. of - 566\n",
    "5. a - 542\n",
    "6. you - 432\n",
    "7. in - 417\n",
    "8. your - 394\n",
    "9. ect - 382\n",
    "10. for - 373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 1 code for HW2.2.  Identity mapper.\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:    \n",
    "    print \"%s\" % (line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW2.2.  Identity reducer that prints its first 10 inputs.\n",
    "\n",
    "import sys\n",
    "i=0\n",
    "for line in sys.stdin:  \n",
    "    if i < 10:\n",
    "        print \"%s\" % (line.strip())\n",
    "    else:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:36:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:36:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-2-1/_SUCCESS\n",
      "16/06/04 13:36:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-2-1/part-00000\n",
      "16/06/04 13:36:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:36:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar3189511823649876900/] [] /tmp/streamjob422449474231994691.jar tmpDir=null\n",
      "16/06/04 13:36:32 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:36:32 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:36:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:36:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:36:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0275\n",
      "16/06/04 13:36:33 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0275\n",
      "16/06/04 13:36:33 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0275/\n",
      "16/06/04 13:36:33 INFO mapreduce.Job: Running job: job_1463787494457_0275\n",
      "16/06/04 13:36:38 INFO mapreduce.Job: Job job_1463787494457_0275 running in uber mode : false\n",
      "16/06/04 13:36:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:36:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:36:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:36:51 INFO mapreduce.Job: Job job_1463787494457_0275 completed successfully\n",
      "16/06/04 13:36:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=69959\n",
      "\t\tFILE: Number of bytes written=498132\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=55600\n",
      "\t\tHDFS: Number of bytes written=77\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6606\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3406\n",
      "\t\tTotal time spent by all map tasks (ms)=6606\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3406\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6606\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3406\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6764544\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3487744\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5490\n",
      "\t\tMap output records=5490\n",
      "\t\tMap output bytes=58973\n",
      "\t\tMap output materialized bytes=69965\n",
      "\t\tInput split bytes=186\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5490\n",
      "\t\tReduce shuffle bytes=69965\n",
      "\t\tReduce input records=5490\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=10980\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=173\n",
      "\t\tCPU time spent (ms)=1830\n",
      "\t\tPhysical memory (bytes) snapshot=651616256\n",
      "\t\tVirtual memory (bytes) snapshot=6292680704\n",
      "\t\tTotal committed heap usage (bytes)=504365056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=55414\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=77\n",
      "16/06/04 13:36:51 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-2-1\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous runs of this command\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-2-1/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-2-1\n",
    "# Run a Hadoop streaming job.  The input file 'enronemail_1h.txt' is passed as input.\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-D stream.num.map.output.key.field=2 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k2,2 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr\" \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/word_counts.txt -output /user/hadoop/outputHW2-2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 12:57:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-26 12:57 /user/hadoop/outputHW2-2-1/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup         77 2016-05-26 12:57 /user/hadoop/outputHW2-2-1/part-00000\n",
      "16/05/26 12:57:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the\t1247\n",
      "to\t963\n",
      "and\t668\n",
      "of\t566\n",
      "a\t542\n",
      "you\t432\n",
      "in\t417\n",
      "your\t394\n",
      "ect\t382\n",
      "for\t373\n"
     ]
    }
   ],
   "source": [
    "# Print out what files were created from the MapReduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-2-1\n",
    "# Print the output file which contains the sorted numbers\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-2-1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.3.** Multinomial NAIVE BAYES with NO Smoothing.  Using the Enron data from HW1 and Hadoop MapReduce, write a mapper/reducer job(s) that will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters).  NOTE: please assume one reducer.\n",
    "\n",
    "**HW2.3.1.** Count up how many times you need to process a zero probabilty for each class and report.  \n",
    "ANSWER:  \n",
    "Number of zero probabilities for Ham class:  5617  \n",
    "Number of zero probabilities for Spam class:  4960  \n",
    "\n",
    "**HW2.3.2.** Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier.  \n",
    "ANSWER:  Training Error for HW2.3: 0% (0/100 documents misclassified)\n",
    "\n",
    "**HW2.3.3.** Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see.  \n",
    "ANSWER:  (See histograms below.)  Histograms for both the posterior probabilites and log posterior probabilities (called scores for the rest of this problem) were plotted.  The probability histograms suffer from floating-point underflow and will not be used.  For the scores histograms, the spam scores for the ham documents were assigned a very large negative number by the model.  (And also the ham scores for the spam documents were assigned a very large negative number.)  This number has been adjusted to -30,000 so that the values display in a more readable way in the histogram.  \n",
    "\n",
    "The ham scores histogram shows the 44 documents that are classified as spam at the -30,000 frequency.  For the rest of the documents (the ham documents), there is a negatively skewed distribution with values accumulating just below 0.  The range of the values is from -9711.807285 to -29.483180.  The average value is -1682.705902.  The median value is -1094.666381.\n",
    "\n",
    "The spam scores histogram shows the 56 documents that are classified as spam at the -30,000 frequency.  For the rest of the documents (the spam documents), there is a negatively skewed distribution with values accumulating just below 0.  The range of the values is from -28415.476872 to -55.316830.  The average value is -2838.403308.  The median value is -975.516131.\n",
    "\n",
    "As the score gets more and more negative (further from 0) it means that there are more words in that document in general or more words in that document that are contributing to the classification of the document.  For the spam class there are a few more outlier words that have large negative values than the ham class.  The average score for spam is lower than for ham.  This could mean that in general either spam documents are longer than ham documents or more words are used to classify documents as spam than as ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -9711.807285, -6330.956431, -5683.511342, -5027.774416, -4183.541952, -3656.550806, -3628.592339, -3207.512239, -3033.157012, -2906.357969, -2754.069037, -2750.998601, -2740.222547, -2694.596537, -1908.42553, -1783.952568, -1753.450314, -1652.851395, -1520.983979, -1444.307516, -1379.201515, -1364.741873, -1328.429372, -1266.125083, -1244.284239, -1180.553382, -1176.968494, -1102.79263, -1086.540133, -1085.933136, -988.159991, -983.248804, -910.184001, -894.568932, -829.093564, -777.59393, -740.821221, -689.5157, -681.002141, -678.138612, -662.30242, -589.805873, -492.008252, -489.604258, -445.866953, -438.336438, -425.885545, -412.100803, -380.540486, -336.646947, -301.633045, -201.084174, -141.73486, -78.266071, -44.714622, -29.48318]\n",
      "\n",
      "\n",
      "Range of ham values: -9711.807285 to -29.483180\n",
      "Average Ham: -1682.705902, Median Ham: -1094.666381\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBxTiGeHegtT6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe962b7d810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -30000, -28415.476872, -16821.891197, -15962.696793, -7662.008571, -5938.923501, -4433.339528, -3610.868416, -3488.565338, -3456.055866, -3327.999987, -2645.34503, -2372.297577, -1516.883469, -1496.171699, -1478.182135, -1296.9463, -1293.093393, -1274.550319, -1272.033171, -1149.101126, -1063.366585, -1018.973163, -932.059099, -932.059099, -921.727375, -894.550862, -894.550862, -851.89197, -843.304476, -818.62467, -800.858948, -799.577775, -783.235519, -778.902558, -676.86741, -659.5266, -606.53935, -525.924462, -387.054605, -211.571639, -189.339428, -174.019524, -157.472449, -55.31683]\n",
      "\n",
      "\n",
      "Range of spam values: -28415.476872 to -55.316830\n",
      "Average Spam: -2838.403308, Median Spam: -975.516131\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns0w5AZiZZcoJwMwsU04AZmaZ+v9bBeW8DzWzaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe94a461c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEZCAYAAABmTgnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGRaSpE6GhSSpk2EhSer0/wFEjl7oo04U2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9472da250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEZCAYAAABmTgnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe94c124950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# PART 2.3.3. Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create lists to store the ham and spam scores and probabilities.  The scores are the sums of the\n",
    "# log probabilities\n",
    "doc_scores_ham = []\n",
    "doc_scores_spam = []\n",
    "doc_probs_ham = []\n",
    "doc_probs_spam = []\n",
    "# Read the data from the doc_scores.txt file that was saved after the HW2.3 predictions were made using\n",
    "# Hadoop streaming.\n",
    "with open(\"doc_scores.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        items = line.split('\\t')\n",
    "        if len(items) == 4:\n",
    "            ID = items[0]\n",
    "            TRUTH = int(items[1])\n",
    "            # Extract the score data and store it in the doc_scores_ham and doc_scores_spam lists\n",
    "            ham_score = float(items[2])\n",
    "            spam_score = float(items[3])\n",
    "            if ham_score <= -1000000000:\n",
    "                ham_score = -30000\n",
    "            if spam_score <= -1000000000:\n",
    "                spam_score = -30000\n",
    "            doc_scores_ham.append(ham_score)\n",
    "            doc_scores_spam.append(spam_score)\n",
    "            # Extract the score data and store it in the doc_scores_ham and doc_scores_spam lists\n",
    "            ham_prob = math.exp(float(items[2]))\n",
    "            spam_prob = math.exp(float(items[3]))\n",
    "            doc_probs_ham.append(ham_prob)\n",
    "            doc_probs_spam.append(spam_prob)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plot the Ham Scores Histogram\n",
    "print(sorted(doc_scores_ham))\n",
    "print('\\n')\n",
    "print('Range of ham values: %f to %f' % (-9711.807285, max(doc_scores_ham)))\n",
    "l = [-9711.807285, -6330.956431, -5683.511342, -5027.774416, -4183.541952, -3656.550806, -3628.592339, -3207.512239, \n",
    "              -3033.157012, -2906.357969, -2754.069037, -2750.998601, -2740.222547, -2694.596537, -1908.42553, -1783.952568,\n",
    "              -1753.450314, -1652.851395, -1520.983979, -1444.307516, -1379.201515, -1364.741873, -1328.429372, -1266.125083, \n",
    "              -1244.284239, -1180.553382, -1176.968494, -1102.79263, -1086.540133, -1085.933136, -988.159991, -983.248804, \n",
    "              -910.184001, -894.568932, -829.093564, -777.59393, -740.821221, -689.5157, -681.002141, -678.138612, -662.30242, \n",
    "              -589.805873, -492.008252, -489.604258, -445.866953, -438.336438, -425.885545, -412.100803, -380.540486, -336.646947, \n",
    "              -301.633045, -201.084174, -141.73486, -78.266071, -44.714622, -29.48318]\n",
    "print('Average Ham: %f, Median Ham: %f' % (np.mean(l), np.median(l)))\n",
    "binwidth = 500\n",
    "plt.hist(doc_scores_ham, bins=np.arange(min(doc_scores_ham), max(doc_scores_ham) + binwidth, binwidth))\n",
    "plt.title(\"Ham Scores Histogram\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the Spam Scores Histogram\n",
    "print(sorted(doc_scores_spam))\n",
    "print('\\n')\n",
    "print('Range of spam values: %f to %f' % (-28415.476872, max(doc_scores_spam)))\n",
    "l = [-28415.476872, -16821.891197, -15962.696793, -7662.008571, -5938.923501, -4433.339528, -3610.868416, -3488.565338, \n",
    "     -3456.055866, -3327.999987, -2645.34503, -2372.297577, -1516.883469, -1496.171699, -1478.182135, -1296.9463, \n",
    "     -1293.093393, -1274.550319, -1272.033171, -1149.101126, -1063.366585, -1018.973163, -932.059099, -932.059099, -921.727375,\n",
    "     -894.550862, -894.550862, -851.89197, -843.304476, -818.62467, -800.858948, -799.577775, -783.235519, -778.902558, -676.86741, \n",
    "     -659.5266, -606.53935, -525.924462, -387.054605, -211.571639, -189.339428, -174.019524, -157.472449, -55.31683]\n",
    "print('Average Spam: %f, Median Spam: %f' % (np.mean(l), np.median(l)))\n",
    "binwidth = 500\n",
    "plt.hist(doc_scores_spam, bins=np.arange(min(doc_scores_spam), max(doc_scores_spam) + binwidth, binwidth))\n",
    "plt.title(\"Spam Scores Histogram\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the Ham Probabilities Histogram\n",
    "plt.hist(doc_probs_ham)\n",
    "plt.title(\"Ham Probabilities Histogram\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the Spam Probabilities Histogram\n",
    "plt.hist(doc_probs_spam)\n",
    "plt.title(\"Spam Probabilities Histogram\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/21 14:24:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   3 hadoop supergroup     203981 2016-05-21 14:24 /user/hadoop/enronemail_1h.txt\n",
      "drwxr-xr-x   - hadoop supergroup          0 2016-05-21 14:06 /user/hadoop/outputHW2-1\n",
      "-rw-r--r--   3 hadoop supergroup     123850 2016-05-20 22:09 /user/hadoop/random_integers.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the input file into HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -copyFromLocal enronemail_1h.txt /user/hadoop\n",
    "# Check to make sure the file is there\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 1 code for HW2.3\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "# Variable vocab:  stores the unique terms used in this file\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Update 'count_docs' by 1 for the class of this document\n",
    "    count_docs[TRUTH] += 1\n",
    "    # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "    count_words[TRUTH] += len(words_all)\n",
    "    # Create a variable to store the unique words from this line.\n",
    "    vocab_line = []\n",
    "    # Loop through each of the terms in the words of this line\n",
    "    for term in words_all:\n",
    "        # Update the vocab for this file\n",
    "        if term not in vocab:\n",
    "            vocab.append(term)\n",
    "        # Update the vocab for this line\n",
    "        if term not in vocab_line:\n",
    "            vocab_line.append(term)\n",
    "        # Initialize a list to store the count of this term\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [0,0]\n",
    "    # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "    # of the term' for reducer.py\n",
    "    for term in vocab_line:\n",
    "        # Store the number of occurances of the vocab word in the document\n",
    "        count_term = words_all.count(term)\n",
    "        # Update 'count_vocab_terms' for the class of this document\n",
    "        count_vocab_terms[term][TRUTH] += count_term\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "# Print the document count from this part of the file for Ham and Spam\n",
    "print('count,docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "# Print the word count from this part of the file for Ham and Spam\n",
    "print('count,words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "# Print each term in the vocab and then its count for Ham and Spam\n",
    "for term in vocab:\n",
    "    print('%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 1 code for HW2.3\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    elements = line.split(\"\\t\")\n",
    "    # The 1st value in the line is the ID of the document\n",
    "    label = elements[0]\n",
    "    # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "    if label == 'count,docs':\n",
    "        count_docs[0] += int(elements[1])\n",
    "        count_docs[1] += int(elements[2])\n",
    "    # Update the 'count_words' variable with the numbers from the file for each class\n",
    "    elif label == 'count,words':\n",
    "        count_words[0] += int(elements[1])\n",
    "        count_words[1] += int(elements[2])\n",
    "    # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "    else:\n",
    "        term = label\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [int(elements[1]), int(elements[2])]\n",
    "        else:\n",
    "            count_vocab_terms[term][0] += int(elements[1])\n",
    "            count_vocab_terms[term][1] += int(elements[2])\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Use smoothing by adding 1.0 to the numerator and the denominator in the condprob equation.\n",
    "# Create a variable to control if smoothing is used.\n",
    "SMOOTHING = False\n",
    "# Print the prior probabilities for each class\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "print('prior,prob\\t%f\\t%f' % (prior[0], prior[1]))\n",
    "\n",
    "# Print the conditional probabilities for each term for Ham and Spam\n",
    "for term in count_vocab_terms:\n",
    "    for cls in classes:\n",
    "        # If using smoothing, then add 1.0 to the numerator and the denominator in the condprob equation.\n",
    "        if SMOOTHING:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "        # If NOT using smoothing, then leave the numerator and the denominator alone.\n",
    "        else:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]) / (float(count_words[cls]))\n",
    "    print('%s\\t%f\\t%f' % (term, condprob[term][0], condprob[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:37:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:37:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-3a/_SUCCESS\n",
      "16/06/04 13:37:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-3a/part-00000\n",
      "16/06/04 13:37:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:37:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar8250329680862140496/] [] /tmp/streamjob745885037887334678.jar tmpDir=null\n",
      "16/06/04 13:37:24 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:37:24 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:37:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:37:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:37:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0276\n",
      "16/06/04 13:37:26 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0276\n",
      "16/06/04 13:37:26 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0276/\n",
      "16/06/04 13:37:26 INFO mapreduce.Job: Running job: job_1463787494457_0276\n",
      "16/06/04 13:37:31 INFO mapreduce.Job: Job job_1463787494457_0276 running in uber mode : false\n",
      "16/06/04 13:37:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:37:36 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/04 13:37:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:37:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:37:41 INFO mapreduce.Job: Job job_1463787494457_0276 completed successfully\n",
      "16/06/04 13:37:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=95652\n",
      "\t\tFILE: Number of bytes written=547118\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=140783\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7807\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2076\n",
      "\t\tTotal time spent by all map tasks (ms)=7807\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2076\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7807\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2076\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7994368\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2125824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=7065\n",
      "\t\tMap output bytes=81516\n",
      "\t\tMap output materialized bytes=95658\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5492\n",
      "\t\tReduce shuffle bytes=95658\n",
      "\t\tReduce input records=7065\n",
      "\t\tReduce output records=5491\n",
      "\t\tSpilled Records=14130\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=229\n",
      "\t\tCPU time spent (ms)=1970\n",
      "\t\tPhysical memory (bytes) snapshot=652791808\n",
      "\t\tVirtual memory (bytes) snapshot=6294007808\n",
      "\t\tTotal committed heap usage (bytes)=505413632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=140783\n",
      "16/06/04 13:37:41 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-3a\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-3a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-3a\n",
    "# Run a hadoop streaming job to create the model\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 13:40:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-25 11:35 /user/hadoop/outputHW2-3a/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup     140783 2016-05-25 11:35 /user/hadoop/outputHW2-3a/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Reivew the output files obtained from the above job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-3a\n",
    "# Look at the contents of the output of the reducer in the part-00000 file in HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-3a/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/21 23:29:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/21 23:29:10 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 1st reducer to the local file system\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-3a/part-00000 model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 2 code for HW2.3\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Initialize the 'condprob' variable to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "\n",
    "# Load the model from the model.txt file.\n",
    "# 1.  prior:  stores the prior probabilites for Ham (0) and Spam (0)\n",
    "# 2.  condprob:  stores the conditional probabilities for each term for Ham (0) and Spam (0)\n",
    "with open(\"model.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'prior,prob':\n",
    "            prior = [float(elements[1]), float(elements[2])]\n",
    "        else:\n",
    "            term = elements[0]\n",
    "            condprob[term] = [float(elements[1]), float(elements[2])]\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Read in the lines from the input file\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Make a prediction for each document based on the model\n",
    "    score = [0, 0]\n",
    "    for cls in classes:\n",
    "        # Initialize the score with the prior probability for the class\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term in words_all:\n",
    "            # Add the conditional probability to the scoare for each term.\n",
    "            # If any of the conditional probabilities for the document are 0, then set the score\n",
    "            # to -100000000 because, in actuality the probabilities are being multiplied, so if one is 0\n",
    "            # then the whole score will be 0.  If this happens, set the score to a very low number so that\n",
    "            # this class will have a very low score and won't be selected and break out of the loop\n",
    "            if condprob[term][cls] == 0.0:\n",
    "                score[cls] = -1000000000\n",
    "                count_docs_zero_prob[cls] += 1\n",
    "            else:\n",
    "                score[cls] += math.log(condprob[term][cls])\n",
    "    # Set the prediction for the document to the class that has the highest score\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    # Print the document ID, the real classification, the predicted classification and the score\n",
    "    # for each class for the reducer\n",
    "    print(\"%s\\t%d\\t%d\\t%f\\t%f\" % (ID, TRUTH, prediction, score[0], score[1]))\n",
    "\n",
    "# Print the number of times a zero probability for reporting purposes.\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 2 code for HW2.3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Initialize a dictionary to store the IDs and the scores from the predictions\n",
    "doc_probs = {}\n",
    "\n",
    "# Input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the zero probability counts\n",
    "    if items[0] == 'count,zeroprob':\n",
    "        count_docs_zero_prob[0] += int(items[1])\n",
    "        count_docs_zero_prob[1] += int(items[2])\n",
    "    # Update the incorrect and total counts based on the accuracy of the prediction\n",
    "    # Store the scores for each class in the doc_probs dictionary for reporting purposes\n",
    "    else:\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        prediction = int(items[2])\n",
    "        if TRUTH != prediction:\n",
    "            count_incorrect += 1\n",
    "        count_total += 1\n",
    "        doc_probs[ID] = [TRUTH, float(items[3]), float(items[4])]\n",
    "\n",
    "# Print the Training Error, zero probability counts and scores for reporting purposes\n",
    "print('Training Error:\\t%f' % (count_incorrect / float(count_total)))\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))\n",
    "for ID, probs in doc_probs.iteritems():\n",
    "    print('%s\\t%d\\t%f\\t%f' % (ID, probs[0], probs[1], probs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:38:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:38:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-3b/_SUCCESS\n",
      "16/06/04 13:38:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-3b/part-00000\n",
      "16/06/04 13:38:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:38:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5581183124064829843/] [] /tmp/streamjob2808169144689199887.jar tmpDir=null\n",
      "16/06/04 13:38:33 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:38:33 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:38:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:38:34 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:38:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0277\n",
      "16/06/04 13:38:34 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0277\n",
      "16/06/04 13:38:34 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0277/\n",
      "16/06/04 13:38:34 INFO mapreduce.Job: Running job: job_1463787494457_0277\n",
      "16/06/04 13:38:39 INFO mapreduce.Job: Job job_1463787494457_0277 running in uber mode : false\n",
      "16/06/04 13:38:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:38:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:38:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:38:49 INFO mapreduce.Job: Job job_1463787494457_0277 completed successfully\n",
      "16/06/04 13:38:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6083\n",
      "\t\tFILE: Number of bytes written=368907\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=5673\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6653\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2258\n",
      "\t\tTotal time spent by all map tasks (ms)=6653\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2258\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6653\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2258\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6812672\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2312192\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=5873\n",
      "\t\tMap output materialized bytes=6089\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=6089\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=201\n",
      "\t\tCPU time spent (ms)=1290\n",
      "\t\tPhysical memory (bytes) snapshot=646279168\n",
      "\t\tVirtual memory (bytes) snapshot=6291476480\n",
      "\t\tTotal committed heap usage (bytes)=505413632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5673\n",
      "16/06/04 13:38:49 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-3b\n"
     ]
    }
   ],
   "source": [
    "# Remove the outputs from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-3b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-3b\n",
    "# Call a hadoop streaming job to classify the documents.  Pass in the model.txt file so it is available\n",
    "# to all of the mappers\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py,model.txt -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:38:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-04 13:38 /user/hadoop/outputHW2-3b/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup       5673 2016-06-04 13:38 /user/hadoop/outputHW2-3b/part-00000\n",
      "16/06/04 13:38:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training Error:\t0.000000\n",
      "count,zeroprob\t5617\t4960\n",
      "0010.2003-12-18.GP\t1\t-1000000008.866751\t-55.316830\n",
      "0010.2001-06-28.SA_and_HP\t1\t-1000000000.000000\t-3456.055866\n",
      "0001.2000-01-17.beck\t0\t-3628.592339\t-1000000000.000000\n",
      "0018.1999-12-14.kaminski\t0\t-988.159991\t-1000000000.000000\n",
      "0005.1999-12-12.kaminski\t0\t-777.593930\t-1000000000.000000\n",
      "0011.2001-06-29.SA_and_HP\t1\t-1000000005.372825\t-15962.696793\n",
      "0008.2004-08-01.BG\t1\t-1000000000.000000\t-5938.923501\n",
      "0009.1999-12-14.farmer\t0\t-589.805873\t-1000000000.000000\n",
      "0017.2003-12-18.GP\t1\t-1000000086.657287\t-211.571639\n",
      "0011.2001-06-28.SA_and_HP\t1\t-1000000051.479294\t-1496.171699\n",
      "0015.2001-07-05.SA_and_HP\t1\t-1000000009.567015\t-932.059099\n",
      "0015.2001-02-12.kitchen\t0\t-5027.774416\t-1000000000.000000\n",
      "0009.2001-06-26.SA_and_HP\t1\t-1000000010.257534\t-1274.550319\n",
      "0017.1999-12-14.kaminski\t0\t-380.540486\t-1000000000.000000\n",
      "0012.2000-01-17.beck\t0\t-2740.222547\t-1000000000.000000\n",
      "0003.2000-01-17.beck\t0\t-1266.125083\t-1000000000.000000\n",
      "0004.2001-06-12.SA_and_HP\t1\t-1000000031.066249\t-894.550862\n",
      "0008.2001-06-12.SA_and_HP\t1\t-1000000031.066249\t-894.550862\n",
      "0007.2001-02-09.kitchen\t0\t-1652.851395\t-1000000030.657339\n",
      "0016.2004-08-01.BG\t1\t-1000000000.000000\t-659.526600\n",
      "0015.2000-06-09.lokay\t0\t-141.734860\t-1000000000.000000\n",
      "0005.1999-12-14.farmer\t0\t-1086.540133\t-1000000000.000000\n",
      "0016.1999-12-15.farmer\t0\t-740.821221\t-1000000000.000000\n",
      "0013.2004-08-01.BG\t1\t-1000000000.000000\t-1516.883469\n",
      "0005.2003-12-18.GP\t1\t-1000000000.000000\t-7662.008571\n",
      "0012.2001-02-09.kitchen\t0\t-489.604258\t-1000000000.000000\n",
      "0003.2001-02-08.kitchen\t0\t-1328.429372\t-1000000000.000000\n",
      "0009.2001-02-09.kitchen\t0\t-5683.511342\t-1000000033.751865\n",
      "0006.2001-02-08.kitchen\t0\t-9711.807285\t-1000000061.360026\n",
      "0014.2003-12-19.GP\t1\t-1000000000.000000\t-174.019524\n",
      "0010.1999-12-14.farmer\t0\t-1379.201515\t-1000000000.000000\n",
      "0010.2004-08-01.BG\t1\t-1000000023.249804\t-2372.297577\n",
      "0014.1999-12-14.kaminski\t0\t-1753.450314\t-1000000000.000000\n",
      "0006.1999-12-13.kaminski\t0\t-492.008252\t-1000000000.000000\n",
      "0011.1999-12-14.farmer\t0\t-1908.425530\t-1000000000.000000\n",
      "0013.1999-12-14.kaminski\t0\t-1244.284239\t-1000000000.000000\n",
      "0001.2001-02-07.kitchen\t0\t-336.646947\t-1000000000.000000\n",
      "0008.2001-02-09.kitchen\t0\t-4183.541952\t-1000000000.000000\n",
      "0007.2003-12-18.GP\t1\t-1000000006.343578\t-1272.033171\n",
      "0017.2004-08-02.BG\t1\t-1000000000.000000\t-2645.345030\n",
      "0014.2004-08-01.BG\t1\t-1000000000.000000\t-818.624670\n",
      "0006.2003-12-18.GP\t1\t-1000000000.000000\t-1149.101126\n",
      "0016.2001-07-05.SA_and_HP\t1\t-1000000009.567015\t-932.059099\n",
      "0008.2003-12-18.GP\t1\t-1000000000.000000\t-1018.973163\n",
      "0014.2001-07-04.SA_and_HP\t1\t-1000000000.000000\t-3610.868416\n",
      "0001.2001-04-02.williams\t0\t-1364.741873\t-1000000063.546628\n",
      "0012.2000-06-08.lokay\t0\t-910.184001\t-1000000016.569504\n",
      "0014.1999-12-15.farmer\t0\t-1176.968494\t-1000000000.000000\n",
      "0009.2000-06-07.lokay\t0\t-2694.596537\t-1000000000.000000\n",
      "0001.1999-12-10.farmer\t0\t-44.714622\t-1000000016.594472\n",
      "0008.2001-06-25.SA_and_HP\t1\t-1000000000.000000\t-4433.339528\n",
      "0017.2001-04-03.williams\t0\t-438.336438\t-1000000064.847000\n",
      "0014.2001-02-12.kitchen\t0\t-1444.307516\t-1000000012.063359\n",
      "0016.2001-07-06.SA_and_HP\t1\t-1000000005.372825\t-16821.891197\n",
      "0015.1999-12-15.farmer\t0\t-829.093564\t-1000000000.000000\n",
      "0009.1999-12-13.kaminski\t0\t-6330.956431\t-1000000012.917354\n",
      "0001.2000-06-06.lokay\t0\t-3656.550806\t-1000000004.808879\n",
      "0011.2004-08-01.BG\t1\t-1000000000.000000\t-676.867410\n",
      "0004.2004-08-01.BG\t1\t-1000000022.820419\t-800.858948\n",
      "0018.2003-12-18.GP\t1\t-1000000000.000000\t-3488.565338\n",
      "0002.1999-12-13.farmer\t0\t-2906.357969\t-1000000015.246296\n",
      "0016.2003-12-19.GP\t1\t-1000000000.000000\t-799.577775\n",
      "0004.1999-12-14.farmer\t0\t-1102.792630\t-1000000014.224154\n",
      "0015.2003-12-19.GP\t1\t-1000000000.000000\t-1296.946300\n",
      "0006.2004-08-01.BG\t1\t-1000000000.000000\t-1063.366585\n",
      "0009.2003-12-18.GP\t1\t-1000000000.000000\t-778.902558\n",
      "0007.1999-12-14.farmer\t0\t-689.515700\t-1000000000.000000\n",
      "0005.2000-06-06.lokay\t0\t-425.885545\t-1000000000.000000\n",
      "0010.1999-12-14.kaminski\t0\t-201.084174\t-1000000000.000000\n",
      "0007.2000-01-17.beck\t0\t-2754.069037\t-1000000000.000000\n",
      "0003.1999-12-14.farmer\t0\t-78.266071\t-1000000000.000000\n",
      "0003.2004-08-01.BG\t1\t-1000000000.000000\t-783.235519\n",
      "0017.2004-08-01.BG\t1\t-1000000000.000000\t-921.727375\n",
      "0013.2001-06-30.SA_and_HP\t1\t-1000000005.372825\t-28415.476872\n",
      "0003.1999-12-10.kaminski\t0\t-412.100803\t-1000000000.000000\n",
      "0012.1999-12-14.farmer\t0\t-3207.512239\t-1000000007.111096\n",
      "0004.1999-12-10.kaminski\t0\t-1085.933136\t-1000000000.000000\n",
      "0018.2001-07-13.SA_and_HP\t1\t-1000000000.000000\t-3327.999987\n",
      "0002.2001-02-07.kitchen\t0\t-445.866953\t-1000000000.000000\n",
      "0007.2004-08-01.BG\t1\t-1000000007.365040\t-1478.182135\n",
      "0012.1999-12-14.kaminski\t0\t-983.248804\t-1000000000.000000\n",
      "0005.2001-06-23.SA_and_HP\t1\t-1000000000.000000\t-189.339428\n",
      "0007.1999-12-13.kaminski\t0\t-1520.983979\t-1000000009.826527\n",
      "0017.2000-01-17.beck\t0\t-2750.998601\t-1000000000.000000\n",
      "0006.2001-06-25.SA_and_HP\t1\t-1000000000.000000\t-387.054605\n",
      "0006.2001-04-03.williams\t0\t-301.633045\t-1000000016.831326\n",
      "0005.2001-02-08.kitchen\t0\t-894.568932\t-1000000000.000000\n",
      "0002.2003-12-18.GP\t1\t-1000000047.216482\t-1293.093393\n",
      "0003.2003-12-18.GP\t1\t-1000000000.000000\t-851.891970\n",
      "0013.2001-04-03.williams\t0\t-662.302420\t-1000000000.000000\n",
      "0004.2001-04-02.williams\t0\t-681.002141\t-1000000000.000000\n",
      "0010.2001-02-09.kitchen\t0\t-3033.157012\t-1000000033.751865\n",
      "0001.1999-12-10.kaminski\t0\t-29.483180\t-1000000012.463682\n",
      "0013.1999-12-14.farmer\t0\t-1783.952568\t-1000000007.111096\n",
      "0015.1999-12-14.kaminski\t0\t-678.138612\t-1000000057.165577\n",
      "0012.2003-12-19.GP\t1\t-1000000000.000000\t-157.472449\n",
      "0016.2001-02-12.kitchen\t0\t-1180.553382\t-1000000000.000000\n",
      "0002.2004-08-01.BG\t1\t-1000000007.951879\t-843.304476\n",
      "0002.2001-05-25.SA_and_HP\t1\t-1000000009.567015\t-606.539350\n",
      "0011.2003-12-18.GP\t1\t-1000000000.000000\t-525.924462\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output produced from the mapreduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-3b\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-3b/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 23:36:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/24 23:36:51 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 2nd reducer to the local file system for reporting purposes.\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-3b/part-00000 doc_scores.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.4** Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "ANSWER:  \n",
    "Training Error for HW2.3: 0% (0/100 documents misclassified)  \n",
    "Training Error for HW2.4: 0% (0/100 documents misclassified)\n",
    "\n",
    "Using Laplace plus-one smoothing causes no difference in the training error rate.  Smoothing does not change any results in this case because there are no term-class combinations that did not occur in the training data.  Smoothing only makes a difference when there is a new term-class combination that would have been 0 in the training data had we not added 1 to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 1 code for HW2.4\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "# Variable vocab:  stores the unique terms used in this file\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Update 'count_docs' by 1 for the class of this document\n",
    "    count_docs[TRUTH] += 1\n",
    "    # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "    count_words[TRUTH] += len(words_all)\n",
    "    # Create a variable to store the unique words from this line.\n",
    "    vocab_line = []\n",
    "    # Loop through each of the terms in the words of this line\n",
    "    for term in words_all:\n",
    "        # Update the vocab for this file\n",
    "        if term not in vocab:\n",
    "            vocab.append(term)\n",
    "        # Update the vocab for this line\n",
    "        if term not in vocab_line:\n",
    "            vocab_line.append(term)\n",
    "        # Initialize a list to store the count of this term\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [0,0]\n",
    "    # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "    # of the term' for reducer.py\n",
    "    for term in vocab_line:\n",
    "        # Store the number of occurances of the vocab word in the document\n",
    "        count_term = words_all.count(term)\n",
    "        # Update 'count_vocab_terms' for the class of this document\n",
    "        count_vocab_terms[term][TRUTH] += count_term\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "# Print the document count from this part of the file for Ham and Spam\n",
    "print('count,docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "# Print the word count from this part of the file for Ham and Spam\n",
    "print('count,words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "# Print each term in the vocab and then its count for Ham and Spam\n",
    "for term in vocab:\n",
    "    print('%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 1 code for HW2.4\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    elements = line.split(\"\\t\")\n",
    "    # The 1st value in the line is the ID of the document\n",
    "    label = elements[0]\n",
    "    # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "    if label == 'count,docs':\n",
    "        count_docs[0] += int(elements[1])\n",
    "        count_docs[1] += int(elements[2])\n",
    "    # Update the 'count_words' variable with the numbers from the file for each class\n",
    "    elif label == 'count,words':\n",
    "        count_words[0] += int(elements[1])\n",
    "        count_words[1] += int(elements[2])\n",
    "    # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "    else:\n",
    "        term = label\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [int(elements[1]), int(elements[2])]\n",
    "        else:\n",
    "            count_vocab_terms[term][0] += int(elements[1])\n",
    "            count_vocab_terms[term][1] += int(elements[2])\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Use smoothing by adding 1.0 to the numerator and the denominator in the condprob equation.\n",
    "# Create a variable to control if smoothing is used.\n",
    "# For 2.4 set SMOOTHING to True\n",
    "SMOOTHING = True\n",
    "# Print the prior probabilities for each class\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "print('prior,prob\\t%f\\t%f' % (prior[0], prior[1]))\n",
    "\n",
    "# Print the conditional probabilities for each term for Ham and Spam\n",
    "for term in count_vocab_terms:\n",
    "    for cls in classes:\n",
    "        # If using smoothing, then add 1.0 to the numerator and the denominator in the condprob equation.\n",
    "        if SMOOTHING:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "        # If NOT using smoothing, then leave the numerator and the denominator alone.\n",
    "        else:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]) / (float(count_words[cls]))\n",
    "    print('%s\\t%f\\t%f' % (term, condprob[term][0], condprob[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:39:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:39:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-4a/_SUCCESS\n",
      "16/06/04 13:39:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-4a/part-00000\n",
      "16/06/04 13:39:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:39:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar8044779600659825015/] [] /tmp/streamjob1113455125883362075.jar tmpDir=null\n",
      "16/06/04 13:39:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:39:28 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:39:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:39:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:39:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0278\n",
      "16/06/04 13:39:29 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0278\n",
      "16/06/04 13:39:29 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0278/\n",
      "16/06/04 13:39:29 INFO mapreduce.Job: Running job: job_1463787494457_0278\n",
      "16/06/04 13:39:34 INFO mapreduce.Job: Job job_1463787494457_0278 running in uber mode : false\n",
      "16/06/04 13:39:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:39:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:39:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:39:44 INFO mapreduce.Job: Job job_1463787494457_0278 completed successfully\n",
      "16/06/04 13:39:44 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=95652\n",
      "\t\tFILE: Number of bytes written=547121\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=140783\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7293\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2393\n",
      "\t\tTotal time spent by all map tasks (ms)=7293\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2393\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7293\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2393\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7468032\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2450432\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=7065\n",
      "\t\tMap output bytes=81516\n",
      "\t\tMap output materialized bytes=95658\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5492\n",
      "\t\tReduce shuffle bytes=95658\n",
      "\t\tReduce input records=7065\n",
      "\t\tReduce output records=5491\n",
      "\t\tSpilled Records=14130\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=225\n",
      "\t\tCPU time spent (ms)=1690\n",
      "\t\tPhysical memory (bytes) snapshot=649764864\n",
      "\t\tVirtual memory (bytes) snapshot=6291644416\n",
      "\t\tTotal committed heap usage (bytes)=505937920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=140783\n",
      "16/06/04 13:39:44 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-4a\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-4a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-4a\n",
    "# Run a hadoop streaming job to create the model\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 13:39:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-24 16:40 /user/hadoop/outputHW2-4a/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup     140783 2016-05-24 16:40 /user/hadoop/outputHW2-4a/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Reivew the output files obtained from the above job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-4a\n",
    "# Look at the contents of the output of the reducer in the part-00000 file in HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-4a/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 16:40:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/24 16:40:43 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 1st reducer to the local file system\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-4a/part-00000 model24.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 2 code for HW2.4\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Initialize the 'condprob' variable to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "\n",
    "# Load the model from the model.txt file.\n",
    "# 1.  prior:  stores the prior probabilites for Ham (0) and Spam (0)\n",
    "# 2.  condprob:  stores the conditional probabilities for each term for Ham (0) and Spam (0)\n",
    "with open(\"model24.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'prior,prob':\n",
    "            prior = [float(elements[1]), float(elements[2])]\n",
    "        else:\n",
    "            term = elements[0]\n",
    "            condprob[term] = [float(elements[1]), float(elements[2])]\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Read in the lines from the input file\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Make a prediction for each document based on the model\n",
    "    score = [0, 0]\n",
    "    for cls in classes:\n",
    "        # Initialize the score with the prior probability for the class\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term in words_all:\n",
    "            # Add the conditional probability to the scoare for each term.\n",
    "            # If any of the conditional probabilities for the document are 0, then set the score\n",
    "            # to -100000000 because, in actuality the probabilities are being multiplied, so if one is 0\n",
    "            # then the whole score will be 0.  If this happens, set the score to a very low number so that\n",
    "            # this class will have a very low score and won't be selected and break out of the loop\n",
    "            if condprob[term][cls] == 0.0:\n",
    "                score[cls] = -1000000000\n",
    "                count_docs_zero_prob[cls] += 1\n",
    "            else:\n",
    "                score[cls] += math.log(condprob[term][cls])\n",
    "    # Set the prediction for the document to the class that has the highest score\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    # Print the document ID, the real classification, the predicted classification and the score\n",
    "    # for each class for the reducer\n",
    "    print(\"%s\\t%d\\t%d\\t%f\\t%f\" % (ID, TRUTH, prediction, score[0], score[1]))\n",
    "\n",
    "# Print the number of times a zero probability for reporting purposes.\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 2 code for HW2.4\n",
    "\n",
    "import sys\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Initialize a dictionary to store the IDs and the scores from the predictions\n",
    "doc_probs = {}\n",
    "\n",
    "# Input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the zero probability counts\n",
    "    if items[0] == 'count,zeroprob':\n",
    "        count_docs_zero_prob[0] += int(items[1])\n",
    "        count_docs_zero_prob[1] += int(items[2])\n",
    "    # Update the incorrect and total counts based on the accuracy of the prediction\n",
    "    # Store the scores for each class in the doc_probs dictionary for reporting purposes\n",
    "    else:\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        prediction = int(items[2])\n",
    "        if TRUTH != prediction:\n",
    "            count_incorrect += 1\n",
    "        count_total += 1\n",
    "        doc_probs[ID] = [float(items[3]), float(items[4])]\n",
    "\n",
    "# Print the Training Error, zero probability counts and scores for reporting purposes\n",
    "print('Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))\n",
    "for ID, probs in doc_probs.iteritems():\n",
    "    print('%s\\t%f\\t%f' % (ID, probs[0], probs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:40:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:40:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-4b/_SUCCESS\n",
      "16/06/04 13:40:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-4b/part-00000\n",
      "16/06/04 13:40:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:40:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5178182028172822009/] [] /tmp/streamjob3595721987412404893.jar tmpDir=null\n",
      "16/06/04 13:40:29 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:40:29 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:40:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:40:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:40:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0279\n",
      "16/06/04 13:40:30 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0279\n",
      "16/06/04 13:40:30 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0279/\n",
      "16/06/04 13:40:30 INFO mapreduce.Job: Running job: job_1463787494457_0279\n",
      "16/06/04 13:40:34 INFO mapreduce.Job: Job job_1463787494457_0279 running in uber mode : false\n",
      "16/06/04 13:40:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:40:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:40:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:40:47 INFO mapreduce.Job: Job job_1463787494457_0279 completed successfully\n",
      "16/06/04 13:40:47 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5423\n",
      "\t\tFILE: Number of bytes written=367611\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=4820\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6538\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3342\n",
      "\t\tTotal time spent by all map tasks (ms)=6538\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3342\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6538\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3342\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6694912\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3422208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=5213\n",
      "\t\tMap output materialized bytes=5429\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=5429\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=184\n",
      "\t\tCPU time spent (ms)=1290\n",
      "\t\tPhysical memory (bytes) snapshot=644956160\n",
      "\t\tVirtual memory (bytes) snapshot=6290931712\n",
      "\t\tTotal committed heap usage (bytes)=507510784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4820\n",
      "16/06/04 13:40:47 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-4b\n"
     ]
    }
   ],
   "source": [
    "# Remove the outputs from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-4b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-4b\n",
    "# Call a hadoop streaming job to classify the documents.  Pass in the model.txt file so it is available\n",
    "# to all of the mappers\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py,model24.txt -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:40:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-06-04 13:40 /user/hadoop/outputHW2-4b/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup       4820 2016-06-04 13:40 /user/hadoop/outputHW2-4b/part-00000\n",
      "16/06/04 13:40:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training Error: 0.000000\t\n",
      "count,zeroprob\t0\t0\n",
      "0010.2003-12-18.GP\t-53.695743\t-52.688604\n",
      "0010.2001-06-28.SA_and_HP\t-3689.417177\t-3381.099048\n",
      "0001.2000-01-17.beck\t-3535.918583\t-4160.185188\n",
      "0018.1999-12-14.kaminski\t-965.647201\t-990.262795\n",
      "0005.1999-12-12.kaminski\t-747.471559\t-865.846157\n",
      "0011.2001-06-29.SA_and_HP\t-17518.334352\t-15660.798148\n",
      "0008.2004-08-01.BG\t-6063.024332\t-5737.480780\n",
      "0009.1999-12-14.farmer\t-571.861332\t-666.858911\n",
      "0017.2003-12-18.GP\t-218.324629\t-205.870590\n",
      "0011.2001-06-28.SA_and_HP\t-1607.855117\t-1465.857041\n",
      "0015.2001-07-05.SA_and_HP\t-993.761919\t-914.876316\n",
      "0015.2001-02-12.kitchen\t-4911.303935\t-5410.489006\n",
      "0009.2001-06-26.SA_and_HP\t-1315.095241\t-1237.128525\n",
      "0017.1999-12-14.kaminski\t-370.072433\t-385.797513\n",
      "0012.2000-01-17.beck\t-2701.328372\t-3335.685208\n",
      "0003.2000-01-17.beck\t-1243.592017\t-1497.073312\n",
      "0004.2001-06-12.SA_and_HP\t-961.409413\t-879.521308\n",
      "0008.2001-06-12.SA_and_HP\t-961.409413\t-879.521308\n",
      "0007.2001-02-09.kitchen\t-1610.565683\t-1784.855061\n",
      "0016.2004-08-01.BG\t-701.615794\t-643.329167\n",
      "0015.2000-06-09.lokay\t-138.453774\t-145.985047\n",
      "0005.1999-12-14.farmer\t-1065.387063\t-1367.457368\n",
      "0016.1999-12-15.farmer\t-724.263535\t-776.085827\n",
      "0013.2004-08-01.BG\t-1541.547028\t-1466.724678\n",
      "0005.2003-12-18.GP\t-7985.711311\t-7411.772314\n",
      "0012.2001-02-09.kitchen\t-475.108076\t-513.669098\n",
      "0003.2001-02-08.kitchen\t-1299.553170\t-1533.721196\n",
      "0009.2001-02-09.kitchen\t-5557.291724\t-6276.101227\n",
      "0006.2001-02-08.kitchen\t-9463.162446\t-10311.283692\n",
      "0014.2003-12-19.GP\t-179.472225\t-168.503038\n",
      "0010.1999-12-14.farmer\t-1331.160663\t-1786.639919\n",
      "0010.2004-08-01.BG\t-2540.228920\t-2296.091070\n",
      "0014.1999-12-14.kaminski\t-1729.741826\t-2146.862745\n",
      "0006.1999-12-13.kaminski\t-478.104117\t-506.480299\n",
      "0011.1999-12-14.farmer\t-1864.347246\t-2055.640412\n",
      "0013.1999-12-14.kaminski\t-1232.202033\t-1609.623594\n",
      "0001.2001-02-07.kitchen\t-330.215531\t-390.536082\n",
      "0008.2001-02-09.kitchen\t-4023.378377\t-4826.387119\n",
      "0007.2003-12-18.GP\t-1300.411813\t-1221.491249\n",
      "0017.2004-08-02.BG\t-2698.512250\t-2565.820380\n",
      "0014.2004-08-01.BG\t-817.625619\t-781.771891\n",
      "0006.2003-12-18.GP\t-1160.125158\t-1099.629736\n",
      "0016.2001-07-05.SA_and_HP\t-993.761919\t-914.876316\n",
      "0008.2003-12-18.GP\t-1087.228128\t-991.891593\n",
      "0014.2001-07-04.SA_and_HP\t-3844.050523\t-3531.027979\n",
      "0001.2001-04-02.williams\t-1324.731975\t-1375.649881\n",
      "0012.2000-06-08.lokay\t-882.268083\t-910.584309\n",
      "0014.1999-12-15.farmer\t-1148.752708\t-1342.404095\n",
      "0009.2000-06-07.lokay\t-2612.706655\t-2974.106118\n",
      "0001.1999-12-10.farmer\t-42.161465\t-45.773974\n",
      "0008.2001-06-25.SA_and_HP\t-4882.807069\t-4329.083277\n",
      "0017.2001-04-03.williams\t-426.987854\t-444.118067\n",
      "0014.2001-02-12.kitchen\t-1393.359881\t-1570.923035\n",
      "0016.2001-07-06.SA_and_HP\t-18721.876702\t-16548.902586\n",
      "0015.1999-12-15.farmer\t-809.285271\t-977.534057\n",
      "0009.1999-12-13.kaminski\t-6224.849893\t-8836.419140\n",
      "0001.2000-06-06.lokay\t-3570.026736\t-4022.693413\n",
      "0011.2004-08-01.BG\t-698.982829\t-653.395042\n",
      "0004.2004-08-01.BG\t-805.239892\t-767.980386\n",
      "0018.2003-12-18.GP\t-3585.955761\t-3392.113678\n",
      "0002.1999-12-13.farmer\t-2838.046962\t-3359.001755\n",
      "0016.2003-12-19.GP\t-836.975305\t-772.025491\n",
      "0004.1999-12-14.farmer\t-1085.559788\t-1450.146478\n",
      "0015.2003-12-19.GP\t-1387.937116\t-1263.718616\n",
      "0006.2004-08-01.BG\t-1088.644956\t-1025.300738\n",
      "0009.2003-12-18.GP\t-814.785935\t-752.760393\n",
      "0007.1999-12-14.farmer\t-670.897497\t-759.693336\n",
      "0005.2000-06-06.lokay\t-409.071197\t-432.388698\n",
      "0010.1999-12-14.kaminski\t-195.961836\t-222.065446\n",
      "0007.2000-01-17.beck\t-2712.837530\t-3349.948691\n",
      "0003.1999-12-14.farmer\t-76.059160\t-93.913451\n",
      "0003.2004-08-01.BG\t-801.856081\t-753.017348\n",
      "0017.2004-08-01.BG\t-907.909015\t-861.693995\n",
      "0013.2001-06-30.SA_and_HP\t-31159.644601\t-27903.510343\n",
      "0003.1999-12-10.kaminski\t-403.858124\t-464.070055\n",
      "0012.1999-12-14.farmer\t-3145.572059\t-3684.797383\n",
      "0004.1999-12-10.kaminski\t-1057.984407\t-1211.986277\n",
      "0018.2001-07-13.SA_and_HP\t-3465.455045\t-3253.328834\n",
      "0002.2001-02-07.kitchen\t-430.634558\t-447.142540\n",
      "0007.2004-08-01.BG\t-1649.297963\t-1437.688362\n",
      "0012.1999-12-14.kaminski\t-968.320680\t-1301.968907\n",
      "0005.2001-06-23.SA_and_HP\t-196.688627\t-183.614804\n",
      "0007.1999-12-13.kaminski\t-1491.477757\t-1681.245269\n",
      "0017.2000-01-17.beck\t-2710.666049\t-3347.318864\n",
      "0006.2001-06-25.SA_and_HP\t-391.213193\t-372.602068\n",
      "0006.2001-04-03.williams\t-292.913667\t-307.062659\n",
      "0005.2001-02-08.kitchen\t-862.777602\t-978.745645\n",
      "0002.2003-12-18.GP\t-1363.204777\t-1250.520165\n",
      "0003.2003-12-18.GP\t-869.179297\t-822.560504\n",
      "0013.2001-04-03.williams\t-643.990590\t-673.124978\n",
      "0004.2001-04-02.williams\t-661.416034\t-701.340966\n",
      "0010.2001-02-09.kitchen\t-2974.118733\t-3427.785554\n",
      "0001.1999-12-10.kaminski\t-28.548664\t-30.627061\n",
      "0013.1999-12-14.farmer\t-1753.714026\t-1928.200744\n",
      "0015.1999-12-14.kaminski\t-659.219931\t-741.609204\n",
      "0012.2003-12-19.GP\t-166.851470\t-152.931056\n",
      "0016.2001-02-12.kitchen\t-1148.982688\t-1335.575920\n",
      "0002.2004-08-01.BG\t-870.726916\t-825.281634\n",
      "0002.2001-05-25.SA_and_HP\t-640.407173\t-588.669618\n",
      "0011.2003-12-18.GP\t-556.703459\t-507.708419\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output produced from the mapreduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-4b\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-4b/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 16:44:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/24 16:44:24 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 2nd reducer to the local file system for reporting purposes.\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-4b/part-00000 doc_scores24.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.5.** Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset. Report the error and the change in error. HINT: ignore tokens with a frequency of less than three (3). Think of this as a preprocessing step. How many mapreduce jobs do you need to solve this homework?\n",
    "\n",
    "ANSWER:  After removing tokens with a frequency of less than 3 (i.e. terms whose (ham_count + spam_count) < 3), the model went from having a vocabulary of about 5,490 terms to about 1,862 terms.  After the terms were removed from the vocabulary the total word_counts for each class was decreased accordingly.  After removing the terms, the new training error is 2% (2/100 documents were misclassified).  The training error without removing the terms was 0% (0/100 documents were misclassified).  This is a change of 2%.  This indicates that some of the terms that were removed were important to classifying the documents correctly.\n",
    "\n",
    "This problem required 2 mapreduce jobs just like problem 2.4.  No additional mapreduce jobs are required.\tThe extra pass over the terms to remove the terms with freqency less than 3 can be done in the 1st reducer where the model is being built.  This pass loops through the current vocabulary and counts and removes any terms that have a freq less than 3 and then updates the total word counts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 1 code for HW2.5\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "# Variable vocab:  stores the unique terms used in this file\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Update 'count_docs' by 1 for the class of this document\n",
    "    count_docs[TRUTH] += 1\n",
    "    # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "    count_words[TRUTH] += len(words_all)\n",
    "    # Create a variable to store the unique words from this line.\n",
    "    vocab_line = []\n",
    "    # Loop through each of the terms in the words of this line\n",
    "    for term in words_all:\n",
    "        # Update the vocab for this file\n",
    "        if term not in vocab:\n",
    "            vocab.append(term)\n",
    "        # Update the vocab for this line\n",
    "        if term not in vocab_line:\n",
    "            vocab_line.append(term)\n",
    "        # Initialize a list to store the count of this term\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [0,0]\n",
    "    # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "    # of the term' for reducer.py\n",
    "    for term in vocab_line:\n",
    "        # Store the number of occurances of the vocab word in the document\n",
    "        count_term = words_all.count(term)\n",
    "        # Update 'count_vocab_terms' for the class of this document\n",
    "        count_vocab_terms[term][TRUTH] += count_term\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "# Print the document count from this part of the file for Ham and Spam\n",
    "print('count,docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "# Print the word count from this part of the file for Ham and Spam\n",
    "print('count,words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "# Print each term in the vocab and then its count for Ham and Spam\n",
    "for term in vocab:\n",
    "    print('%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 1 code for HW2.5\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    elements = line.split(\"\\t\")\n",
    "    # The 1st value in the line is the ID of the document\n",
    "    label = elements[0]\n",
    "    # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "    if label == 'count,docs':\n",
    "        count_docs[0] += int(elements[1])\n",
    "        count_docs[1] += int(elements[2])\n",
    "    # Update the 'count_words' variable with the numbers from the file for each class\n",
    "    elif label == 'count,words':\n",
    "        count_words[0] += int(elements[1])\n",
    "        count_words[1] += int(elements[2])\n",
    "    # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "    else:\n",
    "        term = label\n",
    "        if term not in count_vocab_terms:\n",
    "            count_vocab_terms[term] = [int(elements[1]), int(elements[2])]\n",
    "        else:\n",
    "            count_vocab_terms[term][0] += int(elements[1])\n",
    "            count_vocab_terms[term][1] += int(elements[2])\n",
    "\n",
    "# Per HW2.5 instructions, update vocabulary to ignore terms that have frequency less than 3.\n",
    "# Create a flag to update the total word counts after removing the infrequent terms.\n",
    "UPDATE_WORD_COUNTS = True\n",
    "for term in count_vocab_terms.keys():\n",
    "    # store the counts of the term for each class\n",
    "    count_0 = count_vocab_terms[term][0]\n",
    "    count_1 = count_vocab_terms[term][1]\n",
    "    # If a term has a frequency of less than 3, then remove it from the vocabulary.\n",
    "    if (count_0 + count_1) < 3:\n",
    "        if UPDATE_WORD_COUNTS:\n",
    "            count_words[0] -= count_0\n",
    "            count_words[1] -= count_1\n",
    "        # Remove the term\n",
    "        del count_vocab_terms[term]\n",
    "            \n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Use smoothing by adding 1.0 to the numerator and the denominator in the condprob equation.\n",
    "# Create a variable to control if smoothing is used.\n",
    "# For 2.4 and 2.5 set SMOOTHING to True\n",
    "SMOOTHING = True\n",
    "# Print the prior probabilities for each class\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "print('prior,prob\\t%f\\t%f' % (prior[0], prior[1]))\n",
    "\n",
    "# Print the conditional probabilities for each term for Ham and Spam\n",
    "for term in count_vocab_terms:\n",
    "    for cls in classes:\n",
    "        # If using smoothing, then add 1.0 to the numerator and the denominator in the condprob equation.\n",
    "        if SMOOTHING:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "        # If NOT using smoothing, then leave the numerator and the denominator alone.\n",
    "        else:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]) / (float(count_words[cls]))\n",
    "    print('%s\\t%f\\t%f' % (term, condprob[term][0], condprob[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:41:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:41:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-5a/_SUCCESS\n",
      "16/06/04 13:41:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-5a/part-00000\n",
      "16/06/04 13:41:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:41:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5340425079441502008/] [] /tmp/streamjob281278581976175190.jar tmpDir=null\n",
      "16/06/04 13:41:39 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:41:39 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:41:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:41:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:41:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0280\n",
      "16/06/04 13:41:40 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0280\n",
      "16/06/04 13:41:40 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0280/\n",
      "16/06/04 13:41:40 INFO mapreduce.Job: Running job: job_1463787494457_0280\n",
      "16/06/04 13:41:45 INFO mapreduce.Job: Job job_1463787494457_0280 running in uber mode : false\n",
      "16/06/04 13:41:45 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:41:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:41:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:41:55 INFO mapreduce.Job: Job job_1463787494457_0280 completed successfully\n",
      "16/06/04 13:41:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=95652\n",
      "\t\tFILE: Number of bytes written=547118\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=46549\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7446\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2403\n",
      "\t\tTotal time spent by all map tasks (ms)=7446\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2403\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7446\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2403\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7624704\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2460672\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=7065\n",
      "\t\tMap output bytes=81516\n",
      "\t\tMap output materialized bytes=95658\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5492\n",
      "\t\tReduce shuffle bytes=95658\n",
      "\t\tReduce input records=7065\n",
      "\t\tReduce output records=1863\n",
      "\t\tSpilled Records=14130\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=229\n",
      "\t\tCPU time spent (ms)=1670\n",
      "\t\tPhysical memory (bytes) snapshot=650502144\n",
      "\t\tVirtual memory (bytes) snapshot=6292455424\n",
      "\t\tTotal committed heap usage (bytes)=505937920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=46549\n",
      "16/06/04 13:41:55 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-5a\n"
     ]
    }
   ],
   "source": [
    "# Remove output from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-5a/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-5a\n",
    "# Run a hadoop streaming job to create the model\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/26 13:38:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-24 21:43 /user/hadoop/outputHW2-5a/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup      46549 2016-05-24 21:43 /user/hadoop/outputHW2-5a/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Reivew the output files obtained from the above job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-5a\n",
    "# Look at the contents of the output of the reducer in the part-00000 file in HDFS\n",
    "#!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-5a/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 21:44:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/05/24 21:44:42 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 1st reducer to the local file system\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-5a/part-00000 model25.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Mapper 2 code for HW2.5\n",
    "\n",
    "# Import print function from python 3\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Initialize the 'condprob' variable to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "\n",
    "# Load the model from the model.txt file.\n",
    "# 1.  prior:  stores the prior probabilites for Ham (0) and Spam (0)\n",
    "# 2.  condprob:  stores the conditional probabilities for each term for Ham (0) and Spam (0)\n",
    "with open(\"model25.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'prior,prob':\n",
    "            prior = [float(elements[1]), float(elements[2])]\n",
    "        else:\n",
    "            term = elements[0]\n",
    "            condprob[term] = [float(elements[1]), float(elements[2])]\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Read in the lines from the input file\n",
    "    # Split the line tabs and store the result in the 'items' list.\n",
    "    items = line.split('\\t')\n",
    "    # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "    if len(items) == 1:\n",
    "        continue\n",
    "    # The 1st value of the line is the ID of the document\n",
    "    ID = items[0]\n",
    "    # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "    TRUTH = int(items[1])\n",
    "    # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    words_all = re.findall(WORD_RE, items[2].lower())\n",
    "    # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "    # break it up into separate words using the WORD_RE regular expression\n",
    "    if len(items) == 4:\n",
    "        words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "    # Make a prediction for each document based on the model\n",
    "    score = [0, 0]\n",
    "    for cls in classes:\n",
    "        # Initialize the score with the prior probability for the class\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term in words_all:\n",
    "            # If the term is in the vocabulary, then add the conditional probability to the score.\n",
    "            if term in condprob:\n",
    "                # Add the conditional probability to the scoare for each term.\n",
    "                # If any of the conditional probabilities for the document are 0, then set the score\n",
    "                # to -100000000 because, in actuality the probabilities are being multiplied, so if one is 0\n",
    "                # then the whole score will be 0.  If this happens, set the score to a very low number so that\n",
    "                # this class will have a very low score and won't be selected and break out of the loop\n",
    "                if condprob[term][cls] == 0.0:\n",
    "                    score[cls] = -1000000000\n",
    "                    count_docs_zero_prob[cls] += 1\n",
    "                else:\n",
    "                    score[cls] += math.log(condprob[term][cls])\n",
    "    # Set the prediction for the document to the class that has the highest score\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    # Print the document ID, the real classification, the predicted classification and the score\n",
    "    # for each class for the reducer\n",
    "    print(\"%s\\t%d\\t%d\\t%f\\t%f\" % (ID, TRUTH, prediction, score[0], score[1]))\n",
    "\n",
    "# Print the number of times a zero probability for reporting purposes.\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: Reducer 2 code for HW2.5\n",
    "\n",
    "import sys\n",
    "\n",
    "# Initialize a list to store the # of times a zero probability is processed for each class\n",
    "count_docs_zero_prob = [0, 0]\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Initialize a dictionary to store the IDs and the scores from the predictions\n",
    "doc_probs = {}\n",
    "\n",
    "# Input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # Split the line by tabs\n",
    "    items = line.split('\\t')\n",
    "    # Store the zero probability counts\n",
    "    if items[0] == 'count,zeroprob':\n",
    "        count_docs_zero_prob[0] += int(items[1])\n",
    "        count_docs_zero_prob[1] += int(items[2])\n",
    "    # Update the incorrect and total counts based on the accuracy of the prediction\n",
    "    # Store the scores for each class in the doc_probs dictionary for reporting purposes\n",
    "    else:\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        prediction = int(items[2])\n",
    "        if TRUTH != prediction:\n",
    "            count_incorrect += 1\n",
    "        count_total += 1\n",
    "        doc_probs[ID] = [float(items[3]), float(items[4])]\n",
    "\n",
    "# Print the Training Error, zero probability counts and scores for reporting purposes\n",
    "print('Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('count,zeroprob\\t%d\\t%d' % (count_docs_zero_prob[0], count_docs_zero_prob[1]))\n",
    "for ID, probs in doc_probs.iteritems():\n",
    "    print('%s\\t%f\\t%f' % (ID, probs[0], probs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/04 13:42:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:42:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-5b/_SUCCESS\n",
      "16/06/04 13:42:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/outputHW2-5b/part-00000\n",
      "16/06/04 13:42:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/04 13:42:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/tmp/hadoop-unjar5382785472976614956/] [] /tmp/streamjob5963435146081484085.jar tmpDir=null\n",
      "16/06/04 13:42:30 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:42:30 INFO client.RMProxy: Connecting to ResourceManager at master/50.97.205.254:8032\n",
      "16/06/04 13:42:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/04 13:42:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/04 13:42:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1463787494457_0281\n",
      "16/06/04 13:42:31 INFO impl.YarnClientImpl: Submitted application application_1463787494457_0281\n",
      "16/06/04 13:42:31 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1463787494457_0281/\n",
      "16/06/04 13:42:31 INFO mapreduce.Job: Running job: job_1463787494457_0281\n",
      "16/06/04 13:42:35 INFO mapreduce.Job: Job job_1463787494457_0281 running in uber mode : false\n",
      "16/06/04 13:42:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/04 13:42:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/04 13:42:46 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/04 13:42:47 INFO mapreduce.Job: Job job_1463787494457_0281 completed successfully\n",
      "16/06/04 13:42:47 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5399\n",
      "\t\tFILE: Number of bytes written=367560\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216869\n",
      "\t\tHDFS: Number of bytes written=4796\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6439\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3309\n",
      "\t\tTotal time spent by all map tasks (ms)=6439\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3309\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6439\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3309\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6593536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3388416\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=5189\n",
      "\t\tMap output materialized bytes=5405\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=101\n",
      "\t\tReduce shuffle bytes=5405\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=183\n",
      "\t\tCPU time spent (ms)=1310\n",
      "\t\tPhysical memory (bytes) snapshot=647720960\n",
      "\t\tVirtual memory (bytes) snapshot=6289633280\n",
      "\t\tTotal committed heap usage (bytes)=507510784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216679\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4796\n",
      "16/06/04 13:42:47 INFO streaming.StreamJob: Output directory: /user/hadoop/outputHW2-5b\n"
     ]
    }
   ],
   "source": [
    "# Remove the outputs from previous mapreduce jobs\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rm /user/hadoop/outputHW2-5b/*\n",
    "!/usr/local/hadoop/bin/hdfs dfs -rmdir /user/hadoop/outputHW2-5b\n",
    "# Call a hadoop streaming job to classify the documents.  Pass in the model.txt file so it is available\n",
    "# to all of the mappers\n",
    "!/usr/local/hadoop/bin/hadoop jar $hadoopStreamingJar \\\n",
    "-files mapper.py,reducer.py,model25.txt -mapper mapper.py -reducer reducer.py \\\n",
    "-input /user/hadoop/enronemail_1h.txt -output /user/hadoop/outputHW2-5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 21:45:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 hadoop supergroup          0 2016-05-24 21:45 /user/hadoop/outputHW2-5b/_SUCCESS\n",
      "-rw-r--r--   3 hadoop supergroup       4796 2016-05-24 21:45 /user/hadoop/outputHW2-5b/part-00000\n",
      "16/05/24 21:45:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training Error: 0.020000\t\n",
      "count,zeroprob\t0\t0\n",
      "0010.2003-12-18.GP\t-15.169600\t-16.658759\n",
      "0010.2001-06-28.SA_and_HP\t-2849.832563\t-2580.608804\n",
      "0001.2000-01-17.beck\t-2769.597025\t-3286.915589\n",
      "0018.1999-12-14.kaminski\t-806.859479\t-812.082096\n",
      "0005.1999-12-12.kaminski\t-439.889532\t-521.289855\n",
      "0011.2001-06-29.SA_and_HP\t-14970.017660\t-13156.279116\n",
      "0008.2004-08-01.BG\t-3703.990612\t-3474.178015\n",
      "0009.1999-12-14.farmer\t-383.937997\t-450.869066\n",
      "0017.2003-12-18.GP\t-159.088673\t-147.322627\n",
      "0011.2001-06-28.SA_and_HP\t-1229.918209\t-1112.181681\n",
      "0015.2001-07-05.SA_and_HP\t-862.563193\t-789.008878\n",
      "0015.2001-02-12.kitchen\t-4014.751533\t-4389.156067\n",
      "0009.2001-06-26.SA_and_HP\t-993.570765\t-927.208692\n",
      "0017.1999-12-14.kaminski\t-301.944797\t-309.292003\n",
      "0012.2000-01-17.beck\t-2637.871832\t-3254.060872\n",
      "0003.2000-01-17.beck\t-1078.483489\t-1310.072828\n",
      "0004.2001-06-12.SA_and_HP\t-868.637752\t-788.705106\n",
      "0008.2001-06-12.SA_and_HP\t-868.637752\t-788.705106\n",
      "0007.2001-02-09.kitchen\t-1270.395253\t-1397.262276\n",
      "0016.2004-08-01.BG\t-519.870537\t-469.014021\n",
      "0015.2000-06-09.lokay\t-118.268870\t-123.130618\n",
      "0005.1999-12-14.farmer\t-940.573919\t-1223.542436\n",
      "0016.1999-12-15.farmer\t-649.832079\t-691.516450\n",
      "0013.2004-08-01.BG\t-978.098195\t-925.623005\n",
      "0005.2003-12-18.GP\t-5292.278054\t-4837.298752\n",
      "0012.2001-02-09.kitchen\t-343.594814\t-368.600320\n",
      "0003.2001-02-08.kitchen\t-1102.956774\t-1310.120221\n",
      "0009.2001-02-09.kitchen\t-4588.517052\t-5159.862634\n",
      "0006.2001-02-08.kitchen\t-7584.035838\t-8176.341411\n",
      "0014.2003-12-19.GP\t-119.863143\t-111.297579\n",
      "0010.1999-12-14.farmer\t-801.094858\t-1193.947671\n",
      "0010.2004-08-01.BG\t-1659.758984\t-1458.311753\n",
      "0014.1999-12-14.kaminski\t-1580.916647\t-1974.314808\n",
      "0006.1999-12-13.kaminski\t-356.283376\t-370.034796\n",
      "0011.1999-12-14.farmer\t-1548.494272\t-1698.594143\n",
      "0013.1999-12-14.kaminski\t-1179.100045\t-1544.533719\n",
      "0001.2001-02-07.kitchen\t-290.669953\t-343.798165\n",
      "0008.2001-02-09.kitchen\t-2331.436280\t-2927.012999\n",
      "0007.2003-12-18.GP\t-608.327268\t-564.666097\n",
      "0017.2004-08-02.BG\t-1956.415904\t-1850.092309\n",
      "0014.2004-08-01.BG\t-381.579498\t-362.060641\n",
      "0006.2003-12-18.GP\t-558.121537\t-525.817076\n",
      "0016.2001-07-05.SA_and_HP\t-862.563193\t-789.008878\n",
      "0008.2003-12-18.GP\t-794.331861\t-710.079229\n",
      "0014.2001-07-04.SA_and_HP\t-3118.036850\t-2822.828308\n",
      "0001.2001-04-02.williams\t-1014.278637\t-1028.136738\n",
      "0012.2000-06-08.lokay\t-674.203814\t-676.575990\n",
      "0014.1999-12-15.farmer\t-916.795576\t-1073.477826\n",
      "0009.2000-06-07.lokay\t-1897.420033\t-2169.201872\n",
      "0001.1999-12-10.farmer\t-24.043393\t-26.315386\n",
      "0008.2001-06-25.SA_and_HP\t-3933.973949\t-3405.315016\n",
      "0017.2001-04-03.williams\t-332.671326\t-336.930913\n",
      "0014.2001-02-12.kitchen\t-888.005590\t-999.818067\n",
      "0016.2001-07-06.SA_and_HP\t-16613.762750\t-14476.834281\n",
      "0015.1999-12-15.farmer\t-643.602120\t-784.258437\n",
      "0009.1999-12-13.kaminski\t-5735.757760\t-8266.987828\n",
      "0001.2000-06-06.lokay\t-3053.526175\t-3435.308279\n",
      "0011.2004-08-01.BG\t-425.212785\t-387.447241\n",
      "0004.2004-08-01.BG\t-423.847576\t-402.167585\n",
      "0018.2003-12-18.GP\t-2626.641477\t-2463.897063\n",
      "0002.1999-12-13.farmer\t-2281.425472\t-2731.423953\n",
      "0016.2003-12-19.GP\t-361.690901\t-333.421685\n",
      "0004.1999-12-14.farmer\t-985.626659\t-1333.320666\n",
      "0015.2003-12-19.GP\t-1072.896822\t-960.927565\n",
      "0006.2004-08-01.BG\t-645.976217\t-602.063466\n",
      "0009.2003-12-18.GP\t-358.631913\t-332.404984\n",
      "0007.1999-12-14.farmer\t-453.119111\t-513.346899\n",
      "0005.2000-06-06.lokay\t-220.118758\t-219.300584\n",
      "0010.1999-12-14.kaminski\t-149.605557\t-168.715591\n",
      "0007.2000-01-17.beck\t-2622.636950\t-3239.883207\n",
      "0003.1999-12-14.farmer\t-74.767544\t-92.219122\n",
      "0003.2004-08-01.BG\t-449.308868\t-414.711983\n",
      "0017.2004-08-01.BG\t-121.319297\t-111.618104\n",
      "0013.2001-06-30.SA_and_HP\t-27003.730322\t-23800.516692\n",
      "0003.1999-12-10.kaminski\t-343.971004\t-395.329648\n",
      "0012.1999-12-14.farmer\t-2654.828232\t-3117.290293\n",
      "0004.1999-12-10.kaminski\t-821.783098\t-944.703992\n",
      "0018.2001-07-13.SA_and_HP\t-2807.084205\t-2612.452132\n",
      "0002.2001-02-07.kitchen\t-335.394502\t-340.484921\n",
      "0007.2004-08-01.BG\t-1266.259597\t-1070.500092\n",
      "0012.1999-12-14.kaminski\t-886.991669\t-1208.203126\n",
      "0005.2001-06-23.SA_and_HP\t-136.814838\t-126.474070\n",
      "0007.1999-12-13.kaminski\t-1313.649690\t-1478.175404\n",
      "0017.2000-01-17.beck\t-2629.200375\t-3246.208981\n",
      "0006.2001-06-25.SA_and_HP\t-243.604029\t-230.717022\n",
      "0006.2001-04-03.williams\t-209.133558\t-213.095841\n",
      "0005.2001-02-08.kitchen\t-580.593573\t-660.169827\n",
      "0002.2003-12-18.GP\t-861.233147\t-770.352186\n",
      "0003.2003-12-18.GP\t-590.606092\t-553.871157\n",
      "0013.2001-04-03.williams\t-502.387335\t-513.126455\n",
      "0004.2001-04-02.williams\t-528.052005\t-550.320500\n",
      "0010.2001-02-09.kitchen\t-2506.278759\t-2876.942231\n",
      "0001.1999-12-10.kaminski\t-19.294321\t-20.294803\n",
      "0013.1999-12-14.farmer\t-1525.428787\t-1658.377344\n",
      "0015.1999-12-14.kaminski\t-499.090298\t-561.432347\n",
      "0012.2003-12-19.GP\t-116.937285\t-105.420242\n",
      "0016.2001-02-12.kitchen\t-905.716164\t-1057.115541\n",
      "0002.2004-08-01.BG\t-693.872103\t-653.234167\n",
      "0002.2001-05-25.SA_and_HP\t-460.075407\t-414.603387\n",
      "0011.2003-12-18.GP\t-351.079972\t-310.204185\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output produced from the mapreduce job\n",
    "!/usr/local/hadoop/bin/hdfs dfs -ls /user/hadoop/outputHW2-5b\n",
    "!/usr/local/hadoop/bin/hdfs dfs -cat /user/hadoop/outputHW2-5b/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/24 21:46:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "copyToLocal: `doc_scores25_no_update.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# Copy the model output from the 2nd reducer to the local file system for reporting purposes.\n",
    "!/usr/local/hadoop/bin/hdfs dfs -copyToLocal /user/hadoop/outputHW2-5b/part-00000 doc_scores25.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.6** Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm.  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "**2.6.1.** Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "\n",
    "ANSWER:  MultinomialNB - Scikit Learn with Smoothing Training Error: 0.020000\n",
    "\n",
    "**2.6.2.** Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error.\n",
    "\n",
    "ANSWER:\n",
    "\n",
    "| Algorithm | Training Error |\n",
    "| ------ | ----------- |\n",
    "| MultinomialNB - SciKit-Learn   | 2% |\n",
    "| MultinomialNB - Hadoop Streaming | 2% |\n",
    "\n",
    "**2.6.3.** Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn \n",
    "\n",
    "ANSWER:  There is no difference between the training error rate with the HW2.5 solution and the SciKit-Learn Multinomial Naive Bayes implemention.  The training error rates are the same.  This makes sense for the following reasons:  1.  Add-1 smoothing was used in both cases.  2.  The words fed in to each algorithm were the same.  The text was first parsed in the same way and a regular expression was used to break down the text in to words.  The reduced vocabulary where only words that appeared at least 3 times was used for both algorithms.  (To enable this for the SciKit-Learn algorithm, the vocabulary from the model in HW2.5 was read in and passed to the CountVectorizer function so that only the tokens from the reduced vocabulary were used to train and test the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 2.6.1\n",
      "MultinomialNB - SciKit-Learn with smoothing Training Error: 0.020000\n",
      "\n",
      "PART 2.6.2\n",
      "                                  Training Error\n",
      "--------------------------------  ----------------\n",
      "MultinomialNB - SciKit-Learn      2%\n",
      "MultinomialNB - Hadoop Streaming  2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import SK-learn libraries for learning.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Load the model from the model.txt file.  Extrat the vocabulary from HW2.5 into the vocab list\n",
    "vocab = []\n",
    "with open(\"model25.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        elements = line.split('\\t')\n",
    "        if elements[0] == 'prior,prob':\n",
    "            pass\n",
    "        else:\n",
    "            vocab.append(elements[0])\n",
    "            \n",
    "# Transform the training data into a format that the sklearn libraries can use.\n",
    "# train_ids:  stores the IDs from the training data.\n",
    "train_ids = []\n",
    "# train_data:  stores the text from each document\n",
    "train_data = []\n",
    "# train_labels:  stores the true labels for each document\n",
    "train_labels = []\n",
    "# set filename to the file that is being analyzed\n",
    "filename = \"enronemail_1h.txt\"\n",
    "# Open the file\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Loop through each line in the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the lines by tab\n",
    "        items = line.split('\\t')\n",
    "        # This bypasses line 60 which is only a partial input line, so it is ignored\n",
    "        if len(items) == 1:\n",
    "            continue\n",
    "        # Add each ID to the 'train_ids' list\n",
    "        train_ids.append(items[0])\n",
    "        # Add each label to the 'train_labels' list\n",
    "        train_labels.append(int(items[1]))\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_all = re.findall(WORD_RE, items[2].lower())\n",
    "        # If the 4th value exists, it's the body of the line.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        if len(items) == 4:\n",
    "            words_all = words_all + re.findall(WORD_RE, items[3].lower())\n",
    "        # Create one string containing all of the words in the document separated by spaces\n",
    "        # and then add that string to the 'train_data' list\n",
    "        train_data.append(' '.join(words_all))\n",
    "\n",
    "# Create a CountVectorizer object that will transform the text data into a feature vector\n",
    "cv = CountVectorizer(vocabulary=vocab)\n",
    "# Transform the train_data into a feature vector\n",
    "train_fv = cv.fit_transform(train_data)\n",
    "\n",
    "##### PART 2.6.1 #######\n",
    "# Create a MultinomialNB model using the default parameters\n",
    "multiNB = MultinomialNB()\n",
    "# Fit the model with the training data\n",
    "multiNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = multiNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('PART 2.6.1')\n",
    "print('MultinomialNB - SciKit-Learn with smoothing Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('')\n",
    "\n",
    "##### PART 2.6.2 #######\n",
    "# Import tabulate to create a table\n",
    "from tabulate import tabulate\n",
    "# Create a variable 'headers' to store the headers of the table\n",
    "headers = ['   ', 'Training Error']\n",
    "# Initialize a list called data to hold the data for the rows of the table\n",
    "data = [['MultinomialNB - SciKit-Learn', '2%'],\n",
    "        ['MultinomialNB - Hadoop Streaming', '2%']]\n",
    "\n",
    "# Print the table\n",
    "print('PART 2.6.2')\n",
    "print(tabulate(data, headers=headers))\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
