{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  1\n",
    "* **Date:**  5/16/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW0.0:** Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell.\n",
    "\n",
    "**Bio:**\n",
    "I am currently working on my Master of Information and Data Science (MIDS) degree at UC Berkeley (expected graduation August 2016).  My courses include Machine Learning at Scale, Statistics, Data Analysis and Data Storage and Retrieval.  I have a bachelor’s and master’s degree in computer science from MIT and 10 years work experience as a technical project manager at Trilogy Software (proprietary software company), Nordstrom.com and Washington Mutual Bank.  I live in Seattle, Washington where I enjoy improvisation comedy and biking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bio word length:**  81 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.0.** Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "Big data is a term used in the digital age to describe the large volumes of data that are now be collected by organizations.  This data includes click streams, log files and mathematical results.  The term is usually described in relation to 3 categories:  volume, velocity and variety.  \n",
    "\n",
    "Volume:  Every day the world is creating approximately 10^15 bytes of data.  Ninety percent of the data that exists today was created in the last 2 years.  This data comes in the form of sensor data, log files, videos, images, sales transactions, social media and more.\n",
    "\n",
    "Velocity:  Although batch jobs analyzing massive quantities of data are still run at off-peak times, data collection is all occurring real-time from social media sites and log files and other sources.  Organizations want to analyze this data in real-time.\n",
    "\n",
    "Variety:  Data is coming from different sources and can occur in different formats.  Data could be structured with a regular schema as is found in a relational database or data could be unstructured like emails and audio and video files.\n",
    "\n",
    "It is common to think of big data as an amount of data that currently overwhelms the systems in place in terms of storage, processing and cost.  As the world deals with the new amount, speed and type of data being generated, new technologies are being created to store and process it.\n",
    "\n",
    "An example of a company using big data is insurance companies putting sensors in cars and recording their customer’s driving habits like speed or the amount of pressure used for braking.  Depending on how frequently the sensor output data, the amount of data collected per driver could be very large.  They can then process and analyze this data and create new metrics to use for assessing a customer’s insurance risk.  One such example would be to find out if a customer regularly drives the speed limit.  It could be argued that if a person regularly exceeds the speed limit, then they are not as safe of a driver as a person who follows the speed limit.  Drivers that are less safe could be a bigger insurance risk and could be charged higher rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**HW1.0.1.**  Bias Variance.  In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?\n",
    "\n",
    "Not included in this document.  Due date extended to 5/20/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.1.** Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below.  A simple cell in the notebook with a print statmement with a \"done\" string will suffice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.2.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance\"' are as follows:\n",
    "\n",
    "Total number of occurances of \"assistance\" is: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.2.\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Create a variable to store the count of the word that is passed in to the function\n",
    "# Initialize this count to 0.\n",
    "count = 0\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# findword is the 2nd argument passed in to the module and is the word whose\n",
    "# occurances will be counted.  Convert the string to lowercase.\n",
    "findword = sys.argv[2].lower()\n",
    "\n",
    "# Open the file passed in to the module\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Loop through each line of the file.\n",
    "    for line in myfile.readlines():\n",
    "        # Create a list of words found in each line\n",
    "        words = re.findall(WORD_RE, line.lower())\n",
    "        # Count the number of occruances of findword in the words list.\n",
    "        count += words.count(findword)\n",
    "\n",
    "# Print the name of the word and the number of occurances of the word.  These values\n",
    "# will be passed to reducer.py\n",
    "print('%s %d' % (findword, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "\n",
    "# Create a variable to store the total number occurances of the word that was given by the user\n",
    "# Initialize this total to 0.\n",
    "total = 0\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # Loop through each line of the file.\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by spaces\n",
    "            words = line.split(\" \")\n",
    "            # The 1st value in the line is the word that is being analyzed.\n",
    "            findword = words[0]\n",
    "            # The 2nd value in the line is the count of the word from each file.  Add the count from\n",
    "            # this line to the total count variable.\n",
    "            total += int(words[1])\n",
    "\n",
    "# Print the word that is being analyzed as well as the total number of occurances of the word.\n",
    "print('Total number of occurances of %s is: %d' % (findword, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the permissions for mapper.py, reducer.py and pNaiveBayes.sh so that they include\n",
    "# execute permissions.\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of occurances of assistance is: 10\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.2 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output\n",
    "!rm enronemail_1h.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.3.** HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   \n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance\"' are that as follows:\n",
    "\n",
    "The program had an error rate of 40%.  It classified 40 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# vocab is the 2nd argument passed in to the module and is the word that will be used\n",
    "# to classify the documents.  Convert the string to lowercase.\n",
    "vocab = sys.argv[2].lower()\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the word that is\n",
    "# being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Store the number of occurances of the vocab word in the document\n",
    "        vocab_terms_in_doc = words_all.count(vocab)\n",
    "        # Update 'count_vocab_terms' for the class of this document\n",
    "        count_vocab_terms[TRUTH] += vocab_terms_in_doc \n",
    "        # Print the ID, TRUTH and count of the vocab word that get passed to reducer.py\n",
    "        print('%s\\t%s\\t%d' % (ID, TRUTH, vocab_terms_in_doc))\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "print('count_vocab_terms\\t%d\\t%d' % (count_vocab_terms[0], count_vocab_terms[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the word that is\n",
    "# being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    count_vocab_terms[0] += int(elements[1])\n",
    "                    count_vocab_terms[1] += int(elements[2])                 \n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and the # of occurances of the vocab\n",
    "                # word in a list.\n",
    "                TRUTH = int(elements[1])\n",
    "                vocab_terms_in_doc = int(elements[2])\n",
    "                all_docs[ID] = [TRUTH, vocab_terms_in_doc]\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of the vocab word for\n",
    "# each class.\n",
    "condprob = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) by dividing the total # of terms in that class by the \n",
    "# total number of terms in all document in that class\n",
    "# Do not use smoothing.\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    condprob[cls] = count_vocab_terms[cls] / float(count_words[cls])\n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# (# of times the vocab term occurs in the document) * log of the conditional probability for the\n",
    "# vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "for key, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        score[cls] += vocab_terms_in_doc * math.log(condprob[cls])\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (key, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "Training Error: 0.400000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.3 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output\n",
    "!rm enronemail_1h.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.4.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy).\n",
    "\n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"' are that as follows:\n",
    "\n",
    "* The program had an accuracy of 60%. It classified 60 documents correctly.\n",
    "* The program had an error rate of 40%. It classified 40 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "# Import pring function from python 3\n",
    "from __future__ import print_function\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# vocab is the 2nd argument passed in to the module and is the list of words that will be used\n",
    "# to classify the documents.  Convert the string to lowercase and create a list called vocab\n",
    "# that stores each word passed in.\n",
    "vocab = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "for term in vocab:\n",
    "    count_vocab_terms[term] = [0, 0]\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Print the ID and true classification to send reducer.py\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "        # of the term' for reducer.py\n",
    "        for term in vocab:\n",
    "            # Store the number of occurances of the vocab word in the document\n",
    "            count_term = words_all.count(term)\n",
    "            # Update 'count_vocab_terms' for the class of this document\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "# Keep track of the first file in order to just store once the vocab that is being analyzed\n",
    "FIRST_FILE = True\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "                    # if it's the first file, save the vocab\n",
    "                    if FIRST_FILE:\n",
    "                        vocab.append(term)\n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and a dictionary consisting of the each\n",
    "                # term in the vocab followed by # of occurances of that term\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "    # Update the FIRST_FILE variable after the first file is read.\n",
    "    FIRST_FILE = False\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in vocab:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Do not use smoothing.\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in vocab:\n",
    "        condprob[term][cls] = count_vocab_terms[term][cls] / float(count_words[cls])\n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# for each term in the vocab: (# of times the vocab term occurs in the document) * log of the\n",
    "# conditional probability for the vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            if condprob[term][cls] > 0.0:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Accuracy: %f' % (count_correct / float(count_total)))\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "Accuracy: 0.600000\r\n",
      "Training Error: 0.400000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.4 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output\n",
    "!rm enronemail_1h.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.5.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "   \n",
    "The results from running the command line code './pNaiveBayes.sh 4' are that as follows:\n",
    "\n",
    "The program had an error rate of 0%.  It classified 0 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "# Import pring function from python 3\n",
    "from __future__ import print_function\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "# Variable vocab:  stores the unique terms used in this file\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Create a variable to store the unique words from this line.\n",
    "        vocab_line = []\n",
    "        # Loop through each of the terms in the words of this line\n",
    "        for term in words_all:\n",
    "            # Update the vocab for this file\n",
    "            if term not in vocab:\n",
    "                vocab.append(term)\n",
    "            # Update the vocab for this line\n",
    "            if term not in vocab_line:\n",
    "                vocab_line.append(term)\n",
    "            # Initialize a list to store the count of this term\n",
    "            if term not in count_vocab_terms:\n",
    "                count_vocab_terms[term] = [0,0]\n",
    "        # Print the ID and true classification to send reducer.py\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "        # of the term' for reducer.py\n",
    "        for term in vocab_line:\n",
    "            # Store the number of occurances of the vocab word in the document\n",
    "            count_term = words_all.count(term)\n",
    "            # Update 'count_vocab_terms' for the class of this document\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and a dictionary consisting of each\n",
    "                # term in the vocab followed by # of occurances of that term\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Use smoothing by adding 1.0 to the numerator and the denominator in the condprob equation.\n",
    "# Create a variable to control if smoothing is used.\n",
    "SMOOTHING = True\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in count_vocab_terms:\n",
    "        # If using smoothing, then add 1.0 to the numerator and the denominator in the condprob equation.\n",
    "        if SMOOTHING:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "        # If NOT using smoothing, then leave the numerator and the denominator alone.\n",
    "        else:\n",
    "            condprob[term][cls] = (count_vocab_terms[term][cls]) / (float(count_words[cls]))\n",
    "            \n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# for each term in the vocab: (# of times the vocab term occurs in the document) * log of the\n",
    "# conditional probability for the vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "count_docs_zero_prob = 0\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            # If any of the conditional probabilities for the document are 0, then set the score\n",
    "            # to -100000000 because, in actuality the probabilities are being multiplied, so if one is 0\n",
    "            # then the whole score will be 0.  If this happens, set the score to a very low number so that\n",
    "            # this class will have a very low score and won't be selected and break out of the loop\n",
    "            if condprob[term][cls] == 0.0:\n",
    "                score[cls] = -1000000000\n",
    "                count_docs_zero_prob += 1\n",
    "                #print('%d, term: %s, class: %d' % (count_docs_zero_prob, term, cls))\n",
    "                break\n",
    "            else:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    i += 1\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Accuracy: %f' % (count_correct / float(count_total)))\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))\n",
    "if not SMOOTHING:\n",
    "    print('With NO smoothing, number of documents that contain at least one term with zero probability')\n",
    "    print('in its non-true class: %d' % (count_docs_zero_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "Accuracy: 1.000000\r\n",
      "Training Error: 0.000000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.5 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output\n",
    "!rm enronemail_1h.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.6.** Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes.\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "**1.6.1.** Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "\n",
    "ANSWER:  MultinomialNB - Scikit Learn Training Error: 0.000000\n",
    "\n",
    "**1.6.2.** Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "\n",
    "ANSWER:  BernoullNB - Scikit Learn Training Error: 0.160000\n",
    "\n",
    "**1.6.3.** Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "\n",
    "ANSWER:  MultinomialNB - Command Line Training Error: 0.000000\n",
    "\n",
    "**1.6.4.** Please prepare a table to present your results\n",
    "\n",
    "ANSWER:\n",
    "\n",
    "| Algorithm | Training Error |\n",
    "| ------ | ----------- |\n",
    "| MultinomialNB - SciKit-Learn   | 0% |\n",
    "| BernoulliNB - SciKit-Learn | 16% |\n",
    "| MultinomialNB - Command Line | 0% |\n",
    "\n",
    "**1.6.5.** Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture).\n",
    "\n",
    "ANSWER:  There is no difference between the training error rate with the HW1.5 solution and the SciKit-Learn Multinomial Naive Bayes implemention.  The training error rates are the same.  This makes sense for the following reasons:  1.  Add-1 smoothing was used in both cases.  2.  The words fed in to each algorithm were the same.  The text was first parsed in the same way and a regular expression was used to break down the text in to words.  The same set of words were passed to both the algorithms.\n",
    "\n",
    "**1.6.6.** Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the Bernoulli Naive Bayes implementation in SciKit-Learn.\n",
    "\n",
    "ANSWER:  The training error rate for the SciKit-Learn MultinomialNB was 0% but for the SciKit-Learn Bernoulli algorithm it was 16%.  In the MultinomialNB model, the number of occurances of a word in a document is used to make the classification and there is no penalty assigned if a word from the vocabulary is not in the document.  In the BernouliNB model, the existence of a word in a document (either yes or no) is used to make the classification and there is a penalty assigned if a word from the vocabulary is not in the document.  Given these definitions, since the training error rate is higher for the Bernoulli model, it means that the frequency of a word in a document makes a difference in it's classification.  And it also means that the words that are present in the document make more of a difference in the classification that the words that are not present.\n",
    "\n",
    "**1.6.7.** What is the accuracy of the your SciKit-Learn model on the training data for a Naive Bayes model with smoothing and without smoothing using all the vocabulary? Please comment on what you see? Explain!\n",
    "\n",
    "ANSWER:  The accuracy of the SciKit-Learn Multinomial Naive Bayes model with and without smoothing is the same.  They are both 100% accurate.\n",
    "\n",
    "The reason that the non-smoothing model is 100% accurate is because without smoothing if a term occurs in a document, but has 0 conditional probability for one of the classes, then the score for that class will go to 0 and the model will predict the other class.  In this case, in the solution to HW1.5, the reducer has the option of running with smoothing or without smoothing by setting the SMOOTHING parameter to True for smoothing and False for no smoothing.  When it is run with no smoothing it counts how many documents encounter 0 probabilities while doing the classification.  The count comes out to 100 or all of them.  Said another way, each document contains at least one term that exists in exactly one class: the class that that document belongs to.  \n",
    "\n",
    "Here is an example.  Document 0001.1999-12-10.farmer is Ham (0).  It contains 5 terms:  'christmas', 'tree', 'farm', 'pictures' and 'NA'.  The terms 'christmas' and 'farm' only appear in documents that are Ham.  They never appear in documents that are Spam.  This means that the conditional probabilities of 'christmas' and 'farm' for the Spam class will be 0.  Further, this means that, since the probabilities are multiplied together, the probability that the document is Spam will be 0, so Ham will be chosen as the class. This document contains 2 terms that only occur in the Ham class.\n",
    "\n",
    "In summary for the non-smoothing model, all documents contain at least 1 term that occurs only in its class and not in the other class.  This forces the probability for the incorrect class to be 0, therefore the correct class is always chosen.\n",
    "\n",
    "The reason that the smoothing model is 100% accurate is because the set of words, H, that occur frequently in Ham documents does not have a lot of overlap with the set of words, S, that occur frequently in Spam documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 1.6.1\n",
      "MultinomialNB - SciKit-Learn with smoothing Training Error: 0.000000\n",
      "\n",
      "PART 1.6.2\n",
      "BernoulliNB - SciKit-Learn Training Error: 0.160000\n",
      "\n",
      "PART 1.6.3\n",
      "MultinomialNB - Command Line Training Error: 0.000000\n",
      "\n",
      "PART 1.6.4\n",
      "                              Training Error\n",
      "----------------------------  ----------------\n",
      "MultinomialNB - SciKit-Learn  0%\n",
      "BernoulliNB - SciKit-Learn    16%\n",
      "MultinomialNB - Command Line  0%\n",
      "\n",
      "PART 1.6.7\n",
      "MultinomialNB - SciKit-Learn with NO smoothing Training Error: 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import SK-learn libraries for learning.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Transform the training data into a format that the sklearn libraries can use.\n",
    "# train_ids:  stores the IDs from the training data.\n",
    "train_ids = []\n",
    "# train_data:  stores the text from each document\n",
    "train_data = []\n",
    "# train_labels:  stores the true labels for each document\n",
    "train_labels = []\n",
    "# set filename to the file that is being analyzed\n",
    "filename = \"enronemail_1h.txt\"\n",
    "# Open the file\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Loop through each line in the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the lines by tab\n",
    "        items = line.split('\\t')\n",
    "        # Add each ID to the 'train_ids' list\n",
    "        train_ids.append(items[0])\n",
    "        # Add each label to the 'train_labels' list\n",
    "        train_labels.append(int(items[1]))\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Create one string containing all of the words in the document separated by spaces\n",
    "        # and then add that string to the 'train_data' list\n",
    "        train_data.append(' '.join(words_all))\n",
    "\n",
    "# Create a CountVectorizer object that will transform the text data into a feature vector\n",
    "cv = CountVectorizer()\n",
    "# Transform the train_data into a feature vector\n",
    "train_fv = cv.fit_transform(train_data)\n",
    "\n",
    "##### PART 1.6.1 #######\n",
    "# Create a MultinomialNB model using the default parameters\n",
    "multiNB = MultinomialNB()\n",
    "# Fit the model with the training data\n",
    "multiNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = multiNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('PART 1.6.1')\n",
    "print('MultinomialNB - SciKit-Learn with smoothing Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('')\n",
    "\n",
    "##### PART 1.6.2 #######\n",
    "# Create a MultinomialNB model\n",
    "BernoulliNB = BernoulliNB()\n",
    "# Fit the model with the training data\n",
    "BernoulliNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = BernoulliNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('PART 1.6.2')\n",
    "print('BernoulliNB - SciKit-Learn Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('')\n",
    "\n",
    "##### PART 1.6.3 #######\n",
    "print('PART 1.6.3')\n",
    "print('MultinomialNB - Command Line Training Error: 0.000000')\n",
    "print('')\n",
    "\n",
    "##### PART 1.6.4 #######\n",
    "# Import tabulate to create a table\n",
    "from tabulate import tabulate\n",
    "# Create a variable 'headers' to store the headers of the table\n",
    "headers = ['   ', 'Training Error']\n",
    "# Initialize a list called data to hold the data for the rows of the table\n",
    "data = [['MultinomialNB - SciKit-Learn', '0%'],\n",
    "        ['BernoulliNB - SciKit-Learn', '16%'],\n",
    "        ['MultinomialNB - Command Line', '0%']]\n",
    "\n",
    "# Print the table\n",
    "print('PART 1.6.4')\n",
    "print(tabulate(data, headers=headers))\n",
    "print('')\n",
    "\n",
    "##### PART 1.6.7 #######\n",
    "# Create a MultinomialNB model using no smoothing (alpha = 0) \n",
    "multiNB = MultinomialNB(alpha=0)\n",
    "# Fit the model with the training data\n",
    "multiNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = multiNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('PART 1.6.7')\n",
    "print('MultinomialNB - SciKit-Learn with NO smoothing Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
