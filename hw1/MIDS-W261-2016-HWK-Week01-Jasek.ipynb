{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **Name:**  Megan Jasek\n",
    "* **Email:**  meganjasek@ischool.berkeley.edu\n",
    "* **Class Name:**  W261-2\n",
    "* **Week Number:**  1\n",
    "* **Date:**  5/16/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW0.0:** Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell.\n",
    "\n",
    "**Bio:**\n",
    "I have a bachelor and master degree in computer science from MIT.  I worked for 10 years as a technical project manager at 3 different companies:  Trilogy software, Nordstrom and Chase Bank.  I am enrolled in the master of data science (MIDS) program at UC Berkeley and expect to graduate in August 2016.  I am interested in machine learning in general and specifically machine learning at scale and deep learning.  I live in Seattle, Washington."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bio word length:**  75 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.0.** Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**HW1.0.1.**  Bias Variance.  In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?\n",
    "\n",
    "Not included in this document.  Due date extended to 5/20/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.1.** Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below.  A simple cell in the notebook with a print statmement with a \"done\" string will suffice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.2.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance\"' are as follows:\n",
    "\n",
    "Total number of occurances of \"assistance\" is: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.2.\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Create a variable to store the count of the word that is passed in to the function\n",
    "# Initialize this count to 0.\n",
    "count = 0\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# findword is the 2nd argument passed in to the module and is the word whose\n",
    "# occurances will be counted.  Convert the string to lowercase.\n",
    "findword = sys.argv[2].lower()\n",
    "\n",
    "# Open the file passed in to the module\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Loop through each line of the file.\n",
    "    for line in myfile.readlines():\n",
    "        # Create a list of words found in each line\n",
    "        words = re.findall(WORD_RE, line.lower())\n",
    "        # Count the number of occruances of findword in the words list.\n",
    "        count += words.count(findword)\n",
    "\n",
    "# Print the name of the word and the number of occurances of the word.  These values\n",
    "# will be passed to reducer.py\n",
    "print('%s %d' % (findword, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "\n",
    "# Create a variable to store the total number occurances of the word that was given by the user\n",
    "# Initialize this total to 0.\n",
    "total = 0\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # Loop through each line of the file.\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by spaces\n",
    "            words = line.split(\" \")\n",
    "            # The 1st value in the line is the word that is being analyzed.\n",
    "            findword = words[0]\n",
    "            # The 2nd value in the line is the count of the word from each file.  Add the count from\n",
    "            # this line to the total count variable.\n",
    "            total += int(words[1])\n",
    "\n",
    "# Print the word that is being analyzed as well as the total number of occurances of the word.\n",
    "print('Total number of occurances of %s is: %d' % (findword, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the permissions for mapper.py, reducer.py and pNaiveBayes.sh so that they include\n",
    "# execute permissions.\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of occurances of assistance is: 10\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.2 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.3.** HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "   \n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance\"' are that as follows:\n",
    "\n",
    "The program had an error rate of 40%.  It classified 40 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# vocab is the 2nd argument passed in to the module and is the word that will be used\n",
    "# to classify the documents.  Convert the string to lowercase.\n",
    "vocab = sys.argv[2].lower()\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the word that is\n",
    "# being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Store the number of occurances of the vocab word in the document\n",
    "        vocab_terms_in_doc = words_all.count(vocab)\n",
    "        # Update 'count_vocab_terms' for the class of this document\n",
    "        count_vocab_terms[TRUTH] += vocab_terms_in_doc \n",
    "        # Print the ID, TRUTH and count of the vocab word that get passed to reducer.py\n",
    "        print('%s\\t%s\\t%d' % (ID, TRUTH, vocab_terms_in_doc))\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "print('count_vocab_terms\\t%d\\t%d' % (count_vocab_terms[0], count_vocab_terms[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the word that is\n",
    "# being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    count_vocab_terms[0] += int(elements[1])\n",
    "                    count_vocab_terms[1] += int(elements[2])                 \n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and the # of occurances of the vocab\n",
    "                # word in a list.\n",
    "                TRUTH = int(elements[1])\n",
    "                vocab_terms_in_doc = int(elements[2])\n",
    "                all_docs[ID] = [TRUTH, vocab_terms_in_doc]\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of the vocab word for\n",
    "# each class.\n",
    "condprob = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) by dividing the total # of terms in that class by the \n",
    "# total number of terms in all document in that class\n",
    "# Do not use smoothing.\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    condprob[cls] = count_vocab_terms[cls] / float(count_words[cls])\n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# (# of times the vocab term occurs in the document) * log of the conditional probability for the\n",
    "# vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "for key, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        score[cls] += vocab_terms_in_doc * math.log(condprob[cls])\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (key, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "Training Error: 0.400000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.3 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.4.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy).\n",
    "\n",
    "The results from running the command line code './pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"' are that as follows:\n",
    "\n",
    "* The program had an accuracy of 60%. It classified 60 documents correctly.\n",
    "* The program had an error rate of 40%. It classified 40 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "# Import pring function from python 3\n",
    "from __future__ import print_function\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "# vocab is the 2nd argument passed in to the module and is the list of words that will be used\n",
    "# to classify the documents.  Convert the string to lowercase and create a list called vocab\n",
    "# that stores each word passed in.\n",
    "vocab = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "for term in vocab:\n",
    "    count_vocab_terms[term] = [0, 0]\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Print the ID and true classification to send reducer.py\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "        # of the term' for reducer.py\n",
    "        for term in vocab:\n",
    "            # Store the number of occurances of the vocab word in the document\n",
    "            count_term = words_all.count(term)\n",
    "            # Update 'count_vocab_terms' for the class of this document\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "# Keep track of the first file in order to just store once the vocab that is being analyzed\n",
    "FIRST_FILE = True\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "                    # if it's the first file, save the vocab\n",
    "                    if FIRST_FILE:\n",
    "                        vocab.append(term)\n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and a dictionary consisting of the each\n",
    "                # term in the vocab followed by # of occurances of that term\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "    # Update the FIRST_FILE variable after the first file is read.\n",
    "    FIRST_FILE = False\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in vocab:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Do not use smoothing.\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in vocab:\n",
    "        condprob[term][cls] = count_vocab_terms[term][cls] / float(count_words[cls])\n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# for each term in the vocab: (# of times the vocab term occurs in the document) * log of the\n",
    "# conditional probability for the vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            if condprob[term][cls] > 0.0:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Accuracy: %f' % (count_correct / float(count_total)))\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "Accuracy: 0.600000\r\n",
      "Training Error: 0.400000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.4 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.5.** Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "   \n",
    "The results from running the command line code './pNaiveBayes.sh 4' are that as follows:\n",
    "\n",
    "The program had an error rate of 0%.  It classified 0 documents incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "# Import pring function from python 3\n",
    "from __future__ import print_function\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "# filename is the 1st argument passed in to the module and is the name of the file\n",
    "# that is to be analyzed.\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the words in the \n",
    "# vocab that is being used for classification in each of the classes.\n",
    "# Variable vocab:  stores the unique terms used in this file\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "\n",
    "# Open the file that was passed in\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # For each line of the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line tabs and store the result in the 'items' list.\n",
    "        items = line.split('\\t')\n",
    "        # The 1st value of the line is the ID of the document\n",
    "        ID = items[0]\n",
    "        # The 2nd value of the line is the true classification of the document called: TRUTH\n",
    "        TRUTH = int(items[1])\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Update 'count_docs' by 1 for the class of this document\n",
    "        count_docs[TRUTH] += 1\n",
    "        # Update 'count_words' by the length of the 'words_all' list for the class of this document\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        # Create a variable to store the unique words from this line.\n",
    "        vocab_line = []\n",
    "        # Loop through each of the terms in the words of this line\n",
    "        for term in words_all:\n",
    "            # Update the vocab for this file\n",
    "            if term not in vocab:\n",
    "                vocab.append(term)\n",
    "            # Update the vocab for this line\n",
    "            if term not in vocab_line:\n",
    "                vocab_line.append(term)\n",
    "            # Initialize a list to store the count of this term\n",
    "            if term not in count_vocab_terms:\n",
    "                count_vocab_terms[term] = [0,0]\n",
    "        # Print the ID and true classification to send reducer.py\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        # For each of the words in the vocabulary print out the 'term' and the '# of occurances\n",
    "        # of the term' for reducer.py\n",
    "        for term in vocab_line:\n",
    "            # Store the number of occurances of the vocab word in the document\n",
    "            count_term = words_all.count(term)\n",
    "            # Update 'count_vocab_terms' for the class of this document\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# Print the count totals for each class that get passed to reducer.py\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "# Import sys library to access arguments passed in when the module is called\n",
    "import sys\n",
    "# Import the math library in order to use the 'log' method to take logorithms\n",
    "import math\n",
    "\n",
    "# Initialize a dictionary 'all_docs' which will store the necessary information for each\n",
    "# document that will be passed to the reducer.py.\n",
    "all_docs = {}\n",
    "# Initialize counts.  All of the variables are lists storing 2 counts.  The 1st count\n",
    "# in the list is the count for class 0 which is Ham and the 2nd count is the count\n",
    "# for class 1 which is Spam.\n",
    "# Variable count_docs:  stores the total number of documents for each class.\n",
    "# Variable count_words:  stores the total number of words in the subject and body fields\n",
    "# for each class.\n",
    "# Variable count_vocab_terms:  stores the total number of occurances of the each of the \n",
    "# words in the vocab that is being used for classification in each of the classes.\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# Loop through each of the files that are passed into the module.\n",
    "for i in range(1, len(sys.argv)):\n",
    "    # Each argument that was passed in is a filename.\n",
    "    filename = sys.argv[i]\n",
    "    # Open the file\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        # For each line in the file\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by tabs\n",
    "            elements = line.split(\"\\t\")\n",
    "            # The 1st value in the line is the ID of the document\n",
    "            ID = elements[0]\n",
    "            # If the ID is one of the labels that reports the counts of an entire file, the\n",
    "            # store that value appropriately.\n",
    "            if ID[:6] == 'count_':\n",
    "                # Update the 'count_docs' variable with the numbers from the file for each class\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                # Update the 'count_words' variable with the numbers from the file for each class\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                # Update the 'count_vocab_terms' variable with the numbers from the file for each class\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "            else:\n",
    "                # Save the document in the all_docs dictionary for later use.  Use the ID as the key\n",
    "                # for the dictionary.  Store the TRUTH value and a dictionary consisting of each\n",
    "                # term in the vocab followed by # of occurances of that term\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "\n",
    "\n",
    "# Create a list to store the 2 classes in this problem:  0 for Ham and 1 for Spam.\n",
    "classes = [0, 1]\n",
    "# Calculate the total number of documents by adding the total counts from each class.\n",
    "total_docs = sum(count_docs)\n",
    "# Initialize the 'prior' varialbe to store the prior probability for each class.\n",
    "prior = [0, 0]\n",
    "# Initialize the 'condprob' varialbe to store the conditional probilitity of each of the\n",
    "# terms in the vocab each class.\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "\n",
    "# Train the MultinomialNB classifer using the algorithm in the book: An Introduction to Information Retrieval\n",
    "# By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schutzepage page 260, Figure 13.2.\n",
    "# For each of the classes:\n",
    "# 1. Calculate the prior probability (prior) by dividing the total # of documents in that class by the total # of documents.\n",
    "# 2. Calculate the conditional probability (condprob) for each term in the vocab by dividing the\n",
    "# total # of that term in that class by the total number of terms in all documents in that class\n",
    "# Use smoothing by adding 1.0 to the numerator and the denominator in the condprob equation.\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in count_vocab_terms:\n",
    "        condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "\n",
    "# Initialize variables to track the total correct predictions and the total predictions\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "# Apply the MultinomialNB classifier to each document and make a prediction for each.\n",
    "# For each document:\n",
    "# 1. Initialize a score of 0 for each class\n",
    "# 2. Store the true value for that document in the 'TRUTH' variable\n",
    "# 3. Store the # of times the vocab term occurs in the document\n",
    "# 4. For each class: calculate the score as a sum of the log of the prior probability + \n",
    "# for each term in the vocab: (# of times the vocab term occurs in the document) * log of the\n",
    "# conditional probability for the vocab term.\n",
    "# 5. Make a prediction:  Choose the class with the highest score.\n",
    "# 6. Print the values to the output file:  ID, true value, prediction\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            if condprob[term][cls] > 0.0:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    # Update the totals used for accuracy\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('Accuracy: %f' % (count_correct / float(count_total)))\n",
    "print('Training Error: %f' % ((count_total - count_correct) / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "Accuracy: 1.000000\r\n",
      "Training Error: 0.000000\r\n"
     ]
    }
   ],
   "source": [
    "# Report the results from HW1.5 be examining the output file: 'enronemail_1h.txt.output' \n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.6.** Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes.\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "1. Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "2. Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "3. Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "4. Please prepare a table to present your results\n",
    "5. Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "6. Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "7. What is the accuracy of the your SciKit-Learn model on the training data for a Naive Bayes model with smoothing and without smoothing using all the vocabulary? Please comment on what you see? Explain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.000000\n",
      "Training Error: 0.160000\n"
     ]
    }
   ],
   "source": [
    "# Import SK-learn libraries for learning.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Import re library to manipulate regular expressions\n",
    "import re\n",
    "# Define the regular expression used to designate what a 'word' is among a string of\n",
    "# characters.\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Transform the training data into a format that the sklearn libraries can use.\n",
    "# train_ids:  stores the IDs from the training data.\n",
    "train_ids = []\n",
    "# train_data:  stores the text from each document\n",
    "train_data = []\n",
    "# train_labels:  stores the true labels for each document\n",
    "train_labels = []\n",
    "# set filename to the file that is being analyzed\n",
    "filename = \"enronemail_1h.txt\"\n",
    "# Open the file\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Loop through each line in the file\n",
    "    for line in myfile.readlines():\n",
    "        # Split the lines by tab\n",
    "        items = line.split('\\t')\n",
    "        # Add each ID to the 'train_ids' list\n",
    "        train_ids.append(items[0])\n",
    "        # Add each label to the 'train_labels' list\n",
    "        train_labels.append(int(items[1]))\n",
    "        # The 3rd value of the line is the subject.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        # The 4th value of the line is the body.  Convert this value to lowercase and then\n",
    "        # break it up into separate words using the WORD_RE regular expression\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        # Concatenate the subject and body in to one list called 'words_all'\n",
    "        words_all = words_subject + words_body\n",
    "        # Create one string containing all of the words in the document separated by spaces\n",
    "        # and then add that string to the 'train_data' list\n",
    "        train_data.append(' '.join(words_all))\n",
    "\n",
    "# Create a CountVectorizer object that will transform the text data into a feature vector\n",
    "cv = CountVectorizer()\n",
    "# Transform the train_data into a feature vector\n",
    "train_fv = cv.fit_transform(train_data)\n",
    "\n",
    "# Create a MultinomialNB model using the default parameters\n",
    "multiNB = MultinomialNB()\n",
    "# Fit the model with the training data\n",
    "multiNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = multiNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('Training Error: %f' % (count_incorrect / float(count_total)))\n",
    "\n",
    "# Create a MultinomialNB model\n",
    "BernoulliNB = BernoulliNB()\n",
    "# Fit the model with the training data\n",
    "BernoulliNB.fit(train_fv, train_labels)\n",
    "# Create predictions from the model using the training data\n",
    "predict = BernoulliNB.predict(train_fv)\n",
    "\n",
    "# Initialize variables to track the total incorrect predictions and the total predictions\n",
    "count_total = 0\n",
    "count_incorrect = 0\n",
    "# Calculate the training error by dividing the count_incorrect by the total count\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] != train_labels[i]:\n",
    "        count_incorrect += 1\n",
    "    count_total += 1\n",
    "print('Training Error: %f' % (count_incorrect / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.5_OLD\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Takes a list of words and returns a string separated by spaces of the\n",
    "# unique words in the list\n",
    "def findUniqueWords(word_list):\n",
    "    unique = []\n",
    "    for w in word_list:\n",
    "        if w not in unique:\n",
    "            unique.append(w)\n",
    "    return ' '.join(unique)\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "#findwords = re.split(\" \",sys.argv[2].lower())\n",
    "findword = sys.argv[2].lower()\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        items = line.split('\\t')\n",
    "        ID = items[0]\n",
    "        TRUTH = items[1]\n",
    "        # Create a list of words found in each line\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        words_all = words_subject + words_body\n",
    "        count_total_words = len(words_all)\n",
    "        all_text = ' '.join(words_all)\n",
    "        vocab = findUniqueWords(words_all)\n",
    "        print('%s\\t%s\\t%d\\t%s\\t%s' % (ID, TRUTH, count_total_words, vocab, all_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.5_OLD\n",
    "import sys\n",
    "\n",
    "def updateVocab(V, word_list):\n",
    "    for w in word_list:\n",
    "        if w not in V:\n",
    "            V.add(w)\n",
    "    return V\n",
    "\n",
    "def countTokensOfTerm(word_list, term):\n",
    "    return word_list.count(term)\n",
    "\n",
    "all_docs = {}\n",
    "all_vocab = set()\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "all_text = [[], []]\n",
    "\n",
    "# Loop through the files once to create the aggregate counts\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            elements = line.split(\"\\t\")\n",
    "            ID = elements[0]\n",
    "            TRUTH = int(elements[1])\n",
    "            count_docs[TRUTH] += 1\n",
    "            count_words[TRUTH] += int(elements[2])\n",
    "            vocab = elements[3].split(' ')\n",
    "            all_vocab = updateVocab(all_vocab, vocab)\n",
    "            text = elements[4].split(' ')\n",
    "            all_text[TRUTH] += text\n",
    "            # save the document for later use\n",
    "            all_docs[ID] = [TRUTH, text]\n",
    "\n",
    "total_docs = sum(count_docs)\n",
    "prior = [0, 0]\n",
    "condprob = [{}, {}]\n",
    "classes = [0, 1]\n",
    "\n",
    "# Train MultinomialNB\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in vocab:\n",
    "        count_term = countTokensOfTerm(all_text[cls], term)\n",
    "        condprob[cls][term] = (count_term + 1.0) / (count_words[cls] + 1.0)\n",
    "\n",
    "# Apply MultinomialNB\n",
    "for key, value in all_docs:\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    for cls in classes:\n",
    "        score[cls] = log(prior[cls])\n",
    "        text = value[1]\n",
    "        for term in text:\n",
    "            score[cls] += log(condprob[cls][term])\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (key, TRUTH, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./reducer.py\", line 52, in <module>\r\n",
      "    for key, value in all_docs:\r\n",
      "ValueError: too many values to unpack\r\n"
     ]
    }
   ],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 99 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2]\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    #Please insert your code\n",
    "    for line in myfile.readlines():\n",
    "        # Create a list of words found in each line\n",
    "        words = re.findall(WORD_RE, line)\n",
    "        count += words.count(findword)\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for line in sys.stdin:\n",
    "    #Please insert your code\n",
    "    sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pGrepCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pGrepCount.sh\n",
    "ORIGINAL_FILE=$1\n",
    "FIND_WORD=$2\n",
    "BLOCK_SIZE=$3\n",
    "CHUNK_FILE_PREFIX=$ORIGINAL_FILE.split\n",
    "SORTED_CHUNK_FILES=$CHUNK_FILE_PREFIX*.sorted\n",
    "usage()\n",
    "{\n",
    "    echo Parallel grep\n",
    "    echo usage: pGrepCount filename word chuncksize\n",
    "    echo greps file file1 in $ORIGINAL_FILE and counts the number of lines\n",
    "    echo Note: file1 will be split in chunks up to $ BLOCK_SIZE chunks each\n",
    "    echo $FIND_WORD each chunk will be grepCounted in parallel\n",
    "}\n",
    "#Splitting $ORIGINAL_FILE INTO CHUNKS\n",
    "split -b $BLOCK_SIZE $ORIGINAL_FILE $CHUNK_FILE_PREFIX\n",
    "#DISTRIBUTE\n",
    "for file in $CHUNK_FILE_PREFIX*\n",
    "do\n",
    "    #grep -i $FIND_WORD $file|wc -l >$file.intermediateCount &\n",
    "    ./mapper.py $FIND_WORD $file >$file.intermediateCount &\n",
    "done\n",
    "wait\n",
    "#MERGEING INTERMEDIATE COUNT CAN TAKE THE FIRST COLUMN AND TOTOL...\n",
    "#numOfInstances=$(cat *.intermediateCount | cut -f 1 | paste -sd+ - |bc)\n",
    "numOfInstances=$(cat *.intermediateCount | ./reducer.py)\n",
    "echo \"found [$numOfInstances] [$FIND_WORD] in the file [$ORIGINAL_FILE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pGrepCount.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pGrepCount filename word chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [11] [COPYRIGHT] in the file [License.txt]\r\n"
     ]
    }
   ],
   "source": [
    "!./pGrepCount.sh License.txt COPYRIGHT 4k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
