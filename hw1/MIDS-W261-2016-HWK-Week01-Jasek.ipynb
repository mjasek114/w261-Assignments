{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Name:  Megan Jasek\\n    Email:  meganjasek@berkeley.edu\\n    Class Name:  W261\\n    Week Number:  1\\n    Date:  5/16/16\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Name:  Megan Jasek\n",
    "    Email:  meganjasek@berkeley.edu\n",
    "    Class Name:  W261\n",
    "    Week Number:  1\n",
    "    Date:  5/16/16\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here.\n",
    "'''\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2].lower()\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Create a list of words found in each line\n",
    "        words = re.findall(WORD_RE, line.lower())\n",
    "        count += words.count(findword)\n",
    "\n",
    "print('%s %d' % (findword, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.2\n",
    "import sys\n",
    "total = 0\n",
    "\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            words = line.split(\" \")\n",
    "            findword = words[0]\n",
    "            total += int(words[1])\n",
    "\n",
    "print('%s\\t%d' % (findword, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "vocab = sys.argv[2].lower()\n",
    "\n",
    "# initialize counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        items = line.split('\\t')\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        # Create a list of words found in each line\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        words_all = words_subject + words_body\n",
    "        count_docs[TRUTH] += 1\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        vocab_terms_in_doc = words_all.count(vocab)\n",
    "        count_vocab_terms[TRUTH] += vocab_terms_in_doc \n",
    "        print('%s\\t%s\\t%d' % (ID, TRUTH, vocab_terms_in_doc))\n",
    "\n",
    "# print the total counts\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "print('count_vocab_terms\\t%d\\t%d' % (count_vocab_terms[0], count_vocab_terms[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.3\n",
    "import sys\n",
    "import math\n",
    "\n",
    "all_docs = {}\n",
    "# Initialize the counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = [0, 0]\n",
    "\n",
    "# Loop through the files\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            elements = line.split(\"\\t\")\n",
    "            ID = elements[0]\n",
    "            if ID[:6] == 'count_':\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                else:\n",
    "                    count_vocab_terms[0] += int(elements[1])\n",
    "                    count_vocab_terms[1] += int(elements[2])                 \n",
    "            else:\n",
    "                # save the document in the all_docs dictionary for later use\n",
    "                TRUTH = int(elements[1])\n",
    "                vocab_terms_in_doc = int(elements[2])\n",
    "                all_docs[ID] = [TRUTH, vocab_terms_in_doc]\n",
    "\n",
    "total_docs = sum(count_docs)\n",
    "prior = [0, 0]\n",
    "condprob = [0, 0]\n",
    "classes = [0, 1]\n",
    "\n",
    "# Train MultinomialNB\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    # do not use smoothing\n",
    "    condprob[cls] = count_vocab_terms[cls] / float(count_words[cls])\n",
    "\n",
    "# Apply MultinomialNB\n",
    "for key, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        score[cls] += vocab_terms_in_doc * math.log(condprob[cls])\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (key, TRUTH, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "vocab = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# initialize counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "for term in vocab:\n",
    "    count_vocab_terms[term] = [0, 0]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        items = line.split('\\t')\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        # Create a list of words found in each line\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        words_all = words_subject + words_body\n",
    "        count_docs[TRUTH] += 1\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        for term in vocab:\n",
    "            count_term = words_all.count(term)\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# print the total counts\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.4\n",
    "import sys\n",
    "import math\n",
    "\n",
    "all_docs = {}\n",
    "# Initialize the counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "vocab = []\n",
    "FIRST_FILE = True\n",
    "\n",
    "# Loop through the files\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            elements = line.split(\"\\t\")\n",
    "            ID = elements[0]\n",
    "            if ID[:6] == 'count_':\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "                    # if it's the first file, save the vocab\n",
    "                    if FIRST_FILE:\n",
    "                        vocab.append(term)\n",
    "            else:\n",
    "                # save the document in the all_docs dictionary for later use\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "    FIRST_FILE = False\n",
    "\n",
    "total_docs = sum(count_docs)\n",
    "prior = [0, 0]\n",
    "condprob = {}\n",
    "for term in vocab:\n",
    "    condprob[term] = [0, 0]\n",
    "classes = [0, 1]\n",
    "\n",
    "# Train MultinomialNB\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    # do not use smoothing\n",
    "    for term in vocab:\n",
    "        condprob[term][cls] = count_vocab_terms[term][cls] / float(count_words[cls])\n",
    "\n",
    "# Apply MultinomialNB\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            if condprob[term][cls] > 0.0:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('accuracy: %f' % (count_correct / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# initialize counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "vocab = []\n",
    "count_vocab_terms = {}\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        items = line.split('\\t')\n",
    "        ID = items[0]\n",
    "        TRUTH = int(items[1])\n",
    "        # Create a list of words found in each line\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        words_all = words_subject + words_body\n",
    "        count_docs[TRUTH] += 1\n",
    "        count_words[TRUTH] += len(words_all)\n",
    "        vocab_line = []\n",
    "        for term in words_all:\n",
    "            if term not in vocab:\n",
    "                vocab.append(term)\n",
    "            if term not in vocab_line:\n",
    "                vocab_line.append(term)\n",
    "            if term not in count_vocab_terms:\n",
    "                count_vocab_terms[term] = [0,0]\n",
    "        print('%s\\t%s' % (ID, TRUTH),end=\"\")\n",
    "        for term in vocab_line:\n",
    "            count_term = words_all.count(term)\n",
    "            count_vocab_terms[term][TRUTH] += count_term\n",
    "            print('\\t%s\\t%d' % (term, count_term),end=\"\")\n",
    "        print()\n",
    "\n",
    "# print the total counts\n",
    "print('count_docs\\t%d\\t%d' % (count_docs[0], count_docs[1]))\n",
    "print('count_words\\t%d\\t%d' % (count_words[0], count_words[1]))\n",
    "for term in vocab:\n",
    "    print('count_vocab_term\\t%s\\t%d\\t%d' % (term, count_vocab_terms[term][0], count_vocab_terms[term][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.5\n",
    "import sys\n",
    "import math\n",
    "\n",
    "all_docs = {}\n",
    "# Initialize the counts\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "count_vocab_terms = {}\n",
    "\n",
    "# Loop through the files\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            elements = line.split(\"\\t\")\n",
    "            ID = elements[0]\n",
    "            if ID[:6] == 'count_':\n",
    "                if ID == 'count_docs':\n",
    "                    count_docs[0] += int(elements[1])\n",
    "                    count_docs[1] += int(elements[2])\n",
    "                elif ID == 'count_words':\n",
    "                    count_words[0] += int(elements[1])\n",
    "                    count_words[1] += int(elements[2])\n",
    "                else:\n",
    "                    term = elements[1]\n",
    "                    if term not in count_vocab_terms:\n",
    "                        count_vocab_terms[term] = [int(elements[2]), int(elements[3])]\n",
    "                    else:\n",
    "                        count_vocab_terms[term][0] += int(elements[2])\n",
    "                        count_vocab_terms[term][1] += int(elements[3])\n",
    "            else:\n",
    "                # save the document in the all_docs dictionary for later use\n",
    "                TRUTH = int(elements[1])\n",
    "                terms = {}\n",
    "                for j in range(2, len(elements),2):\n",
    "                    term = elements[j]\n",
    "                    terms[term] = int(elements[j+1])\n",
    "                all_docs[ID] = [TRUTH, terms]\n",
    "\n",
    "total_docs = sum(count_docs)\n",
    "prior = [0, 0]\n",
    "condprob = {}\n",
    "for term in count_vocab_terms:\n",
    "    condprob[term] = [0, 0]\n",
    "classes = [0, 1]\n",
    "\n",
    "# Train MultinomialNB\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    # do not use smoothing\n",
    "    for term in count_vocab_terms:\n",
    "        condprob[term][cls] = (count_vocab_terms[term][cls]+1.0) / (float(count_words[cls])+1.0)\n",
    "\n",
    "# Apply MultinomialNB\n",
    "count_correct = 0\n",
    "count_total = 0\n",
    "for ID, value in all_docs.iteritems():\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    vocab_terms_in_doc = value[1]\n",
    "    for cls in classes:\n",
    "        score[cls] = math.log(prior[cls])\n",
    "        for term, count in vocab_terms_in_doc.iteritems():\n",
    "            if condprob[term][cls] > 0.0:\n",
    "                score[cls] += (count * math.log(condprob[term][cls]))\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (ID, TRUTH, prediction))\n",
    "    count_total += 1\n",
    "    if TRUTH == prediction:\n",
    "        count_correct += 1\n",
    "print('accuracy: %f' % (count_correct / float(count_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Megan Jasek\n",
    "## Description: mapper code for HW1.5_OLD\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Takes a list of words and returns a string separated by spaces of the\n",
    "# unique words in the list\n",
    "def findUniqueWords(word_list):\n",
    "    unique = []\n",
    "    for w in word_list:\n",
    "        if w not in unique:\n",
    "            unique.append(w)\n",
    "    return ' '.join(unique)\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "#findwords = re.split(\" \",sys.argv[2].lower())\n",
    "findword = sys.argv[2].lower()\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        items = line.split('\\t')\n",
    "        ID = items[0]\n",
    "        TRUTH = items[1]\n",
    "        # Create a list of words found in each line\n",
    "        words_subject = re.findall(WORD_RE, items[2].lower())\n",
    "        words_body = re.findall(WORD_RE, items[3].lower())\n",
    "        words_all = words_subject + words_body\n",
    "        count_total_words = len(words_all)\n",
    "        all_text = ' '.join(words_all)\n",
    "        vocab = findUniqueWords(words_all)\n",
    "        print('%s\\t%s\\t%d\\t%s\\t%s' % (ID, TRUTH, count_total_words, vocab, all_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Megan Jasek\n",
    "## Description: reducer code for HW1.5_OLD\n",
    "import sys\n",
    "\n",
    "def updateVocab(V, word_list):\n",
    "    for w in word_list:\n",
    "        if w not in V:\n",
    "            V.add(w)\n",
    "    return V\n",
    "\n",
    "def countTokensOfTerm(word_list, term):\n",
    "    return word_list.count(term)\n",
    "\n",
    "all_docs = {}\n",
    "all_vocab = set()\n",
    "count_docs = [0, 0]\n",
    "count_words = [0, 0]\n",
    "all_text = [[], []]\n",
    "\n",
    "# Loop through the files once to create the aggregate counts\n",
    "for i in range(1, len(sys.argv)):\n",
    "    filename = sys.argv[i]\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            elements = line.split(\"\\t\")\n",
    "            ID = elements[0]\n",
    "            TRUTH = int(elements[1])\n",
    "            count_docs[TRUTH] += 1\n",
    "            count_words[TRUTH] += int(elements[2])\n",
    "            vocab = elements[3].split(' ')\n",
    "            all_vocab = updateVocab(all_vocab, vocab)\n",
    "            text = elements[4].split(' ')\n",
    "            all_text[TRUTH] += text\n",
    "            # save the document for later use\n",
    "            all_docs[ID] = [TRUTH, text]\n",
    "\n",
    "total_docs = sum(count_docs)\n",
    "prior = [0, 0]\n",
    "condprob = [{}, {}]\n",
    "classes = [0, 1]\n",
    "\n",
    "# Train MultinomialNB\n",
    "for cls in classes:\n",
    "    prior[cls] = count_docs[cls] / float(total_docs)\n",
    "    for term in vocab:\n",
    "        count_term = countTokensOfTerm(all_text[cls], term)\n",
    "        condprob[cls][term] = (count_term + 1.0) / (count_words[cls] + 1.0)\n",
    "\n",
    "# Apply MultinomialNB\n",
    "for key, value in all_docs:\n",
    "    score = [0, 0]\n",
    "    TRUTH = value[0]\n",
    "    for cls in classes:\n",
    "        score[cls] = log(prior[cls])\n",
    "        text = value[1]\n",
    "        for term in text:\n",
    "            score[cls] += log(condprob[cls][term])\n",
    "    prediction = 1 if (score[1] > score[0]) else 0\n",
    "    print('%s\\t%d\\t%d' % (key, TRUTH, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./reducer.py\", line 52, in <module>\r\n",
      "    for key, value in all_docs:\r\n",
      "ValueError: too many values to unpack\r\n"
     ]
    }
   ],
   "source": [
    "# Usage: pNaiveBayes.sh m wordlist\n",
    "!./pNaiveBayes.sh 99 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[2]\n",
    "findword = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    #Please insert your code\n",
    "    for line in myfile.readlines():\n",
    "        # Create a list of words found in each line\n",
    "        words = re.findall(WORD_RE, line)\n",
    "        count += words.count(findword)\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for line in sys.stdin:\n",
    "    #Please insert your code\n",
    "    sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write script to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pGrepCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pGrepCount.sh\n",
    "ORIGINAL_FILE=$1\n",
    "FIND_WORD=$2\n",
    "BLOCK_SIZE=$3\n",
    "CHUNK_FILE_PREFIX=$ORIGINAL_FILE.split\n",
    "SORTED_CHUNK_FILES=$CHUNK_FILE_PREFIX*.sorted\n",
    "usage()\n",
    "{\n",
    "    echo Parallel grep\n",
    "    echo usage: pGrepCount filename word chuncksize\n",
    "    echo greps file file1 in $ORIGINAL_FILE and counts the number of lines\n",
    "    echo Note: file1 will be split in chunks up to $ BLOCK_SIZE chunks each\n",
    "    echo $FIND_WORD each chunk will be grepCounted in parallel\n",
    "}\n",
    "#Splitting $ORIGINAL_FILE INTO CHUNKS\n",
    "split -b $BLOCK_SIZE $ORIGINAL_FILE $CHUNK_FILE_PREFIX\n",
    "#DISTRIBUTE\n",
    "for file in $CHUNK_FILE_PREFIX*\n",
    "do\n",
    "    #grep -i $FIND_WORD $file|wc -l >$file.intermediateCount &\n",
    "    ./mapper.py $FIND_WORD $file >$file.intermediateCount &\n",
    "done\n",
    "wait\n",
    "#MERGEING INTERMEDIATE COUNT CAN TAKE THE FIRST COLUMN AND TOTOL...\n",
    "#numOfInstances=$(cat *.intermediateCount | cut -f 1 | paste -sd+ - |bc)\n",
    "numOfInstances=$(cat *.intermediateCount | ./reducer.py)\n",
    "echo \"found [$numOfInstances] [$FIND_WORD] in the file [$ORIGINAL_FILE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pGrepCount.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pGrepCount filename word chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [11] [COPYRIGHT] in the file [License.txt]\r\n"
     ]
    }
   ],
   "source": [
    "!./pGrepCount.sh License.txt COPYRIGHT 4k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
