{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MidTerm DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Megan Jasek  \n",
    "**Email:** meganjasek@ischool.berkeley.edu  \n",
    "**Class Name:** W261-2  \n",
    "**Week Number:** 8, MidTerm Exam  \n",
    "**Date:** 6/29/16  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRJob for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff**2\n",
    "    \n",
    "    distances = (diffsq.sum(axis = 1))**0.5\n",
    "    # Get the nearest centroid for each instance\n",
    "    min_idx = argmin(distances)\n",
    "    return min_idx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        idx = MinDist(D,self.centroid_points)\n",
    "        yield int(idx), (D[0],D[1],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield int(idx),(sumx,sumy,num)\n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        distances = 0\n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Driver:**  \n",
    "Generate random initial centroids  \n",
    "\n",
    "New Centroids = initial centroids  \n",
    "\n",
    "While(1):  \n",
    "\n",
    "* Cacluate new centroids\n",
    "* stop if new centroids close to old centroids\n",
    "* Updates centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random, array\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv '])\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = [[0,0],[6,3],[3,6]]\n",
    "k = 3\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "for i in range(10):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i+1)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting customers.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile customers.dat\n",
    "1|Alice Bob|31|CA\n",
    "2|Sam Sneed|51|NV\n",
    "3|Jon Sneed|37|CA\n",
    "4|Arnold Wesise|17|NY\n",
    "5|Henry Bob|25|NV\n",
    "6|Yo Yo Ma|37|NY\n",
    "7|Jon York|41|WA\n",
    "8|Alex Ball|26|WA\n",
    "9|Jim Davis|19|CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting orders.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile orders.dat\n",
    "1|Apple\n",
    "3|Garlic\n",
    "2|Milk\n",
    "1|Iphone\n",
    "4|Ipad\n",
    "5|Book\n",
    "7|Potato\n",
    "8|Tomato\n",
    "9|Orange\n",
    "5|shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MrJob class for ReducerSideInnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducersideinnerjoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducersideinnerjoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class innerjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 4:\n",
    "            yield x[0], (\"lefttable\", x[1], x[2], x[3])\n",
    "        else:\n",
    "            yield x[0], (\"righttable\", x[1])\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        customers = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0] == u'lefttable':\n",
    "                customers.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "        for o in orders:\n",
    "            for c in customers:\n",
    "                yield None, [key] + c[1:] + o[1:]\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer=self.reducer)] \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    innerjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/reducersideinnerjoin.hadoop.20160629.220526.650317\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/reducersideinnerjoin.hadoop.20160629.220526.650317/output...\n",
      "null\t[\"7\", \"Jon York\", \"41\", \"WA\", \"Potato\"]\n",
      "null\t[\"8\", \"Alex Ball\", \"26\", \"WA\", \"Tomato\"]\n",
      "null\t[\"9\", \"Jim Davis\", \"19\", \"CA\", \"Orange\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Apple\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Iphone\"]\n",
      "null\t[\"2\", \"Sam Sneed\", \"51\", \"NV\", \"Milk\"]\n",
      "null\t[\"3\", \"Jon Sneed\", \"37\", \"CA\", \"Garlic\"]\n",
      "null\t[\"4\", \"Arnold Wesise\", \"17\", \"NY\", \"Ipad\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"Book\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"shoes\"]\n",
      "Removing temp directory /tmp/reducersideinnerjoin.hadoop.20160629.220526.650317...\n"
     ]
    }
   ],
   "source": [
    "!python reducersideinnerjoin.py 'customers.dat' 'orders.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run the code through python driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reminder: You cannot use the programmatic runner functionality in the same file as your job class. That is because the file with the job class is sent to Hadoop to be run. Therefore, the job file cannot attempt to start the Hadoop job, or you would be recursively creating Hadoop jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use make_runner() to run an MRJob\n",
    "1. seperate driver from mapreduce jobs\n",
    "2. now we can run it within pythonnode book \n",
    "3. In python, typically one class is in each file. Each mrjob job is a seperate class, should be in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'7', u'Jon York', u'41', u'WA', u'Potato']\n",
      "[u'8', u'Alex Ball', u'26', u'WA', u'Tomato']\n",
      "[u'9', u'Jim Davis', u'19', u'CA', u'Orange']\n",
      "[u'1', u'Alice Bob', u'31', u'CA', u'Apple']\n",
      "[u'1', u'Alice Bob', u'31', u'CA', u'Iphone']\n",
      "[u'2', u'Sam Sneed', u'51', u'NV', u'Milk']\n",
      "[u'3', u'Jon Sneed', u'37', u'CA', u'Garlic']\n",
      "[u'4', u'Arnold Wesise', u'17', u'NY', u'Ipad']\n",
      "[u'5', u'Henry Bob', u'25', u'NV', u'Book']\n",
      "[u'5', u'Henry Bob', u'25', u'NV', u'shoes']\n",
      "\n",
      "\n",
      "There are 10 records\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from reducersideinnerjoin import innerjoin\n",
    "mr_job = innerjoin(args=['customers.dat','orders.dat'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducersideleftjoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducersideleftjoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class leftjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 4:\n",
    "            yield x[0], (\"lefttable\", x[1], x[2], x[3])\n",
    "        else:\n",
    "            yield x[0], (\"righttable\", x[1])\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        customers = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0] == u'lefttable':\n",
    "                customers.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "        for c in customers:\n",
    "            for o in orders:\n",
    "                yield None, [key] + c[1:] + o[1:]\n",
    "            if orders == []:\n",
    "                yield None, [key] + c[1:] + [None]\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer=self.reducer)] \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    leftjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/reducersideleftjoin.hadoop.20160629.221207.205060\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/reducersideleftjoin.hadoop.20160629.221207.205060/output...\n",
      "null\t[\"6\", \"Yo Yo Ma\", \"37\", \"NY\", null]\n",
      "null\t[\"7\", \"Jon York\", \"41\", \"WA\", \"Potato\"]\n",
      "null\t[\"8\", \"Alex Ball\", \"26\", \"WA\", \"Tomato\"]\n",
      "null\t[\"9\", \"Jim Davis\", \"19\", \"CA\", \"Orange\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Apple\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Iphone\"]\n",
      "null\t[\"2\", \"Sam Sneed\", \"51\", \"NV\", \"Milk\"]\n",
      "null\t[\"3\", \"Jon Sneed\", \"37\", \"CA\", \"Garlic\"]\n",
      "null\t[\"4\", \"Arnold Wesise\", \"17\", \"NY\", \"Ipad\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"Book\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"shoes\"]\n",
      "Removing temp directory /tmp/reducersideleftjoin.hadoop.20160629.221207.205060...\n"
     ]
    }
   ],
   "source": [
    "!python reducersideleftjoin.py 'customers.dat' 'orders.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducersideleftjoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducersideleftjoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class leftjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 4:\n",
    "            yield x[0], (\"lefttable\", x[1], x[2], x[3])\n",
    "        else:\n",
    "            yield x[0], (\"righttable\", x[1])\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        customers = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0] == u'lefttable':\n",
    "                customers.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "        for c in customers:\n",
    "            for o in orders:\n",
    "                yield None, [key] + c[1:] + o[1:]\n",
    "            if orders == []:\n",
    "                yield None, [key] + c[1:] + [None]\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer=self.reducer)] \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    leftjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/reducersideleftjoin.hadoop.20160629.221207.205060\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/reducersideleftjoin.hadoop.20160629.221207.205060/output...\n",
      "null\t[\"6\", \"Yo Yo Ma\", \"37\", \"NY\", null]\n",
      "null\t[\"7\", \"Jon York\", \"41\", \"WA\", \"Potato\"]\n",
      "null\t[\"8\", \"Alex Ball\", \"26\", \"WA\", \"Tomato\"]\n",
      "null\t[\"9\", \"Jim Davis\", \"19\", \"CA\", \"Orange\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Apple\"]\n",
      "null\t[\"1\", \"Alice Bob\", \"31\", \"CA\", \"Iphone\"]\n",
      "null\t[\"2\", \"Sam Sneed\", \"51\", \"NV\", \"Milk\"]\n",
      "null\t[\"3\", \"Jon Sneed\", \"37\", \"CA\", \"Garlic\"]\n",
      "null\t[\"4\", \"Arnold Wesise\", \"17\", \"NY\", \"Ipad\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"Book\"]\n",
      "null\t[\"5\", \"Henry Bob\", \"25\", \"NV\", \"shoes\"]\n",
      "Removing temp directory /tmp/reducersideleftjoin.hadoop.20160629.221207.205060...\n"
     ]
    }
   ],
   "source": [
    "!python reducersideleftjoin.py 'customers.dat' 'orders.dat'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
